U-GARDEN: AN INTERACTIVE CONTROL SYSTEM FOR MULTIMODAL
PRESENTATION IN MUSEUM

Fu-Song Hsu¹, Wei-Yang Lin¹, Geeng-Neng You², Tzu-Wei Tsai², Hsiu-Mei Huang²

1Department of Computer Science and Information Engineering,

National Chung Cheng University, Min-Hsiung Chia-Yi 621, Taiwan {hfs95p, 
wylin}@cs.ccu.edu.tw

2National Taichung Institute of Technology, Taichung 404, Taiwan {gny, wei, 
hmhuang}@ntit.edu.tw


ABSTRACT

In this rapid development of advanced computer technology
era, the trend for interaction with the input devices has
shifted toward the incorporation of intelligent technology
gradually. To facilitate the intuitive paradigm for human to
use digital equipment, the traditional input devices such as
keyboards and mice are augmented or even replaced by
various novel mechanisms. The provision of the intuitive
functions by using a variety of end devices such as camera,
RFID, WiiMote enhances the interaction and playfulness of
information presentation. This paper presents a set of tools
to  help developers create interactive presentations with
machine intelligence. The design rationale of the set of tools
called the “U-Garden” was described. We also illustrate
how the U-Garden could be adopted to synchronize the
hardware devices and media content to build an intelligent
interaction. To test the performance of the U-Garden, the
tools were applied to interactive presenting applications to
demonstrate the content presenting power for digital
archives. Finally, experiments with user experience on our
proposed content presenting system were conducted and
discussed. The results reveal positive responses that users
are able to appreciate this intelligent presenting of the
content of digital archives through our tools.

Keywords—Intelligent User Interface, Multimodal
System Design, Multimodal Interaction, Digital Archives

1. INTRODUCTION

In recent development, museum environment has often
designed  their  presentation  with  modern  multimedia
technology as shown in the Figure 1. The Figure 1 presented
two systems as examples of the utilization of these
multimedia presentations. One of the systems is the high
level audiovisuals, which is like a 180/360 degree theatre
screen as shown in the Figure 1(a). Imagine that based on
those technology, users can see in any direction with 360
degree of the entire space. But these audiovisual systems are
expensive and inflexible. Another system is the low level
KIOSK system as shown in the Figure 1(b), which is cheap
and easy use. The KIOSK system is easy to be set up at

many places, such as at the entrance, or a corner. Although
the KIOSK system has better flexible and less cost than
using the high level audiovisuals, it is not interesting and
unattractive to users. In order to avoid the drawbacks of
currently multimedia presentation systems, we present a set
of tools to provide functionalities for interacting with system
via a variety of modern end devices in this study.

(a) 360 degree 3D theatre screen    (b) KIOSK system

Fig. 1. Currently multimedia presentation systems within museum.

Currently, the Adobe Flash [15] is arguably the most
popular software for designing and building digital archives.
Although the Adobe Flash provide certain ways for
managing and accessing multimedia resources, a critical
problem  still  significantly  impedes  users’  immerse
experience and pleasure: the user input modes that rely on
the traditional keyboard and mouse, and that user interface
must make clear how to carry out a task using the keyboard
and mouse. Moreover, it has no ability to accompany the
interactive interaction with machine intelligence. Use of
keyboard and mouse in multimedia presentation may not be
enough: People have to spend much time on learning how to
use the interface in browsing content. Therefore, we propose
a novel type of multimedia presentation architecture to
supply the Adobe Flash with multimodal interaction. Based
on our proposed architecture, we developed a platform that
combines the advantages of the multimedia functionality of
Adobe  Flash  and  interaction  ability  of  multimodal
computing. This platform, named “U-Garden”, is able to
handle synchronize hardware devices and process media
content. Based on the U-Garden, multimodal presentations
have been developed. An example that illustrates the
development using the U-Garden is the content presenting

978-1-4244-7493-6/10/$26.00 ©2010 IEEE                           ICME 2010


system for digital archives. Finally, user study of the content
presenting system is offered. We conclude that there is a
value in further study of multimedia presentation through an
intelligent way.

2. RELATED WORK

In  general,  some  researchers  use  the  term  IMMPS
(intelligent multimedia presentation systems) to refer to an
interactive system by exploiting different ways, such as
visual, auditory, tactile modalities. The SRM (standard
reference model) [1] is one of first conceptual models about
the architecture model of IMMPS. To achieve an IMMPS
implementation, the SRM model assumes that a set of five
layers could be considered. They are as control layer,
content  layer,  design  layer,  realization  layer,  and
presentation display layer respectively. This model reflects
the  practical  processes  of  generation  of  multimedia
presentation. Rousseau et al. [2] proposes the WWHT model
that extends the SRM model to enhance the semantic and
presentation evolution.

The concept of human-centric computing should also be
considered to prevent users from doing a great deal of
tedious work. Hence, how to create a truly effective human-
centric ambient interface is the primary problem. One
important issue on human-centric computing is the users
how to control and interact with the systems? One solution
is the multimodal interaction that allows people to interact
with different ways [3]. In order to build a human-centric
ambient interface, multi natural input modes such as speech
recognition, pen, eye gaze and body movements have
widely used. Oviatt [4] has proposed the properties of multi-
modes system. We briefly list the properties as follows: (1)
Expressive power: To be suitable for fast response, the
sensor handling of multi-modes system has been designed
so that it is able to accurately track users’ input. (2)
Naturalness: The operations are done in an intuitive way. It
means to avoid unnecessary process or too much command.

(3) Portability: The developed system will be recycled and
easy to link with other designed objects in the feature.
Tennenhouse [5] defines three viewpoints of the interface
for   the virtual environment to communicate with physical
space: (1) Getting physical: In order to simulate the virtual
objects to real activities, the DSP (Digital Signal
Processing) technology can be adopted to receive the users
actions by sensing the physical space. (2) Getting real:
Tennenhouse suggests that the received control signal
should  be handled and responded immediately. (3) Getting
out: The factor of human has been shifted from the
interactive loop to supervisor role. The third point relied on
a  large degree of system automation, like automatic
detection and recognition technology.

Thus, based on the foregoing concepts and architectures,
the study attempts to develop an integration tool, namely the
central interactive control system serving users to access the

digital archives in the museum environment. It will be
discussed in the next section.

3. SYSTEM ARCHITECTURE

In this section we will illustrate our proposed tool for
multimedia presentation in museum. At first, let us briefly
analyze the advantages and disadvantages of existing
systems. The audiovisual and KIOSK system are well-
known approaches for multimedia content presentation in
museum. The audiovisual system is a good way to attract
users by using the advanced computing and engineering to
build a virtual space. Moreover, the audiovisual system may
need some expensive hardware, such as 180/360 theatre
screen, etc. Generally, the audiovisual system provides
significantly more enjoyment than others. On the other
hand, the KIOSK system allows a flexible setting anywhere
within the museum, like entrance or corner. However,
people usually feel that the KIOSK system plays less
impressive presentation. Therefore, we attempt to propose a
multimodal presentation that combines the advantages of the
audiovisual and KIOSK systems. We also hope that the
proposed system can improve the experience of browsing
and provide service anywhere in the museum. The details
are described in the following sections.

3.1. Interactive Control System

To facilitate the provision of services in any place and the
enjoyment of interactive presentation, the modern end
devices are needed in order to build intelligent ambient
interface. This research attempts to develop a platform that
synchronizes the hardware devices and media content to
build a multimodal presentation. The main architecture is
that the central Interactive Control System is built to help
designer presenting information through modern end device
with machine intelligence. In the study we define the
following roles for the Interactive Control System: (1)
Interface. Communication between visitor and exhibit. (2)
Nerve unit. For the intuitive interaction, the system has
engages in the nerve unit to detect human actions. Besides,
it also drives the system response, such as sound/image
output. (3) System manger. It includes input device, output
device and control device.

Given a set of multimedia resources such as photo, sound,
video and animation, how the interactive control system
drives the visual objects to perform activities and how the
interactive control system synchronizes with the control
signal by different end devices are importance issues. To
overcome  this  challenge,  we  address  a  practicable
architecture that has been built in C++ to devise the
multimodal presentation as shown in the Figure 2. It is
composed of three units. The first unit is about INPUT unit
which allows participants to use their end device to send the
input signals into the interactive control system.



Control signals
of end drives

Interactive
Control System

U-Garden

Signal

Captured In

Video/Audio Out

Video/Audio
In

Body Movements
tracking and analysis

Shockwave Flash

Presentation of
multimedia document

Fig. 2. The architecture of multimodal presentation


The interactive control system could be served as the signal
processing kernel to keep tracking visitors’ actions. Besides,
a given set of media component and script will also be
automatically loaded when a presentation begins. In this
challenge, it’s necessary to use a script to make any spatial
and temporal relationships among objects explicit on
presentation design. The script format is supported by the
Adobe ActionScript language [15] and performed within the
embedded Flash multimedia player. Figure 3 presents a
screen capture of a component behavior on U-Garden with
corresponding  description  in  the  script.  Finally,  the
processing kernel receives the control signal from INPUT
unit and then synchronizes the execution of a given script
into the OUTPUT process via XML-based message during
the presentation.

3.2. Content Presenting System

Based on the interactive control system, we will develop an
intelligent content presenting system for digital archives.
With the machine intelligence of the content presenting
system, users can control the process of presentation by
using their gesture movements as shown in Figure 4. First
we refer to Rousseau’s definition for the concept of WWHT
model to specify all entities for the content presenting
system for digital archives. We are now going to address a
scenario through the WWHT specification. When the
‘Forward (X)’ (“What” step) command executed by users.
The command feedback should be an important role during
the presentation. It not only has concerns about how to
display the results of executed command, but also puts
efforts on how to guide users to browse content. To archive
this goal, the scroll bar of display information allows the
participant to use the hand to interact with presentation in an

intuition way. More precisely, the participant can use his/her
hand as the input signal to manipulate the scroll bar of
presented processing.

var mcl_obj:Object = new Object();
mcl_obj.onLoadInit = function
(target_mc:MovieClip):Void {
target_mc.onEnterFrame = function() {
if (Math.abs(target_mc._x - _xtracker)

< 50) {

target_mc._x = _xtracker;
target_mc._y = _ytracker;

…

(a)

(b)

Fig. 3. (a) Description of a component behavior (b) Presentation of
a component behavior on U-Garden platform;


Fig. 4. The user interacts with the content presenting system on the experiment


When a forward signal issued from the hand movement
is captured by a camera, the feedback will be presented with
various modalities: text, graph and mute. Note that in order
to highlight the feedback, the scroll bar is visually presented
as a geometric shape with a brightly color (“How” step) on
the screen (“Which” step). However, the fixed-size scroll
bar often causes bad display due to a lack of consistency in
content resolution. If lower resolution content is loaded, the
display of feedback must re-render with a suitable layout
(“Then” step).

Ways in which support the interactive interaction with
an intuition way. The human motion understanding
technology is useful to achieve the interactive system
automation. This technology has many practical application
such  as video surveillance [6-8], entertainment [9], human-
computer interaction [10], search and retrieval [16, 17],
smart room [18] and so on. In order to quickly track a hand
object could be segmented from the background or other
object, such as head. We use a simple strategy for real-time
human and gesture tracking. In the content presenting
system, information on the two-dimensional coordinate of
user location is considered with the face himself. Users
perform gestures neighbors on a face, which is natural and
comfortable. To facilitate the process of hand segmentation,
seven blocks are defined, namely B₀-B₆ across a camera
image. The face area is defined as B₀, which obtained
through the face detection based on the boosted cascade
algorithm [11-13]. The hand needs to be properly located in
the B₁-B₆ areas, which are close to the B₀. Furthermore, the
skin color based detection is also applied here to ensure the
correct detection of hand. In order to avoid the suffering
from some interferences in physical environment, the
minimal pixel size of the hand more than a constant percent
of block size by function Block(t):

3.3. Sample applications

In the previous sections, we presented a presentation tool
and its application. We are now going to illustrate the more
complex and realistic application. The presentation tool was
applied on the image processing session of Interactive
Media Art Workshop Achievement Exhibition at the
National Taiwan Museum of Fine Arts. In the session, some
interesting developments are examples of the utilization of
U-Garden. They will be briefly present.

3.3.1. Chinese Sichuan Opera

In the previous study, we present a content presenting
system for browsing digital content in an intelligent way.
Besides, the intelligent technology can be used in a new
avenue for preserving and circulating culture. The basic idea
is that we develop a multimodal presentation for the Chinese
Sichuan Opera as shown in Figure 5. It seems that the
largest feature on the Sichuan Opera is the “Face-Channing”
art, which is unique culture performance. One of the special
features is that the actors do not have to change their
makeup in the background for the next stage. Somehow the
actors are able to completely change their appearances in an
instant and transform into another mood. Sichuan Opera
actors apply the “Face-Changing” effect to show their
emotions, such as the happiness, anger, sadness, joy or
surprise. Based on this scenario, the authors design a
presentation scheme for the Sichuan Opera [19] as follows:
For   a start, while a participant enters the presentation area,
his/her frontal face is detected by a camera immediately.
After that, the application loads a set of predefined Chinese
Sichuan Opera faces. The application will randomly select a
face and render this face on the virtual character. The face


Block(s) = ⎧1:

⎩0 :

Cs > ΩCi

otherwise

(1)

will change when the application detects participant’s head
movement. Moreover, the application allows participant to
use his/her frontal face as the input signal to manipulate the
presented results. When an input signal controlled by a face


The hand can be presence on Block s if Block(s)=1. Ci
presents the number of pixel in the block, and Cs denotes the
skin color pixel number. The Ω is a threshold value to
ensure a large enough amount of pixels with skin color. In
our experiment the Ω value is given between 0.2 and 0.5
depending on the lighting and noise condition.

movement is received by the application, the face-changing
effect will be presented on the screen. Therefore, a
participant can use face movement to physically immerse in
the virtual show and appreciate all of face sets of Chinese
Sichuan Opera.


visitors can use their movements to control it without
physically touch anything. Such type of interaction not only
improves the enjoyment, but also decreases the fees of
hardware maintained as possible for more people to make
use  of it.

Fig. 5. The presentation of Chinese Sichuan Opera

3.3.2. Shadow Puppetry


Shadow puppetry is a traditional form for display shows and
tells stories. It has been a popular entertainment for more
than 20 countries. It’s usually using an opaque portrait over
an illuminated background to create an exhibition, as shown
in Figure 6(a). The Shadow Puppetry application [20] is
designed in order to provide visitors with deeper
experiences. The installation that presents the shadow
puppetry culture relies on a gesture-based interface. This
installation consists of a semi-closed black box and a sensor
that facilitates the hand gesture detection. Figure 6(b) shows
the computer-human interface in this application. Users
perform gestures under the sensor, which is convenient to
capture complete gesture information. After processing the
gesture  information,  the  application  retrieves  the
corresponding portrait action and displays this action on the
screen. One goal of this application is to demonstrate that an
intelligent system can be served to present cultural assets in
an interesting manner.

Fig. 6. (b) The interface

Fig. 7. Pictures of the evil mirror

4. USER STUDY

To evaluate the user experience of multimodal presentation,
we conduct an experiment to understand users' continuance
intention to use the content presenting system. For a start,
the experiment working is presented clearly as follows: The
user enters into the presentation area, and then the user is
detected by the camera. After that, the system automatically
loads a selected digital archive document to show the video
content on the screen. Finally, during the show time, the
system allows the user to use his/her hand without
physically touching anything to control the process of
presentation as shown in Figure 4.

The  experiment  environment  requires  devices  as
following: Pentium 4 CPU, 1 GB Memory RAM and the
Logitech Orbit MP camera. The study was conducted in a
university in central Taiwan with a sample of 150 evaluators.
Participants were asked to take an experiment as described
in the above, and a total of 125 responses were collected.
The evaluation of user experience for our system is gathered
by a questionnaire survey. Participants were asked to
indicate  their  preference  for  this  presentation.  The
questionnaire has 4 items and all adopt 7-points Likert
scales (from “1” which means “strongly disagree” to, “7”


Fig. 6. (a) Screenshot

3.3.3. The Evil Mirror

of shadow puppetry
presentation

“strongly agree”). The results are shown in Table 1, where
we can see people not only have a high continuance
intention in this  multimodal presentation, but also
recommend it to others in the future. The descriptive
statistic means [M] and standard deviations [SD] are also


U-Garden can also be used in game development. The Evil
Mirror application [21] is a game about real life experience.
The Evil Mirror is a large LCD screen that displays videos
captured from physical environment. In addition to the live
video, a set of animation real eyes are added and distributed
over the screen. These eyes represent unwanted attentions
drawn towards players. Players have to wave their hands or
move their bodies to get these attentions away. Figure 7
presents the pictures of people playing the game. Based on
the advance of image processing and analysis technology,

included in Table 1.


Table 1. Understanding users' continuance intention to use

             multimodal presentation       

 Questions                    M  SD 
I will be happy to continue to use this system.   6.10  .91

I will recommend this system to other people.   6.19  .89
I think this kind system is a future trend.     6.38  .83

[8] N.M. Oliver, B. Rosario, and A.P. Pentland, “A Bayesian
computer vision system for modeling human interactions,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, 22(8),
831-843, 2000.

[9] A.F. Bobick, and J.W. Davis, “The recognition of human
movement using temporal templates,” IEEE Transactions on


I think that this kind of systems will be applied

6.34 .87

Pattern Analysis and Machine Intelligence, 23(3), 257-267, 2001


    to  more  people  in  the  future.            

6.25

5. CONCLUSION

This paper has proposed a way to incorporate machine
intelligence into multimedia presentation. In order to
develop a multimedia presentation with an intelligent way,
we present a platform that could be served as the interactive
control system to synchronize the hardware devices and
media content. The platform can be adopted to build many
interesting applications through multimodal interaction. In
this study, a content presenting system with an intuitional
operation interface has been made to present the digital
content of archives in museum. Similar ideas can also be
applied to generate related interactive presentation or
guidance. Moreover, the proposed system seems to integrate
with the audiovisual and KIOSK systems. That’s the
multimodal presentation often entertains users and that it
could  be set in any place within museum to provide
services.

Our contributions in this paper include: (1) investigation
of the appropriateness of theory for multimedia presentation
in museum, (2) a design rationale of the set of tools for
multimodal presentation, (3) experimental validation of the
feasibility, and that the results reveal the multimodal
presentation is a new and playful system for visitors in
museum environment.

6. REFERENCE

[1] M. Bordegoni, G. Faconti, S. Feiner, M. T. Maybury, T. Rist, S.
Ruggieri, P. Trahanias, and M. Wilson, “standard reference model
for intelligent multimedia presentation systems,” Computer
Standards & Interfaces,18 (6-7), 477-496, 1997.

[2] C. Rousseau, Y. Bellik, F. Vernier, and D. Bazalgette, “A
Framework for the Intelligent Multimodal Presentation of
Information,” Signal Processing, 86, 3696-3713, 2006.

[3] E. Roseman and M. Edventures, “Multimodal Interaction in
Human-centric and Proactive Computing,” International Workshop
on Spatial Media, 2006.

[4]  S.  Oviatt,  “Ten  myths  of  multimodal  interaction,”
Communications                                of the ACM, 42, 74-81, 1999.

[5] D. Tennenhouse, “Proactive computing,” Communications of
the ACM, 43, 43-50, 2000.

[6] W. Hu, T. Tan, L. Wang, and S. Maybank, “A survey on visual
surveillance of object motion and behaviors,” IEEE Transactions
on Systems, Man, and Cybernetics, Part C: Applications and
Reviews, 34(3), 334-352, 2004.

[7] Y.A. Ivanov and A.F. Bobick, “Recognition of visual activities
and interactions by stochastic parsing,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, 22(8), 852-872, 2000.

[10] W.T. Freeman, and M. Roth, “Orientation histograms for
hand gesture recognition,” Proceedings of the International
Workshop on Automatic Face and Gesture Recognition, 296-301,
1995.

[11] P. Viola and M.J. Jones, “Rapid object detection using a
boosted cascade of simple features,” Proceeding of the IEEE
Computer Society Conference on Computer Vision and Pattern
Recognition, 511-518, 2001.

[12] R. Lienhart, A. Kuranov, and V. Pisarevsky, “Empirical
Analysis of Detection Cascades of Boosted Classifiers for Rapid
Object Detection,” Proceedings of the Pattern Recognition
Symposium, 297-304, 2003.

[13] P. Viola and M. J. Jones, “Robust real-time face detection,”
International Journal of Computer Vision, 57(2), 137-154, 2004

[14] J. Nielsen, “Usability Engineering”, Academic Press, San
Diego, 1993.

[15] Adobe Flash, http://www.adobe.com/products/flash/

[16] A. Licsár, T. Szirányi, L. Kovács, and B. Pataki, “Tillarom:
an AJAX based folk song search and retrieval system with gesture
interface based on kodály hand,” Proceedings of the 1st ACM
international Workshop on Human-Centered Multimedia, 81-88,
2006.

[17] A. Licsàr, T. Szirànyi, L. Kovàcs, and B. Pataki, “A Folk
Song Retrieval System with a Gesture-Based Interface,” IEEE
MultiMedia, 16(3), 48-59, 2009.

[18] A.F. Bobick, S.S. Intille, J.W. Davis, F. Baird, C. S. Pinhanez,

L. W. Campbell, Y. A. Ivanov, A. Schütte, and A. Wilson, “The
KidsRoom: A perceptually-based interactive and immersive story
environment,” Presence, 8(4), 369-393, 1999.

[19] C.-S. Lu, I.-J. Cho, C.-L. Tsai, and F.-S. Hsu, “Chinese
Sichuan Opera,” Interactive Media Art Workshop Achievement
Exhibition, CDROM, 2008.

[20] P.-Y. Chen, C.-H.Shih, T.-H. Chen, M.-H. Hsieh, W.-C.
Chang, and F.-S. Hsu, “Shadow Puppetry,” Interactive Media Art
Workshop Achievement Exhibition, CDROM, 2008.

[21] W.-Y. Ku, H.-J. Lin, S.-W. Wang, H.-M. Yu, and F.-S. Hsu,
“The Evil Mirror,” Interactive Media Art Workshop Achievement
Exhibition, CDROM, 2008.

