See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220955431

Digital Restoration of Medieval Tapestries.
Conference Paper · January 2005
DOI: 10.2312/VAST/VAST05/143-150 · Source: DBLP

CITATIONS

READS

4

458

3 authors:
Sonja Schar

Hanspeter Bieri

Pädagogische Hochschule Bern

Universität Bern

9 PUBLICATIONS 20 CITATIONS

39 PUBLICATIONS 388 CITATIONS

SEE PROFILE

SEE PROFILE

Xiaoyi Jiang
University of Münster
187 PUBLICATIONS 1,883 CITATIONS
SEE PROFILE

Some of the authors of this publication are also working on these related projects:

Biokybernetik (engl. bio-automation, русский: биокибернетика, 中文: 生物控制论) View project

All content following this page was uploaded by Xiaoyi Jiang on 14 March 2014.
The user has requested enhancement of the downloaded file.

VAST 2005
The 6th International Symposium on Virtual Reality, Archaeology and
Intelligent Cultural Heritage
Incorporating:

3rd EUROGRAPHICS Workshop
on
Graphics and Cultural Heritage
ISTI-CNR Pisa, Italy
November 8–11, 2005

International Program Committee: Co-Chairs
Mark Mudge
Cultural Heritage Imaging
San Francisco, CA, USA

Nick Ryan
Computing Laboratory
Univ. of Kent, Canterbury, UK

Roberto Scopigno
CNR-ISTI
Pisa, Italy

International Program Committee
David Arnold, UK
Juan Antonio Barcelo, ES
Richard Beacham, UK
Jean-Angelo Beraldin, CA
Duncan Brown, UK
Pere Brunet, ES
Caverlee Cary, US
Alan Chalmers, UK
John Cosmas, UK
Yiorgos Chrysanthou, Cyprus
Andy Day, UK
George Drettakis, FR
Martin Doerr, GR
Sabry El-Hakim, CA

Dieter Fellner, AT
Andrej Ferko, SK
Maurizio Forte, IT
Franca Garzotto, IT
Michael Goesele, GE
Antonella Guidazzoli, IT
Sarah Kenderdine, AU
Celine Loscos, UK
Franco Niccolucci, IT
Philippe Martinez, FR
Carol O’Sullivan, IE
George Papagiannakis, CH
Petros Patias, GR
Sumanta Pattanaik, US

Daniel Pletinckx, BE
Marc Pollefeys, US
Maria Roussou, GR
Holly Rushmeier, US
Robert Sablatnig, AT
Tullio Salmon Cinotti, IT
Donald Sanders, USA
Steve Stead, UK
Nadia Thalmann, CH
Juan-Carlos Torres, ES
Ruth Tringham, US
Luc Van Gool, BE
Marc Waelkens, BE
Michael Wimmer, AT

Proceedings Production Editors
Dieter Fellner (Graz University of Technology, Austria)
Stephen Spencer (The University of Washington, USA)
Co-sponsored by ACM SIGGRAPH and EUROGRAPHICS Association

Dieter W. Fellner, Werner Hansmann, Werner Purgathofer, François Sillion
Series Editors
This work is subject to copyright.
All rights reserved, whether the whole or part of the material is concerned, specifically those
of translation, reprinting, re-use of illustrations, broadcasting, reproduction by photocopying
machines or similar means, and storage in data banks.
c
Copyright 2005
by the Eurographics Association
PO Box 16, CH-1288 Aire-la-Ville, Switzerland
Published by the Eurographics Association
PO Box 16, CH-1288 Aire-la-Ville, Switzerland
Printed in Germany
Cover design by Stephen Spencer and Stefanie Behnke
ISBN 3-905673-28-2
ISSN 1811-864X
The electronic version of the proceedings is available from the Eurographics Digital Library at
http://diglib.eg.org

Table of Contents
Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
Acknowledgement and Disclaimer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
Sponsors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
Keynotes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
Reconstruction and Editing
Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts . . . . . . . 13
Gero Müller, Gerhard H. Bendels, and Reinhard Klein
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Passive Reconstruction of High Quality Textured 3D Models of Works of Art . . . . . . . . . . . . . . . . . . . . . 21
N. Brusco, L. Ballan, and G. M. Cortelazzo
Reflection Transformation Imaging and Virtual Representations of Coins from the Hospice
of the Grand St. Bernard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Mark Mudge, Jean-Pierre Voutaz, Carla Schroer, and Marlin Lum
Detail-Preserving Surface Inpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
G. H. Bendels, R. Schnabel, and R. Klein
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
Dynamic Modelling
3D Modeling for Non-Expert Users with the Castle Construction Kit v0.5 . . . . . . . . . . . . . . . . . . . . . . . . 49
Björn Gerth, René Berndt, Sven Havemann, and Dieter W. Fellner
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
3D Face Modeling from Ancient Kabuki Drawings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59
Weiwei Xu, Ryo Akama, and Hiromi T. Tanaka
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
Rendering
Viewpoint Quality and Scene Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Dmitry Sokolov, Dimitri Plemenos
Rapid Visualization of Large Point-Based Surfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
Tamy Boubekeur, Florent Duguet, and Christophe Schlick
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

Table of Contents
Participating Media for High-Fidelity Cultural Heritage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Veronica Sundstedt, Diego Gutierrez, Fermin Gomez, and Alan Chalmers
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
Real-time Shader Rendering for Crowds in Virtual Heritage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Pablo de Heras Ciechomski, Sébastien Schertenleib, Jonathan Maïm, Damien Maupu,
and Daniel Thalmann
Virtual Presentation
Visualizing Temporal Uncertainty in 3D Virtual Reconstructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
T. Zuk, S. Carpendale, and W. D. Glanzman
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
Investigating the Structural Validity of Virtual Reconstructions of Prehistoric Maltese Temples . . . . 107
A. Chalmers and K. Debattista
Exploring Digitized Artworks by Pointing Posture Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
C. Malerczyk, P. Dähne, and M. Schnaider
Assessment of CH Applications
Interface Evaluation for Cultural Heritage Applications: the Case of FERRUM Exhibition . . . . . . . . 121
A. Alzua-Sorzabal, M. T. Linaza, M. Abad, L. Arretxea, and A. Susperregui
Visitors’ Evaluations of ICTs Used in Cultural Heritage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
Ruth Owen, Dimitrios Buhalis, and Daniël Pletinckx
Image Processing for CH
A Texture Based Approach to Reconstruction of Archaeological Finds . . . . . . . . . . . . . . . . . . . . . . . . . . 137
M. S. Sagiroglu and A. Erçil
Digital Restoration of Medieval Tapestries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Sonja Schär, Hanspeter Bieri, and Xiaoyi Jiang
Color Plate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
Cover Image Credits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
Color Plates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

Acknowledgement and Disclaimer
The VAST 2005 Conference has been partly supported by the EPOCH NoE.
EPOCH is funded by the European Commission under the Community’s Sixth Framework Programme, contract no. 507382. However, the content of this publication reflects only the authors’
views and the European Community is not liable for any use that may be made of the information contained herein.

Preface
Welcome to the community of Archaeologists, Computer Scientists/Graphics Researchers, Cultural Heritage Professionals, Artists, and those who understand the value of making humanity’s
cultural legacy available to everyone everywhere, that is VAST2005.
VAST2005 is the 6th International Symposium on Virtual Reality, Archaeology and Intelligent
Cultural Heritage. It is also the 3rd Eurographics Workshop on Graphics and Cultural Heritage.
Moreover, a number of EPOCH ("European Research Network on Excellence in Processing
Open Cultural Heritage", an EU IST Network of Excellence) meetings will also take place,
such as the EPOCH General Assembly and some workshops. Our conference showcases the
strengthening relationship between EPOCH and international initiatives involving Digital Technologies, Computer Graphics and Cultural Heritage work.
The dynamic integration of the digital and graphics information sciences with the cultural heritage discipline is generating a new landscape of opportunities for exploring our collective past.
Many of the best features of this emerging landscape have yet to be imagined. We present here
17 full papers, selected from 63 submissions, which covers most of the topics and domains concerning application of Information Technology and interactive Computer Graphics to Cultural
Heritage themes. Our Keynote speakers, Paul Debevec, Kent Weeks, and Dieter Fellner, are pioneers in this new world. Their work cover three basic topics: methodologies for the accurate
3D sampling of the reality, applications of graphics and IT technologies to archaeology, issues
and perspectives for the integration of 3D data in Digital Libraries.
We extend our thanks to all those whose labor, financial support, and encouragement made the
VAST2005 Conference possible. The International Program Committee, whose members represent a cross-section of the Computer Graphics, Archaeology, and Cultural Heritage communities, evaluated each submission with care, and regardless of the challenges of summer fieldwork
and travel, finished their work on time. The staff of the Italian National Research Council’s Institute of Information Science and Technology, including F. Ganovelli, M. Callieri, P. Cignoni, M.
Dellepiane, F. de Mitry, C. Montani, and G. Turini, managed the logistics of the Conference. The
staff at TU Braunschweig (recently moved to TU Graz), and especially Stefanie Behnke, managed the Eurographics web-based submission system and guided the effort that published these
proceedings. Finally, our corporate and institutional sponsors, who provided the money and
‘gifts in kind’ that made the conference possible: ACS Advanced Computer Systems, Dataport,
Konica Minolta, Leica Geosystems, Epoch, Eurographics, and the Italian National Research
Council.
mark, nick and roberto, Pisa, 2005.

Sponsors
Industrial Sponsors:

ACS Advanced Computer Systems

Data Port

Konica Minolta

Leica Geosystems

Institutional Sponsors:

Eurographics Association

EU IST NoE “EPOCH”

Consiglio Nazionale delle
Ricerche

Istituto di Scienza e Tecnologie
dell’Informazione

In Cooperation with:

ACM Siggraph

Keynote
Photometric Realism: Accurate Reflectance Modeling of Sites and Artifacts

Dr. Paul Debevec
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
paul@debevec.org
http://www.debevec.org/
Abstract
While many 3D scanning techniques can accurately record the geometry of sites and artifacts, a subject’s
reflectance properties are often simplistically recorded as an RGB texture map. In reality, each point in
a subject’s volume scatters incident illumination according to a complex 4D phase function unique to
itself, yielding far richer reflectance behaviors than traditional texture acquisition techniques are able to
model. A goal for digital documentation should be to record and render - in any illumination - the full
gamut of diffuse, specular, translucent reflectance properties of the site or artifact ranging from wood,
clay, precious metals, paints, skins, glass, and jewels. In this talk I will present three recently developed
techniques in pursuit of this goal. The first is the light stage, a device which captures spatially-varying
light scattering properties of artifacts by illuminating them from a dense array of lighting directions. The
second is linear light source reflectometry, a low-cost process that uses a translational pass of an inexpensive neon tube light to capture the reflectance properties of a planar artifact such as a daguerreotype
or an illuminated manuscript. Finally, I will present an environmental reflectometry process based on
inverse global illumination designed developed to digitize the geometry and reflectance properties of the
Parthenon on the Athenian Acropolis and to virtually reunite it with its sculptures in the British Museum.
Joint work with Chris Tchou, Tim Hawkins, Per Einarsson, Andrew Gardner, Jonas Unger, Andrew
Jones, Jonathan Cohen, Mark Bolas, Ian MacDowall, Charis Poullis, Jessi Stumpfel, Andrew Jones,
Nathaniel Yun, Therese Lundgren, Marcos Fajardo, and Philippe Martinez.
Short Biography
Paul Debevec is a research assistant professor at the University of Southern California (USC) and directs
the graphics laboratory at USC’s Institute for Creative Technologies. His 1996 Ph.D. thesis at UC Berkeley presented Facade, an image-based modeling and rendering system for creating photoreal architectural
models from photographs. He subsequently developed techniques for illuminating computer-generated
scenes with real-world lighting captured through high dynamic range photography, demonstrating new
image-based lighting techniques in his animations "Rendering with Natural Light", "Fiat Lux", and "The
Parthenon". He also led the design of HDR Shop, the first widely-used high dynamic range image editing program. Debevec’s recent work has produced several light stage devices that allow objects, actors,
and performances to be synthetically illuminated with novel lighting, recently used to create photoreal
digital actors for the 2004 film "Spider-Man 2". Debevec received ACM SIGGRAPH’s first Significant
New Researcher Award in 2001. See also http://www.debevec.org/

Keynote
The Theban Mapping Project: Website, Databases, and Research

Kent R. Weeks
Professor of Egyptology and
Director of the Theban Mapping Project
The American University in Cairo
113 Sharia Kasr el-Aini
Cairo, Egypt
weekstmp@yahoo.com
Abstract
For twenty years, the Theban Mapping Project has been preparing detailed maps and plans of the tombs
in Egypt’s Valley of the Kings. As a part of that work, it discovered KV 5, a tomb for the many sons
of Pharaoh Rameses II, and the largest tomb ever found in Egypt. The project’s survey and excavation
work are regularly reported on its website, www.kv5.com, and archived in a series of archaeological and
photographic databases. The website currently receives over 6 million hits each month, and its databases
have become the standard reference for all archaeological work at Thebes. This paper will review the
development of these resources, and outline the ways in which the breadth of their coverage has been
combined with great depth to create resources accessible to an audience ranging from school children to
professional Egyptologists.
Short Biography
Kent R. Weeks received his B.A. in anthropology and archaeology from the University of Washington
and his Ph.D. in Egyptology from Yale University. He has worked at the Metropolitan Museum of Art,
the University of Chicago, the University of California, Berkeley, and the American University in Cairo.
The recipient of many awards for teaching, publication, and research, he is honorary member of many
academic societies around the world.

Keynote
title

Dieter W. Fellner
Graz University of Technology, Austria
Abstract
Short Biography

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Rapid Synchronous Acquisition of Geometry and Appearance
of Cultural Heritage Artefacts
Gero Müller, Gerhard H. Bendels, Reinhard Klein
Institut für Computergraphik, Universität Bonn, Germany
{gero,bendels,rk}@cs.uni-bonn.de

Abstract
In order to produce visually appealing digital models of cultural heritage artefacts, a meticulous reconstruction
of the 3D geometry alone is often not sufficient, as colour and reflectance information give essential clues of the
object’s material. Standard texturing methods are often only able to overcome this fact under strict material and
lighting condition limitations. The realistic reconstruction of complex yet frequently encountered materials such
as fabric, leather, wood or metal is still a challenge. In this paper, we describe a novel system to acquire the 3Dgeometry of an object using its visual hull, recorded in multiple 2D images with a multi-camera array. At the same
time, the material properties of the object are measured into Bidirectional Texture Functions (BTF), that faithfully
capture the mesostructure of the surface and reconstruct the look-and-feel of its material. The high rendering
fidelity of the acquired BTF texture data with respect to reflectance and self-shadowing also alleviates the limited
precision of the visual hull approach for 3D geometry acquisition.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Picture/Image Generation]: Digitizing and scanning I.3.7 [Three-Dimensional Graphics and Realism]: Color, shading, shadowing, and texture I.4.1 [Digitization
and Image Capture]: Reflectance

1. Introduction
Three-dimensional digitisation using laser range scanners
has a long tradition in industrial applications, such as reverse
engineering, quality control and part inspection, where the
geometric error of the acquired data compared to the original model is the ultimate criterion. More recently, with hardware becoming available at more reasonable costs, the use of
3D data acquisition has expanded to other application fields
like cultural heritage and entertainment industries. In these
application fields, however, the faithful reconstruction of the
visual appearance is often much more important than the geometric error alone.
In archeological and museum applications, where digital
archives of cultural heritage artefacts become more and more
commonplace, the demand for realistic reconstruction of an
artwork’s material is particularly high, as this often gives
critical hints about important characteristics of the object,
such as for instance the location, epoch, or circumstances
under which the artwork was created.
c The Eurographics Association 2005.

A standard approach to digitise a 3D cultural heritage
artefact would be to acquire the geometry first (up to a certain degree of detail) using a Laser Range Scanner, structured light, tactile sensors or even a volumetric approach
on the basis of MRT- or CT-data. Given an appropriate
parametrisation, additionally recorded 2D-images can then
be projected onto the geometry as diffuse textures to represent colour and fine detail information — at the disadvantage
that the illumination is fixed to the conditions under which
the photographs were taken. Fixing the lighting conditions,
unfortunately, severely limits the use of 3D digital representations in cultural heritage applications, because moving, rotating and seeing an artefact in different environments is an
important operation that often gives additional insight. Traditional texturing methods are not capable of transporting
the characteristic appearance of complex, inhomogeneous,
yet often encountered materials like fabric, leather, or wood.
Another problem of this standard approach, common to all
of the above geometry acquisition methods is their inability
to handle transparent, reflective or fuzzy materials. In ad-

14 G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts
•
•
•
•

Figure 1: Leather Material, rendered using standard texturing (left), bump mapping (middle), and BTF-rendering
(right).

dition, geometry acquisition usually involves sophisticated
procedures during, and after, recording (such as path planning in order to ensure detection of the complete surface,
registration, reconstruction and parametrisation).
As a consequence, 3D photography (as it is also called)
is often perceived by end-users in the museums as a complicated and time-consuming procedure with possibly nonsatisfying results – in one word impractical for large classes
of objects and applications.
With this situation in mind, we present in this paper a
novel high fidelity acquisition system to synchronously capture an object’s 3D geometry and material properties in a
very time-efficient and user-friendly way. Our system exploits images from the multi-camera array depicted in Figure 3 first to reconstruct an artefact’s coarse to medium scale
geometry using a GPU-based visual hull technique, resulting in a closed triangle mesh. In parallel, the images are also
used to capture the object’s appearance into so-called bidirectional texture functions (BTF) – a 6-dimensional texture
representation introduced by Dana et al. [DNvGK97] which
extends the common textures by dependence on light- and
view-direction, and thereby allows for photo-realistic rendering of an objects micro- and mesostructure (cf. Figure 1).
The key contribution of this paper is a system that
• fully automatically acquires 3D-data, capturing an object’s geometry and its visual appearance in form of bidirectional textures
• faithfully reconstructs the object’s mesostructure using
BTF-techniques and therefore effectively overcomes the
limited accuracy of the visual hull technique
• is time efficient and very easy to use.
The reasoning behind our decision to also reconstruct the
explicit geometry (compared to a purely image based and
geometry-free approach) is that in addition to efficiency reasons, having direct access to the geometrical properties appears to be a virtue in itself for various purposes, such as

Statistical Shape Analysis,
Interaction with other objects (collisions, shadows, ...)
Multimodal interaction (sound, haptics)
Modelling applications (surface editing)

The rest of this paper is organised as follows: In the following section we will shortly review the relevant literature
related to our approach presented in this paper, an overview
over which is presented in section 3. The setup of our multicamera acquisition device used to capture the required images will be described in section 4. Our modified Visual
Hull algorithm is discussed in section 5.1, whereas section
6 deals with the techniques used to record and render the
BTF equipped model.
2. Related Work
The desire to create synthetic photo-realistic images of objects under new lighting conditions and for new view points
has a long tradition in computer graphics.
Traditionally the geometry of a surface is modelled explicitly (e.g. with triangles) only up to a certain scale,
while the remaining surface properties responsible for the
reflectance behaviour of a material are simulated using relatively simple analytical bidirectional reflectance distribution
function (BRDF) models (e.g. [LFTG97]). Fine textural surface detail is typically captured by photographic images or
procedurally generated textures which are projected onto the
geometry.
Although this traditional approach has produced remarkable results for numerous examples, it becomes unfeasible
for large classes of materials where the so-called mesostructure is essential for faithfully reproducing important characteristic material properties.
Numerous researchers have proposed image-based methods to overcome these hindrances. Independently, Gortler et
al. [GGSC96] and Levoy and Hanrahan [LH96] introduced
light-field rendering, an efficiently renderable 4D representation of the plenoptic function. In their approaches, an object’s complete appearance is stored in a fourdimensional
function (called Light Field in [LH96] and Lumigraph in
[GGSC96]), that describes the flow of light at the sampled
positions in the sampled directions. While these approaches
allow photographic images – interpreted as a sampling of
the complete light field – to be interpolated to generate images from new viewpoints, the illumination has to remain
constant.
To overcome this drawback, several researchers proposed representing an object in terms of its reflectance field
[DHT∗ 00], [HCD01]. Recently, Hawkins et al. also proposed a novel, dual way of measuring an object’s reflectance
properties [HED05]. In particular [HCD01] demonstrated
remarkably realistic synthesised images of cultural heritage
artefacts under new lighting conditions, although the generation of novel viewpoints remained basic.
c The Eurographics Association 2005.

G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts 15

Figure 2: The Acquisition Pipeline: The multi-camera array records 151 images per camera per light direction, resulting in
22801 images. Here, only the first ten cameras (from left to right) and the first ten light directions (from top to bottom) are
shown. From the full set of these images, the BTF is constructed, while only a subset (the diagonal in this matrix notation) is
used for the geometry reconstruction.

In the above approaches, no explicit geometry is present,
only renderings from new view points and/or under synthesised lighting conditions are feasible. In recent years,
though, researchers have found the classical two-fold
representation of an object as geometry + material to
be superior in terms of time and memory efficiency
[WAA∗ 00][CHLG05][Pul97]. We also rely on such a twofold representation but our material representation captures
both light and view dependent appearance variation. Therefore, in concept our approach is most similar to the methods
of Furukawa et al. [FKIS02] and Lensch et al. [LKG∗ 03],
who also construct a representation that allows rendering objects from novel viewpoints under arbitrary lighting. However, as they employ laser range scanners to record the 3D
geometry of the object, still large classes of objects, in particular those with complex reflectance behaviour, can not be
handled.
In our approach we use an array of fixed cameras with
fixed light sources, all mounted on a hemispherical gantry.
Although similar acquisition setups have been used by
numerous research groups, see e.g. [MPZ∗ 02], [FKIS02],
[HCD01], [MGW01], our setup benefits from massive parallelisation and the fact that no moving parts are required
– a fact that renders tedious and time consuming tasks like
recalibration, registration, etc. unnecessary.
As in our work, the simultaneous reconstruction of geometry and appearance is also an inherent part of the numerous methods aiming at 3D-reconstruction from uncalibrated
image-sequences (e.g. [PGV∗ 04]). But these techniques are
neither designed nor capable of performing the highly accurate reflectance measurements possible with a fully computer controlled and calibrated measurement device.

3. Overview
The basic concept of our algorithm is as follows: In the acquisition phase, we record a dense set of photographs from
c The Eurographics Association 2005.

pre-defined viewpoints, regularly sampled over the hemisphere. For each viewpoint v1 , . . . , vm , we illuminate the
object from light sources likewise regularly positioned at
l1 , . . . , ln . This recording is massively parallelised (as will
be described in the following section), resulting in a set
{Ii j }i=1...m, j=1...n of m × n Images, where Ii j denotes the image taken from viewpoint vi of the object to be digitised under illumination from l j .
With these images at hand, the first step is to reconstruct
a triangulated surface representation from the object. To this
end, we apply a GPU-based modification of the classical volume carving method to transform the visual hull representation derived from the dense set of images to a volumetric
representation, from which the final triangle mesh can then
be reconstructed. For this step, we only exploit the subset
{Iii }i=1...m of the total set of images, i.e. those images where
light and viewing direction is coincident.
After parametrisation of the resulting triangle mesh,
next step is to exploit the full set of images to define
BTF. Efficient extraction, compression and storage of
BTF data is described in section 6. Figure 2 illustrates
whole process.

the
the
the
the

4. Multi-Camera Grid
For fast acquisition of the images required to measure the
BTF and to reconstruct the geometry of the object to be digitised, we use an array of 151 commodity digital still cameras
mounted on a hemispherical gantry (see Figures 3, 4). A similar gantry with mounted light sources was used by Malzbender et al. to capture Polynomial Texture Maps [MGW01].
By arranging the cameras into this array, the acquisition of
the images required to construct the BTF textures is parallelised and no moving parts (e.g. a rotating stage) are needed.
Therefore, the positions of the image sensors and the light
sources can be calibrated in a preprocessing step which only
has to be carried out if a camera has been replaced or after

16 G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts

Figure 3: The Dome - 151 cameras mounted on a hemispherical gantry.

the whole setup has been transported. The low-level postprocessing (geometric correction, colour correction) is fast
enough to be done in parallel to the measurement.

Figure 4: Detail view of the dome. Only commodity class,
off-the-shelf cameras have been used.

with a diffuse black finish to reduce scattered light effects in
the images and controlled by a network of eight commodity personal computers. As light sources, the built-in flash
lights of the cameras are used, no further light sources are
required. Formally, we therefore have n = m and roughly
vi = li for all i = 1, . . . , n in the above formulation. It is
worth noting, however, that as a consequence of the massively parallelised setup, we are able to capture a full dataset
of 151 × 151 = 22801 images in about 40 minutes.
5. Geometry Acquisition

Figure 5: Hemispherical setups – in contrast to full spheres
of cameras and light sources – produce a subsampling of the
full set of possible light and view directions.
For non-planar objects, our hemispherical setup can only
deliver a subsampling of the full set of possible light and
view directions. Figure 5 illustrates this effect. For geometry
and appearance acquisition, however, this subsampling can
easily be completed to cover the full sphere by repositioning
the object. To this end, a registration of the recorded images after repositioning with the original images is required.
To solve this task, we apply the automatic registration technique given in [BDW∗ 04]. It is worth noting, that with a full
automatic technique at hand, this repositioning approach can
also be used to deliberately increase the sampling rate, that
would otherwise be limited by the finite number of cameras
mounted on the gantry.
For our setup, we used consumer-class, off-the-shelf cameras with an image resolution of 2048 × 1536 pixels, coated

One of the major advantages of our acquisition device without any moving parts is that camera and light source positions are known a-priori, hence so are the transformations required to transfer the different views into a common, global
coordinate system. Thus, the registration of the multiple
views is straight forward.
5.1. Visual Hulls
The idea to exploit a set of registered photographs of an object to derive its three-dimensional layout is far from new
[Bau74]. Due to its conceptual simplicity and availability –
only classical 2D photographic images are required – numerous authors have developed algorithms to reconstruct the 3D
geometry of an object from images, long before 3D acquisition devices as laser range scanners have become commonly
available. In our approach, we apply a volumetric approach
in the concept of shape from silhouette [MA83].
In our current implementation, we first extract the objects silhouettes from the acquired 2D photographs by simple thresholding. As we use no backdrop in our setup, we set
every pixel with a brightness of less then a certain threshold to be outside, the remaining pixel are inside (see Figure
c The Eurographics Association 2005.

G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts 17

Figure 6: One example image ∈ {Iii }i=1,...,n (for which light and view direction are coincident) and the corresponding binary
inside/outside-image after thresholding

6). In this step, we only exploit a subset of the set of all acquired images, namely those images where light and view
directions are identical, i.e. {Iii }i=1,...,n , resulting in a set of
binary images {Ji }i=1,...,n .
Together with the viewpoint information known from our
setup, every outside-pixel in each image now defines a ray in
scene space that is known not to intersect the object, whereas
the inside-pixel define rays that intersect the surface at some
unknown distance to the viewpoint. In the continuous case
(pixel width → 0) the union of all these intersecting rays
would define a generalised cone that is guaranteed to contain the object.
As this fact holds for all acquired images, the intersection of all these generalised cones (the Visual Hull,
[Lau94][MBR∗ 00]) describes a tight volume in space in
which the complete object must lie.
We make use of this guarantee by applying volume carving to a regular grid, i.e. we (conceptually) traverse the grid
following the outside-rays and mark any cell that is encountered during traversal as empty. The triangle mesh is then reconstructed using the well-known marching cubes approach
[LC87].

5.2. Efficient Evaluation on the GPU

direction from viewpoint vi onto the object, and let (without loss of generality) ex be the direction such that the scalar
product |he, vi i| is maximal.
We interpret the grid to be a stack of binary 2D textures
{Tk }k=1,...,X , where each Tk has a resolution of Y × Z pixels, see Figure 7. The inside/outside-information is then efficiently collected by projecting every source image Ji to each
texture Tk . We perform bitwise AND-operations during this
projective texture mapping, to set a pixel in Fk to 0 if at least
one Ji indicates so.

6. Bidirectional Texture Functions
The image-based approach to capturing the appearance of
an object for later rendering is to take dense sets of images
under controlled viewing and lighting conditions in order
to sample its reflectance field appropriately. As mentioned,
Malzbender et al. [MGW01] captured the lighting variability of a texture by taking images lit from different directions
and compressed the data to a compact representation called
Polynomial Texture Maps. As we also want to be able to vary
the viewing direction, we rely on BTFs. Mathematically the
BTF can be expressed as a measured 6D-slice of the general
8D-reflectance field

The large number of acquired images and the (potential)
need for finer grids make it impractical to actually traverse the grid following the outside-rays. Instead, we use
a hardware-supported approach based on projective texture
mapping, that we will shortly describe here for completeness.

parameterised over a base surface S:

Suppose, we have an axis-parallel grid of dimension X ×
Y × Z, corresponding to the axis directions ex , ey , and ez , respectively. For reconstruction, we have to assign to each grid
point 0 (for outside) or 1 (for inside). Let vi be the viewing

Please note that fixing the position x restricts the BTF at this
position to a 4-dimensional BRDF, which is often called apparent BRDF, because this function still contains local selfocclusions, scattering, etc. [MMS∗ 05].

c The Eurographics Association 2005.

RFrgb (xi → x0 , ωi → ω0 )

BTFrgb (x, ωi → ω0 ) =

Z
S

RFrgb (xi → x, ωi → ω0 )dxi

18 G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts

Figure 7: The 3D grid is interpreted as set of binary images.

6.1. Compression
One major issue of high fidelity BTF measurements are the
huge memory requirements. A raw BTF data set easily requires multiple Gigabytes of memory, requirements that cannot be fulfilled even by the most state-of-the-art graphics
hardware. Therefore, BTF data is usually compressed either
by fitting standard BRDF models to the data or by using statistical analysis methods, such as principal component analysis (PCA) or clustered PCA. The latter is used in our current
implementation, and the resulting data can easily be decompressed and evaluated using graphics hardware [MMK04].
Statistical analysis, however, requires that data entries in
the BTF are semantically correspondent – an assumption
that holds for the raw data only under the assumptions of planarity, orthographic projection and directional light sources.
This is not the case for our system since the dimensions of
our acquisition setup cannot be considered "large" compared
to the dimensions of the objects to be digitised. Therefore,
we perform resampling of the raw BTF data based on a planar parameterisation of the reconstructed triangle mesh computed with the method of Degener et al. [DMK03] before
compression.
6.2. Incomplete BTF data for non-planar objects
To capture the reflectance of a material independently of
the specific geometric layout, most common approaches for
BTF acquisition record images of a small planar material
sample. Then a projection of the images on a common plane
typically suffices. For digitising 3D cultural heritage artefacts, though, we cannot rely on planarity. Hence, we have
to deal with effects like self-shadowing and occlusion.
Measuring BTFs generally consists of recording for every
point on the surface its reflectance from each view direction under each light direction. For non-flat surfaces, how-

ever, the reflectance for some light and viewing directions
will be zero (or close to) simply because of occlusion and/or
self-shadowing. Using standard approaches, this missing information would be misinterpreted as a property of the material. Instead, we apply a different technique to interpolate
the occluded data.

Figure 8: Polar plot illustration of the measured BTF, that
might be incomplete for some directions Φ due to occlusion
and has to be completed, e.g. using statistical analysis methods.
We first identify from all the points on the object surface
those points that have an incomplete BTF measurement, i.e.
points which are occluded for at least one light source or
camera position. Figure 8 illustrates this situation.
For these points, the BTF has to be completed. In our
current implementation, we first perform statistical analysis, here LPCA, of the measured BTF of all object surface
points for which this is complete. The eigenvectors to the
largest eigenvalues of the corresponding covariance matrices
span a lower dimensional subspace approximating the original measured data. This way each individual bidirectional
reflection function BTFrgb (x, ωi → ω0 ) in a surface point x
can then be approximated by
BTFrgb (x, ωi → ω0 ) = ∑ ck uk ,
c The Eurographics Association 2005.

G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts 19

where the uk are the basis (apparent) BRDFs and the ck the
corresponding weights.
The incomplete BRDFs are completed individually, depending on the specific values that are missing. Let π be the
projection of a full BRDF-vector BTF(x) to a lower dimensional subspace that only contains the present measurements
and neglects the missing values. Then we find a set of coefficients {ck }k=1,...,K such that
||π(BTF(x)) − ∑ ck π(uk )||

References
[Bau74] BAUMGART B. G.: Geometric modeling for computer
vision. Technical Report AIM-249, Artificial Intelligence Lab,
Stanford University, October 1974.
[BDW∗ 04] B ENDELS G. H., D EGENER P., WAHL R., KÖRTGEN
M., K LEIN R.: Image-based registration of 3d-range data using
feature surface elements. In 5th Int. Symp. on Virtual Reality,
Archaeology and Cultural Heritage (VAST) (December 2004).
[CHLG05] C OOMBE G., H ANTAK C., L ASTRA A.,
G RZESZCZUK R.:
Online reconstruction of surface light
fields. In Eurographics Symposium on Rendering (June 2005).

is minimal. The reconstructed vector ∑ ck uk is a reasonable
completion of BTF(x) and hence used in our approach. For
complex materials this process can be iterated, while taking
into account the already completed vectors.

[DHT∗ 00] D EBEVEC P., H AWKINS T., T CHOU C., D UIKER H.P., S AROKIN W., S AGAR M.: Acquiring the reflectance field of a
human face. In SIGGRAPH ’00: Proceedings of the 27th annual
conference on Computer graphics and interactive techniques
(New York, NY, USA, 2000), ACM Press/Addison-Wesley Publishing Co., pp. 145–156.

7. Results & Conclusions

[DMK03] D EGENER P., M ESETH J., K LEIN R.: An adaptable
surface parametrization method. In The 12th International Meshing Roundtable 2003 (September 2003).

Our approach to reconstruct the geometry from the acquired
images using visual hulls computed on the GPU is reliable
and fast. Of course, identifying a nonconvex object using a
silhouette-based approach inherently and inevitably implies
neglecting some features of its surface geometry. Despite
this general seemingly inaptness of the visual hull reconstruction, we were able to produce realistic images of our
captured objects because the neglected surface features are
well-captured in their appearance using the BTF texturing
techniques. Figure 9 demonstrates this effect. The defect on
the front size of this echinite from the Goldberg collection
is not detected by the visual hull algorithm and therefore not
represented in the geometric reconstruction. Its appearance
is nevertheless preserved.
In order to use this appearance preservation such that
users can explore an object and detect little concavities as
depicted in Figure 9, it is important that users can arbitrarily choose viewing and lighting directions, which is possible
using our system.

[DNvGK97] DANA K., NAYAR S., VAN G INNEKEN B., KOEN DERINK J.: Reflectance and texture of real-world surfaces. IEEE
Computer Society Conference on Computer Vision and Pattern
Recognition, Proceedings (1997), 151–157.
[FKIS02] F URUKAWA R., K AWASAKI H., I KEUCHI K.,
S AKAUCHI M.: Appearance based object modeling using
texture database: acquisition, compression and rendering. In
EGRW ’02: Proceedings of the 13th Eurographics workshop
on Rendering (Aire-la-Ville, Switzerland, Switzerland, 2002),
Eurographics Association, pp. 257–266.
[FLP05] F URUKAWA Y., L AZEBNIK S., P ONCE J.: Carved visual hulls for high-accuracy image-based modeling. In Siggraph
Sketches (2005).
[GGSC96] G ORTLER S. J., G RZESZCZUK R., S ZELISKI R., C O HEN M. F.: The lumigraph. In SIGGRAPH ’96: Proceedings of
the 23rd annual conference on Computer graphics and interactive techniques (New York, NY, USA, 1996), ACM Press, pp. 43–
54.
[GSD03] G RAUMAN K., S HAKHNAROVICH G., DARRELL T.: A
bayesian approach to image-based visual hull reconstruction. In
Proceedings of IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (June 2003), pp. 187–194.

To further improve the geometric reconstruction, any of
the numerous extensions of the visual hull reconstruction algorithm as e.g. presented in [IS03], [SCM∗ 04], [LMS03a],
[LMS03b], or [GSD03] can naturally be incorporated into
our system. A promising direction for future work is also the
incorporation of reconstruction techniques based on photometric correspondences as proposed in [FLP05].

[HCD01] H AWKINS T., C OHEN J., D EBEVEC P.: A photometric approach to digitizing cultural artifacts. In VAST ’01: Proceedings of the 2001 conference on Virtual reality, archeology,
and cultural heritage (New York, NY, USA, 2001), ACM Press,
pp. 333–342.

8. Acknowledgements

[HED05] H AWKINS T., E INARSSON P., D EBEVEC P.: A dual
light stage. In Eurographics Symposium on Rendering (June
2005).

This work was partially funded by the European Union under the project RealReflect (IST-2001-34744). Some of the
used high-dynamic range lighting environments were taken
from Paul Debevecs webpage. Special thanks belong to Ralf
Sarlette for allowing us to use the acquisition dome and Dirk
Koch for invaluable help with the acquisition.
c The Eurographics Association 2005.

[IS03] I SIDORO J., S CLAROFF S.: Stochastic refinement of the
visual hull to satisfy photometric and silhouette consistency constraints. In Proc. 9th International Conference of Computer Vision (Nice, France, October 2003), pp. 1335–1342.
[Lau94] L AURENTINI A.: The visual hull concept for silhouettebased image understanding. IEEE Trans. Pattern Anal. Mach.
Intell. 16, 2 (1994), 150–162.

20 G. Müller & G. H. Bendels & R. Klein / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts

Figure 9: Raytraced renderings of a captured and reconstructed echinite under novel lighting and viewing conditions. The
left and middle image are rendered with a small area light source and demonstrate the fine geometric details captured in the
BTF. The right image shows a relighting of the echinite with a complex image-based lighting environment captured in front of
the Pädagogische Fakultät in Bonn. The ground floor in the right image is covered with a synthetic leather BTF courtesy of
DaimlerChrysler AG.

[LC87] L ORENSEN W. E., C LINE H. E.: Marching cubes: A
high resolution 3d surface construction algorithm. In SIGGRAPH
’87: Proceedings of the 14th annual conference on Computer
graphics and interactive techniques (New York, NY, USA, 1987),
ACM Press, pp. 163–169.
[LFTG97] L AFORTUNE E. P. F., F OO S.-C., T ORRANCE K. E.,
G REENBERG D. P.: Non-linear approximation of reflectance
functions. In SIGGRAPH ’97: Proceedings of the 24th annual
conference on Computer graphics and interactive techniques
(New York, NY, USA, 1997), ACM Press/Addison-Wesley Publishing Co., pp. 117–126.
[LH96] L EVOY M., H ANRAHAN P.: Light field rendering. In
SIGGRAPH ’96: Proceedings of the 23rd annual conference on
Computer graphics and interactive techniques (New York, NY,
USA, 1996), ACM Press, pp. 31–42.
[LKG∗ 03] L ENSCH H. P. A., K AUTZ J., G OESELE M., H EI DRICH W., S EIDEL H.-P.: Image-based reconstruction of spatial appearance and geometric detail. ACM Trans. Graph. 22, 2
(2003), 234–257.
[LMS03a] L I M., M AGNOR M., S EIDEL H.-P.: Hardwareaccelerated visual hull reconstruction and rendering. In In Proceedings of GIŠ03 (Halifax, Canada, 2003).
[LMS03b] L I M., M AGNOR M., S EIDEL H.-P.: Improved
hardware-accelerated visual hull rendering. In Vision, Modeling,
and Visualization 2003 (Germany, 2003).
[MA83] M ARTIN W. N., AGGARWAL J. K.: Volumetric description of objects from multiple views. IEEE Trans. Pattern Analysis
and Machine Intelligence 5, 2 (1983), 150–158.
[MBR∗ 00] M ATUSIK W., B UEHLER C., R ASKAR R., G ORTLER
S. J., M C M ILLAN L.: Image-based visual hulls. In SIGGRAPH
’00: Proceedings of the 27th annual conference on Computer
graphics and interactive techniques (New York, NY, USA, 2000),
ACM Press/Addison-Wesley Publishing Co., pp. 369–374.

[MMK04] M ÜLLER G., M ESETH J., K LEIN R.: Fast environmental lighting for local-pca encoded btfs. In Computer Graphics International 2004 (CGI 2004) (June 2004), IEEE Computer
Society, pp. 198–205.
[MMS∗ 05] M ÜLLER G., M ESETH J., S ATTLER M., S ARLETTE
R., K LEIN R.: Acquisition, synthesis and rendering of bidirectional texture functions. Computer Graphics forum 24, 1 (March
2005), 83–109.
[MPZ∗ 02] M ATUSIK W., P FISTER H., Z IEGLER R., N GAN A.,
M C M ILLAN L.: Acquisition and rendering of transparent and
refractive objects. In EGRW ’02: Proceedings of the 13th Eurographics workshop on Rendering (Aire-la-Ville, Switzerland,
Switzerland, 2002), Eurographics Association, pp. 267–278.
[PGV∗ 04] P OLLEFEYS M., G OOL L. V., V ERGAUWEN M.,
V ERBIEST F., C ORNELIS K., T OPS J., KOCH R.: Visual modeling with a hand-held camera. International Journal of Computer
Vision 59, 3 (2004), 207–232.
[Pul97] P ULLI K. A.: Surface reconstruction and display from
range and color data. PhD thesis, 1997. Chairperson-Linda G.
Shapiro.
[SCM∗ 04] S LABAUGH G. G., C ULBERTSON W. B., M ALZBEN DER T., S TEVENS M. R., S CHAFER R. W.: Methods for volumetric reconstruction of visual scenes. Int. J. Comput. Vision 57,
3 (2004), 179–199.
[WAA∗ 00] W OOD D. N., A ZUMA D. I., A LDINGER K., C UR LESS B., D UCHAMP T., S ALESIN D. H., S TUETZLE W.: Surface light fields for 3d photography. In SIGGRAPH ’00: Proceedings of the 27th annual conference on Computer graphics
and interactive techniques (New York, NY, USA, 2000), ACM
Press/Addison-Wesley Publishing Co., pp. 287–296.

[MGW01] M ALZBENDER T., G ELB D., W OLTERS H.: Polynomial texture maps. In SIGGRAPH ’01: Proceedings of the 28th
annual conference on Computer graphics and interactive techniques (New York, NY, USA, 2001), ACM Press, pp. 519–528.
c The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Passive reconstruction of high quality textured 3D
models of works of art
N. Brusco, L. Ballan and G. M. Cortelazzo
Universitá degli Studi di Padova, Dipartimento di Ingegneria dell’Informazione

Abstract
A wide-spread use of 3D models in cultural heritage application requires low cost equipment and simple
modeling procedures. In this context, passive 3D reconstruction methods allow to build 3D models
from a set of calibrated cameras, without the need of expensive machinery. Unfortunately the surfaces
characteristics often lead to bad quality reconstructions. Recent efforts attempt to combine together
information from different passive methods in order to improve the overall quality of the result. The
combination of stereo matching and silhouette information has recently received considerable attention.
Typically the major contribution to the appearance of the model comes from texture, rather than from
geometry. The straightforward application of the photographs over the model can lead to artifacts,
due to errors in 3D reconstructions, which must be minimized. This work, building on recent results,
proposes a variation of an algorithm for 3D geometry recovery from stereo and silhouette information
within a classical deformable model framework, which improves the quality of the shape. In order to
avoid visible texture artifacts, it also proposes a new algorithm for texture synthesis based on wavelet
decomposition. Experimental verification shows the effectiveness of the proposed solution with respect
to robustness, computational speed and quality of the final result.

1. Introduction
In the scientific community the interest for 3D modeling of cultural heritage’s objects such as vases and
statues is gaining interest. The documentation of objects should not just concern their geometrical characteristics but also their color, since usually texture
gives the main contribution to the visual appearance
of an object [CB04].
Methods for recovering the 3D geometry of objects
can be divided into passive and active ones. Passive
sensing refers to the measurement of visible radiation
which is already present in the scene; active sensing
refers instead to the projection of structured light patterns onto the object or scene to be modeled.
Active sensing facilitates the computation of 3D
structure by intrinsically solving the correspondence
problem, a major issue with passive techniques. In
general, active techniques such as those based on laser
range scanning or light pattern projection tend to be
more accurate but more expensive and slower than
c The Eurographics Association 2005.


their passive counterparts. Furthermore, when it is
more important to realize photorealistic 3D models
than metrologically accurate surfaces, passive 3D reconstruction methods, coupled with good quality textures, can lead to good results.
For these reasons and since passive techniques essentially require standard image capture devices such as
photo-cameras or video-cameras, the interest towards
passive 3D reconstruction techniques is bound to remain rather high. Historical passive sensing methods
are stereo vision, structure from motion, shape from
shading, shape from silhouette and space carving.
Recent efforts attempt to combine together informations of different passive methods. Critical issues in
this research are what type of data to use and how to
combine them, in order to actually increase the overall information. The combination of stereo matching
and silhouette information has recently received considerable attention both for obtaining high quality 3D
models [ES04] and for modelling 3D dynamic scenes

22

N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art

[MWTN04], an application often referred to as 3D
video.
This work addresses this approach within the classical deformable models framework and proposes a solution which has some advantages in order to obtain
a high quality 3D model.
After obtaining a 3D model, it is necessary to map
textures over it in order to obtain a photorealistic
result. Textures are available as a collection of images, which often are the same images used for 3D reconstruction. Mapping of such images over the model
could lead to artifacts, which are more evident when
there are errors in the 3D reconstruction, as it is
the case of low precision techniques as typical passive
methods. Such artifacts usually correspond to blurring
of high frequencies spatial features or in visible discontinuities at the boundaries between adjacent triangles,
which are taken from different original source images.
This work presents an original texture processing
method based on wavelet analysis for obtaining high
quality textured 3D models.
This paper has six sections. The second section
recalls the 3D passive recovery from stereo and silhouette information and points out the most delicate issues. Sections 3 reformulates the problem within
classical deformable models framework, defines a new
force related to silhouette information, proves some
theoretical advantages of the proposed reformulation
and shows how to solve it. Section 4 addresses a fine
grain improvement leading to a re-sampling more respectful of the geometrical quality of the mesh. Section
5 describes the method for the creation of a photorealistic texture for the 3D model. Section 6 presents some
experimental results. Section 7 draws the conclusions.
2. Shape recovery from stereo and silhouette
information
The proposed 3D passive shape recovery procedure
combines silhouette and stereo-matching information
as schematically shown in Fig.1. The silhouettes are
obtained by a segmentation algorithm [Can86] [LM01]
from a sequence of photographs of the object taken
from different positions depending on the characteristics of the scene. Stereo matching is also applied to
the picture pairs of the sequence of photographs if the
object is textured. Not all the pictures used for the
silhouettes methods are used for stereo matching.
Silhouettes are first used by a shape-from-silhouette
method [MFK99] [Lau94] [Pot87] in order to obtain a
coarse estimate of the surface; they are then used in
order to correct for stereo-matching errors. The main
advantages of shape-from-silhouette methods are that

Figure 1: The proposed passive 3D modeling pipeline.

the obtained objects are well shaped and there are no
problems with reflecting objects or objects without
texture (if the segmentation algorithm is robust). The
major drawback is that concavities cannot be modelled.
Texture is used by stereo matching methods [Kos93]
[KSK98] which, differently from silhouette based techniques, can model concavities. Stereo-matching does
not work in regions without significant texture or
where the available texture exhibits some periodicity. 3D data near the silhouette edge are usually
missing, since in these regions the object points can
be easily mismatched with the background. Luckily,
shape-from-silhouette methods can model these regions rather well.
Assuming the above 3D reconstruction process provides us with the coordinates of n points lying on a real
surface Λ, the points could be expressed as xi = yi +εi ,
where yi are the true values and εi are the measurement errors. We also have m views Vj of Λ, which can
be considered as functions mapping <3 in <2 through
projective transformations. For each view Vj we know
the projection Pj = Vj (Λ) of the original surface Λ,
i.e. the set of points representing the silhouettes of Λ.
The fusion problem of silhouettes and stereo matching information concerns the estimate of Λ from xi
and Pj .
Such a problem can be solved within a classical deformable model framework [ES04] [MWTN04].
Namely, a surface is made to evolve subject to three
types of forces, an internal and two external ones. The
first one, Fint , keeps the surface as smooth as possible,
while the others, Ftex and Fsil , make it to converge to
Λ. Formally, the evolution of the model at point P can
be described as:
s (0) = s0

(1)

∂s
(t) (P ) = Fint (P, s) + Ftex (P ) + Fsil (P, s)
∂t

(2)

where s (t) is the estimate of Λ at iteration t, s0 is the
estimate obtained through the shape-from-silhouette
c The Eurographics Association 2005.


N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art

23

method, and
Fint (P, s) = ∇2 s (P ) − ∇4 s (P )

(3)

Ftex deforms the model in order to minimize its distance from cloud xi ; Fsil deforms the model in order
to make it consistent with silhouette information i.e.,
Fsil tends to make the model silhouettes as similar as
possible to the acquired ones.
In [ES04] Ftex is expressed as the Gradient Vector Flow (GVF) [XP98], obtained from point cloud xi
with the aim of eliminating the local minima arising
when the surface reaches a boundary concavity.
As for Fsil , in [MWTN04] it is defined as follows:
Fsil (P ) =

m
X

f Vj (P )

(4)

j=1

where f Vj (P ) is nonzero iff Vj (P ) is external to Pj or
is internal to Pj and on the boundary of Vj (s) (that
is, Vj (s) is the silhouette of s viewed from Vj ). In this
case f Vj (P ) is the back-projection of the 2D vector
joining Vj (P ) with the nearest point on Vj (s) boundary. Hence the force is nonzero only along the curves
obtained by sectioning s with the image planes relative
to Vj and passing through P . The force field is therefore strongly discontinuous and can’t have variational
origin, i.e., it can’t be derived from the Euler-Lagrange
equations of a minimum problem. As a consequence,
convergence to a model consistent with silhouette information is not guaranteed. Moreover, the force is
calculated as the sum of terms separately computed
on each silhouette.
In [ES04] Fsil is defined as
Fsil (P ) = α (P ) · dvh (P ) · n (P )

(5)

where n (P ) is the normal to the surface in P and
dvh (P ) is the signed distance from the visual hull defined as dvh (P ) = minj d (Vj (P ) , ∂Pj ), where ∂Pj is
the boundary of Pj and d (Vj (P ) , ∂Pj ) is the signed
distance between the projection v of P viewed from Vj
and ∂Pj , positive when v ∈ Pj and otherwise negative.
α (P ) can be expressed as
(
1
⇐⇒ dvh (P ) 6 0
α (P ) =
1
⇐⇒
dvh (P ) > 0
k
(1+d(V (P ),∂V (s)))
c

c

(6)
where c = arg minj d (Vj (P ) , ∂Pj ) and ∂Vc (s) is the
boundary of the silhouette of s viewed from Vc . The
force field is continuous, defined over the entire model
with the same direction and versus as the surface normal. Namely, the boundary points are subject to a
force equal (both in magnitude and versus) to the distance vector from the visual hull. For the points internal to Vj (s) the force intensity is modulated by α (P )
so as to be inversely proportional to their distance
c The Eurographics Association 2005.


Figure 2: The Buddha.

from the boundary. In this way, the vertices can leave
the visual hull and enter the concavity opposing only a
force decreasing by α (P ) with respect to the distance
from the boundary. Eq.(5) avoids the difficulties arising from the sum of Eq.(4).
For the above reasons we have chosen to use Fsil
introduced in [ES04].
3. Refinement
The results obtained by the method proposed in section 2 are rather good with respect to parametric and
geometric quality, as shown by Fig.3.
However, with this method, the final model is bound
to have a parametrization similar to an isometry, i.e.,
to be uniformly sampled (see Fig.4). Namely, this is
due to the use of Fsil . However, a good quality mesh
should be sampled proportionally to its local curvature while in deformable models the sampling rate is
fixed. In this case some regions could be undersampled
and others oversampled, with consequent poor mesh
quality or too large model sizes, respectively.
In order to solve this problem we defined a second phase of evolution, to be started after the model
reaches adequate convergence to the original object.
A selective subdivision of the model based on local
curvature is first needed. Then, intuitively, we could
think of evolving the model by Eq.(2). Unfortunately,
as shown in Fig.5, this would not result in a correct
evolution because Fsil is not correctly related to the
curvature of a multiresolution mesh, which is our case.
Therefore, we choose to use another type of silhou-

24

N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art

Figure 6: The resolution enhancement allows us to
capture details of the original model in high curvature regions. Left: model obtained using classical deformable models. Right: model obtained using variable
sampling rate.

ette force

Figure 3: 3D Model of Buddha reconstructed using
snakes.

Figure 4: Detail of Buddha wireframe.

0
(P, s) = −κn
Fint

(7)

where κ represents the mean curvature of the surface and n is the surface normal. As shown in Fig.5,
0
is independent of the chosen parametrization. UnFint
0
is hard to compute and we refer to
fortunately Fint
[DMSB99] for a numerical implementation.
In this way only the surface geometrical properties are changed, while the parametrization chosen
through subdivision is untouched. We thus obtain a
sampling which respects compactness and geometrical quality of the mesh. The resolution enhancement
allows us to capture details of the original model in
high curvature regions (see Fig.6). In the formulation
of [ES04] this would have been impossible, as it would
imply a prohibitive model size.
4. Texture
We assume that images V i , i = 0...N − 1 of the object are available, taken by a pre-calibrated camera. A
new texture V ∗ for the full 3D model may be obtained
through projection of textures V i and a weighted average [ABC04] [ABC03]. Unfortunately, any imperfection in the surface geometry description produces misalignment the separate projections, so that averaging
tends to blur high frequency spatial features.

0
Figure 5: Comparison between Fint and Fint
.

Another solution consists in keeping the original images V i and selecting a single most appropriate original view image for each triangle. Although stitching
avoids the blurring problem, it tends to produce visible discontinuities at the boundaries between adjacent
triangles which are mapped on different original source
views.
To hide these artifacts we could form each possic The Eurographics Association 2005.


N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art
V

W

0

0

V

...

0

i

...

V

N

V

Warping

(V )

j*

DWT Synthesis

DWT Analysis

Figure 7: Generation of V j∗ .
V*

V

0*

from the viewpoint of both image V i and V j . In this
case both images have a planar triangular patch imaging ∆n , which we will denote as V i (∆n ) and V j (∆n )
respectively. The relationship between such two planar patches is an affine warping. Call Wij the warping operator which transforms image V i in the image
Wij (V i ). This operator warps every V i (∆n ) into the
planar triangular patch corresponding to the triangle
of geometry ∆n projected on V j .
The texture generation procedure, which we call
DWT stitching, has the following steps:

Triangles
selection

Triangles
selection

25

...

j
V *

...

V

N*

DWT Synthesis

Figure 8: Generation of the global texture V ∗ .

ble rendering V ∗ in real time in the discrete wavelet
transform or DWT domain as described in [ZBTC05].
Since rendering engines do not cope with the distortion framework of [ZBTC05], here we prefer to produce a texture statically connected with the geometry, which can be easily rendered with any commercial
software.
Our problem is that of obtaining a global multispectral texture V ∗ from N images V i , i = 0, 1, ..., N − 1.
The registration of each texture on the mesh of triangles induces on each image a subdivision in planar triangular patches. Consider surface triangle ∆n and assume that the geometry portion including ∆n is visible
c The Eurographics Association 2005.


1. For each image V j , j = 0...N − 1, warp each image
V i , i = 0...N −1 to obtain the set of N 2 warped image Wij (V i ), as pictorially shown in the left upper
part of Fig. 7.
2. apply DWT analysis on the set of N 2 images
Wij (V i ), i, j = 0...N − 1 over the set of subbands,
LLid and LHid , HLid , HHid for d = 1, 2, . . . , D, where
D is the number of DWT decomposition levels, as
pictorially shown in the left part of Fig. 7.
3. for each V j∗ , j = 0...N − 1 , build the DWT coefficients from those of the images Wij (V i ), i = 0...N −
1 by assigning to each triangle V j∗ (∆n ) of every
subband the DWT coefficients of that subband associated to only one source image Wij (V i (∆n )), i =
0...N − 1. This source image of index i is selected
as the one with normal more parallel to that of the
surface triangle ∆n . This step is pictorially shown
by the lower part of Fig. 7.
4. recover images V j∗ , j = 0, ..., N − 1 from their coefficients by DWT synthesis, as pictorially shown by
the right part of Fig. 7.
5. generate the global texture by projecting on each
triangle ∆n of the mesh the V j∗ (∆n ) with the same
criteria used before for the choice of triangles, as
pictorially shown in Fig. 8.
Note that images V j∗ are modified versions of V j
consistent with geometry. Blurring is avoided, since
DWT synthesis maintains high frequency details. Furthermore discontinuities are smoothed. In the final
model, each triangle ∆n of the geometry is taken from
the most parallel view V j∗ , and the wavelet coefficients of V j∗ (∆n ) come from the ones of V j (∆n ), since
the choice of triangles is the same in points 3 and 5.
However, the portion of image V j∗ (∆n ) is different
from V j (∆n ) because the contribution of adjacent triangles on the boundaries smooths the transition between different V j∗ . An example of V j and V j∗ is
shown in Fig.9. Details of Fig.9c) and Fig.9d) show
the little but important texture variations.
The final texture is made of V j∗ , j = 0...N − 1,
and of a mapping from each triangle ∆n and a corresponding V j∗ . The final textured model of buddha is
shown in Fig.10. Benefits of DWT are shown in Fig.11:

26

N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art

(a)
(a)

(b)

(c)

(d)

(b)

Figure 11: back of Buddha a) without DWT stitching
b) with DWT stitching.

Figure 9: The Buddha: a) V j and b) V j∗ c) detail of
V j d) detail of V j∗ .

Figure 12: Synthetic model used to evaluate the reconstruction error of our system.

rendering was supervisioned by a script which also calculated the relative projection matrices.

Figure 10: The textured Buddha

in Fig.11a) stitching without DWT is applied, and a
discontinuity is evident. In Fig.11b), thanks to DWT,
the transition in very smooth.
5. Experimental results
Tests for the shape reconstruction were first performed
on synthetic models. The pictures of the synthetic
models were generated by a rendering software. The
model was framed by n 43mm target cameras. The

Differently from stereo-based procedures, our
method produces closed surfaces which are manifolds and which can be shown to be both geometrically and parametrically regular. Differently from
shape-from-silhouette procedures, our method can accurately model concavities and produce a model closer
to the real object shape. In Fig.3 the model is shown,
obtained through snakes.
Moreover, we observed that silhouette information
compensates for the lack of texture information and
enhances the level of details in regions where texture
is present. This property comes from the fact that intrinsic error of silhouette information is remarkably
smaller than texture error. We also found that silhouette information corrects matching errors.
Reconstruction error was evaluated on a synthetic
120x200x260 mm3 model shown in Fig.12. The model
was acquired at 50cm distance with a 1024x768 pixel
spatial resolution and a field of view of 35◦ .
Having both the original and the reconstructed
c The Eurographics Association 2005.


N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art

model, we finally estimated the average and maximum
distance between the two surfaces. In our case we obtained daverage = 0, 82 mm (0, 2 on the diameter) with
a 0, 62 mm2 variance and dmax = 9, 6mm.
Test was performed over the Buddha of Fig.2. The
acquisition system consisted in a rotating table and
a fixed camera. This choice is due to the simplicity
and the automation of the acquiring set-up, although
an hand-held camera with a calibration system can
be employed. The object was positioned on the table while the software controlled table rotation and
picture shooting. Sixty images were taken, spaced 6
degrees apart, for the construction of the 3D model.
Texture has been obtained from six images, spaced 60
degrees apart, and applied to the model, which can be
seen in Fig.10. 3D reconstruction, with the two-stages
approach described above, is nice, even in presence of
some reflections over the model surface. Because of an
intrinsic error in 3D reconstruction, due to concavities
not well modelled because of a lack of texture, some
artifacts inevitably appear, but the overall result is
definitely good. High frequency details are maintained
and some artifacts due to illumination or error in geometry acquisition disappear, as shown in Fig.11.
6. Conclusions
The main contributions of this paper can be summarized as follows: the definition of a second stage of
evolution in the silhouette-stereo fusion framework of
[ES04] in order to obtain good quality meshes; an algorithm for the construction of a photorealistic texture
which minimizes artifacts, based on wavelet decomposition. More specifically this paper reformulates a 3D
passive multimodal reconstruction digitization scheme
using both texture and silhouette information which
synergically combines the typical properties of both
passive techniques. Indeed, it can be proved to be resilient to measurement errors and capable of reconstructing a remarkably wide range of objects which
includes:
• Surfaces characterized by good quality texture, sufficient lighting and not too high a specular reflectance (we recall that stereo-based methods completely fail to acquire objects even with minimal
specular reflectance);
• Specular surfaces without texture or with a periodical texture, provided that the pictures take the profile of such surfaces (information in this case comes
from the silhouettes);
• Concavities characterized by good texture and sufficient lighting.
The proposed method still doesn’t allow the reconstruction of reflecting or transparent regions, nor the
c The Eurographics Association 2005.


27

modelling of objects not exhibiting the above mentioned features.
Indeed, the proposed approach gives a closed regular manifold with a regular parametrization, unlike
stereo-based methods where neither the manifold nor
the closure hypothesis generally hold. Therefore the
proposed approach leads to a feasible minimum problem with respect to mesh quality.
Furthermore, the reconstruction error is rather satisfactory. For instance, the surface reconstructed from
1024x768 pictures taken from 50cm distance is affected
by an average error of 0,8mm. Such an error can be
remarkably reduced using digital cameras of higher
resolution.
We also presented an algorithm for texturing the 3D
model, in order to obtain a photorealistic result, coherent with the kind of errors typical of passive methods. The proposed method avoids most of the artifacts
usually present in textured 3D models.
Further research will concern the combination of
other passive methods together with silhouettes and
stereo and the reformulation of the silhouette-stereo
fusion framework of [ES04]. Current work attempts
to incorporate in the method the shadow-carving
[SRBP01].
References
[ABC03] Andreetto M., Brusco N., Cortelazzo G. M.: Color equalization of 3d textured surfaces. In Proceedings of Eurographics 2003 (2003),
pp. 168–173.
[ABC04] Andreetto M., Brusco N., Cortelazzo G. M.: Automatic 3d modeling of textured
cultural heritage objects. IEEE Trans. on Image
Processing 13, 3 (March 2004).
[Can86] Canny J.: Computational approach to edge
detection. PAMI 8 (1986), 679–698.
[CB04] Cheng I., Basu A.:. In Reliability and
Judging Fatigue Reduction in 3D Perceptual Quality
(September 2004).
[DMSB99] Desbrun M., Meyer M., Schroder
P., Barr A. H.: Implicit fairing of irregular meshes
using diffusion and curvature flow. International
Conference on Computer Graphics and Interactive
Techniques (1999), 317–324.
[ES04] Esteban C. H., Schmitt F.: Silhouette and
stereo fusion for 3d object modeling. Computer Vision and Image Understanding 96, 3 (2004), 367–
392.
[Kos93] Koschan A.: Dense stereo correspondence
using polychromatic block matching. In Proc. of the

28

N.Brusco & L. Ballan & G.M.Cortelazzo / Pass. rec. of high quality textured 3D models of works of art

5th Int. Conf. on Computer Analysis of Images and
Patterns, Budapest, Hungary (1993), pp. 538–542.
[KSK98] Klette R., Schlns K., Koschan A.:
Three-Dimensional Data from Images. Springer,
Singapore, 1998.
[Lau94] Laurentini A.: The visual hull concept for
silhouette based image understanding. IEEE PAMI
16, 2 (1994), 150–162.
[LM01] Lucchese L., Mitra S. K.: Color image
segmentation: A state of the art approach. Proc. of
the Indian National Science Academy 67, 2 (march
2001), 207–221.
[MFK99] Matsumoto Y., Fujimura K., Kitamura T.: Shape-from-silhouette/stereo and its application to 3-d digitizer. Proceedings of Discrete
Geometry for Computing Imagery (1999), 177–190.
[MWTN04] Matsuyama T., Wu X., Takai T.,
Nobuhara S.: Real-time 3d shape reconstruction,
dynamic 3d mesh deformation, and high fidelity visualiziation for 3d video. Computer Vision and Image Understanding 96, 3 (2004), 393–434.
[Pot87] Potmesil M.: Generating octree models of
3d objects from their silhouettes in a sequence of images. Computer Vision, Graphics, and Image Processing 40 (1987), 1–29.
[SRBP01] Savarese S., Rushmeier H. E.,
Bernardini F., Perona P.: Shadow carving.
ICCV (2001), 190–197.
[XP98] Xu C., Prince J. L.: Snakes, shapes, and
gradient vector flow. IEEE Transactions on Image
Processing (1998), 359–369.
[ZBTC05] Zanuttigh P., Brusco N., Taubman
D., Cortelazzo G.: Greedy non-linear optimization of the plenoptic function for interactive transmission of 3d scenes. International Conference of
Image Processing ICIP2005, Genova (September
2005).

c The Eurographics Association 2005.


The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Reflection Transformation Imaging and Virtual
Representations of Coins from the Hospice of the Grand St.
Bernard
†

*

†

†

Mark Mudge , Jean-Pierre Voutaz , Carla Schroer , and Marlin Lum
† Cultural Heritage Imaging, San Francisco, California, USA
* Congregation of the Grand St. Bernard, Switzerland

Abstract
Reflection transformation imaging offers a powerful new method of documenting and communicating numismatic
cultural heritage information. The challenges of documenting numismatic material will be examined along with the
limitations of traditional documentary techniques. Previous uses of structured light scanning and PTMs in
numismatic documentation are reviewed and evaluated. A novel, low cost method for capturing PTMs at remote
locations and subsequent data processing operations is described. Reflection transformation imaging is shown to
capture more complete documentation than traditional photographic methods and communicate this information
with ease through digital media. The advantages of interactive relighting of numismatic PTM images in conjunction
with enhancement operations are explored along with the potential of informed choice of the most information rich
illumination directions. Advantages of joint capture of structured light and PTMs are examined including the
inherent registration of range and normal data, using range and normal information together to improve 3D
position accuracy, and the enhanced evidentiary reliability that results.
1. Introduction
Since the advent of coinage in western Asia Minor during
the late 8th century and early 7th century BCE, numismatic
material has played a central role in the story of humanity’s
past. Coins, and their distribution patterns, reflect the political and economic activity of the societies in which they
were used. As small bas-relief sculptures, coins form a
record of artistic styles and iconography. Though small,
many are considered among the finest expressions of our
ancestor’s sculptural work.
We will examine how virtual reality representations
employing reflection transformation imaging capture a
robust information set which can be illuminated from any
direction, permitting the interactive and intuitive examination of coins. We will see how specular reflections from
materials like gold are handled robustly and without data
loss. Examples will demonstrate that mathematical enhancement of reflectance information can disclose as much, if not
more, data than direct physical inspection.
2. Background of Reflection Transformation Imaging
and previous applications.
The term ‘Reflection Transformation’ was first used by
Tom Malzbender of Hewlett-Packard (HP) Labs in connecc The Eurographics Association 2005.


tion with a set of image processing methods he invented
known as Polynomial Texture Mapping (PTM)
[MWG2001]. Malzbender et al. presented a mathematical
model describing luminance information for each pixel in an
image in terms of a function representing the direction of
incident illumination. The illumination direction function is
approximated in the form of a biquadratic polynomial whose
six coefficients are stored along with the color information
of each pixel. The per pixel information is able to record
properties including surface inter-reflection, subsurface
scattering, and self-shadowing.
The per pixel information present in the polynomial coefficients is important in cultural heritage applications because
it contains essential shape data about the subject of the 2D
PTM image. By determining the brightest direction dependant luminance value for each pixel, the polynomial function indicates the directional vector, relative to the XY plane
of the image, corresponding to the source position of the
incident illumination that produced it. In turn, this directional vector discloses key information about the shape of
the original object at the spatial location represented in the
PTM image by that pixel. The directional vector pointing
toward the source of the brightest incident illumination for
that pixel is identical to what is mathematically known as
the ‘normal’ vector perpendicular to the surface of the represented object at the position indicated by the pixel. Taken

30

Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

collectively, the normals of a surface describe its shape and
are used by computer graphics lighting models to determine
surface reflection properties [P1975]. Consequently, PTMs
can be understood as 2D images possessing true three
dimensional data. This three dimensional data is retrieved
from the PTM through interactive viewing software employing various methods of reflection transformation imaging.
PTMs differ from previous work in the shape from shading
category including photometric stereo [KSK1988] and more
recent Bidirectional Texture Function (BTF) and normal
aquisition approaches employing concave parabolic mirrors
[WD2003]. Beyond the use of a polynomial representation,
PTMs can communicate useful shape information using
purely image based transformations with new material properties in light space without full photometric stereo or other
reconstruction from the surface normals in 3D Cartesian
space. When used in Cartesian space, these normal based
reconstruction methods are known to generate low frequency ‘warps’ in their resultant 3D geometry. This is also
true for integration of geometry from PTMs. Because of this
limitation, surface orientation information has often been
used as bump maps in the rendering of 3D geometry rather
than as a source of original geometry [BMMR2002]. While
PTMs have also been used to add surface orientation information to acquired geometry as will be seen in section 2.2,
when used independently from 3D geometry, light space
based reflectance transformation imaging will be seen to
communicate large amounts of empirically useful shape
information.
Light space is a mathematical construct built with a three
dimensional parametric geometry representing the information describing the impact of incident illumination direction
upon the luminance intensity of the PTM image’s pixels.
When interactively examining a PTM image, light space can
be thought of as a hemispheric dome enclosing the PTM.
This imaginary dome corresponds to the parametric geometry of the light space mathematically stored in the form of
polynomial coefficients in the PTM image’s pixels. Because
PTMs use essentially the same 3D lighting models used to
render 3D geometry, image content can be interactively illuminated by the user as if it occupied 3D Cartesian Space. In
PTM viewing software, when the user drags their mouse
through the 2D region representing the image’s light space,
they are dragging their mouse over a hemispheric volume.
When the mouse cursor is in the center of the light space
area, the illumination is analogous to the sun at 'high noon'.
As the mouse cursor is dragged toward the edge of the light
space area, the sun approaches the 'horizon'.
Illumination in Light Space has similarities with illumination in the 'real world'. The PTM Viewer's viewing location
is fixed perpendicular to the image plane, at the Light Space
dome's 'high noon'. Illumination originating from the same
'high noon' location, equivalent to the center of the PTM,

will be mainly reflected back to the viewing location and
will be at its brightest. As the light direction approaches the
horizon, equivalent to the edge of the PTM, fewer light rays
will find their way to the viewing location and the PTM will
be less brightly illuminated.

Figure 1: Silver Athenian Tetradrachma, 449 BCE
Figure 1 shows three images from the same PTM with the
light source coming from three different directions. In the
left image, the light is positioned at 'high noon'. In the middle image, the light is positioned near the 'horizon' from the
direction corresponding to the top of the image. In the right
image, the light is positioned near the 'horizon' from the
direction corresponding to the bottom of the image
There are two implementations of PTM viewing software.
The first PTM Viewer software, written in C++, was developed at HP Labs [HPLweb]. The second, written in Java,
was developed by Harvard graduate student Clifford Lyon
[Lyonweb]. While each viewer possesses unique attributes,
they share a core functionality and user experience of light
space.
It is through this transforming interplay of light and
shadow over the surface of the represented object that PTMs
communicate the nature of the object’s form. The intuitive
similarity of the interplay of light and shadow over shapes in
the ‘real world’ in our everyday experience to the experience offered by reflection transformation imaging is the
source of this technique’s documentary and communicative
power.
As we have seen, existing PTM viewers enable illumination of a subject from all directions possible in the ‘real’
world. PTMs also enable reflection transformation models
that mathematically enhance 3D shape information. The two
enhancement methods present in PTM viewing software are
‘Diffuse Gain’ and ‘Specular Enhancement’.
The work by researchers in a number of fields reviewed in
section 2.1 will demonstrates the value of these techniques
and that the diffuse gain and specular enhancement modalities of PTMs permit extraction of empirical data not discernible to the naked eye, standard magnification methods, or
traditional photographic techniques.
PTMs of physical objects also possess properties which
can algorithmically indicate how to light an object in a manner that will capture the most information about it. The
determination of the most informative illumination direction
c The Eurographics Association 2005.


Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

of an object from a given viewpoint can be made through a
maximum entropy analysis [MO2005]. Given the ability to
relight a properly captured PTM from any arbitrary direction, the lighting direction that conveys the most information about the object can be analytically derived. In
principle, it is possible to determine and generate the most
informationally rich view of an object at a time subsequent
to the empirical documentation of the object. This process
can also be used to determine the most informative illumination direction when using enhancement methods. These
enhancement methods can be pragmatically applied to the
PTM and the informationally optimum lighting direction
computed according to the requirements of the user.
2.1 2D virtual applications
The first use of PTMs was the imaging of cuneiform epigraphy [MWGZ2000]. When engaged in traditional direct
physical inspection of a clay cuneiform tablet, epigraphers
would place it under a directional light, either at a window
or under a desk lamp, and turn the tablet to discern its markings. This was necessary because many features were only
clearly readable when illuminated from certain directions.
Epigraphic scholars using photographic documentary methods would record their research subjects using multiple
images, each using illumination from a different location.
Using the HP Labs PTM viewer [MWG2001], that enabled
illumination of a cuneiform PTM image from any desired
direction, allowed the epigrapher to view the tablet in a
manner reminiscent of direct physical inspection and take
advantage of optimal illumination conditions.
Reflection transformation tools have also been used in the
field of Paleontology to provide noticeable improvement in
the imaging of fossil specimens with low color contrast and
low but definite relief [HBMG2002].
These same tools applied to PTMs of ancient stone tools
reveal fine details of concoidal knapping fractures, use scarring, and stone grain [Mud2004], [CHIweb2].
Joint work done by the National Gallery in London and the
Tate Gallery demonstrate that more information about the
surface textures of oil paintings was disclosed through
examination of a PTM under specular enhancement than
using photographic images illuminated with grazing incident light [NGweb].
Specular enhancement has been used in law enforcement
after other techniques have failed, underscoring its use to
reveal hard to discern information. The FBI has employed
PTM imaging during its investigative work and the PTM
process was presented to the Criminalistics Section of the
American Academy of Forensic Sciences [Mor2003].
Reflection transformation is also being explored as a
means to capture an actor's live-action performance in such

c The Eurographics Association 2005.


31

a way that the lighting and reflectance of the actor can be
designed and modified in postproduction [WGT*2005]. This
work employs high speed video capture and a 156 light
spherical array, creating a spherical rather than hemispherical incident illumination environment, to acquire time-varying surface normals, albedo, and an estimate of ambient
occlusion for the actor in the video sequence.
2.2 3D Virtual Applications
A virtual 3D cuneiform tablet has been generated using
PTM texture maps registered to 3D geometry captured with
structured light scanning [Mud2004]. The light dome used
to capture PTM images at Hewlett-Packard Labs
[MWG2001] was modified to accept a fiber optic structured
light projector constructed by Cultural Heritage Imaging.
The projector was turned on and a photo was taken of the
cuneiform object illuminated with a structured light pattern.
Next the projector was turned off and, without changing the
position of the camera or cuneiform tablet, the PTM capture
dome was used to photograph fifty texture images, each illuminated from a different direction. These fifty images were
then synthesized, using PTM Fitter software [hp.com], into
a PTM. Because the structured light geometry was extracted
from a photograph aligned identically with the fifty photographs used to generate the PTM, the resulting 3D geometry
and PTM were perfectly registered. This procedure was
repeated eight times with different views of the tablet to
capture overlapping sets of 3D scans and PTMs, documenting the entire surface. Structured light scanning software
produced by Eyetronics NV was used to build and align the
3D geometry and texture maps. The eight scans and associated texture maps were aligned using the Eyetronics module
‘Shapematcher [CIG*2003]. A special 3D PTM viewer was
written by D. Gelb at HP Labs [Mud2004]. This viewer
enabled Translation, rotation, and zooming of the cuneiform
tablet along with the interactive manipulation of the illumination direction, used to exploit the shape information contained in the PTM
3. Issues Relating to the Study and Documentation of
Numismatic Material
Cultural heritage efforts to document and display coins are
complicated by their essential characteristics of metallic
composition, low relief design and small physical size. The
study of a coin’s features has traditionally been most successful through direct hands-on examination employing
magnification tools and a strong directional light source.
The examiner would pick up the coin, hold it under the light,
and move the coin to discern its information. Unfortunately,
security concerns due to ease of concealment, lack of unique
identification, and often high resale value severely limit
opportunities for this method of examination. When numis-

32

Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

matic materials are secured in displays, even those employing mirrors and magnifying optics, the ability to perceive
coins surface relief is significantly reduced.
The quality of numismatic information that can be gleaned
from a coin decreases further if in-person examination is not
possible. Section 3.1 shows that traditional photographic
documentation, when available, conveys only a subset of the
information discernible through direct physical inspection.
3.1 2D Photographic Numismatic Documentation
Numismatic photographic documentation must contend
with characteristics of often highly reflective metallic materials with a great variety of delicate surface patinas and a
small, fine featured, low relief morphology. The photographer must balance the desire to limit data obscuring shadows
and reflections with the need to portray as much of a coin’s
surface shape information as possible. The resulting photographic judgements are compromises that strive to capture
the most data but inevitably leave significant quantities of
valuable information undocumented.
The numismatic images with the fewest shadows and
reflections also disclose the fewest surface relief features.
Photographically this ‘flat’ style of image is created using a
diffusion structure, usually a tent or dome, that surrounds
the subject and lights it evenly from all directions
[Hob1981]. Incident illumination that is parallel to the camera viewing axis, such as a ring flash, also produces shadowless images but generate reflections from surface areas that
bounce light back toward the camera. While not as ‘flat’ as
the previous technique, these images still disclose relatively
few surface relief features. Sometimes the angle of the coin
is tilted so that it is no longer perpendicular to the camera/
light axis to increase tonal contrast and help reveal additional surface form [Hob1981]. This technique results in a
scale distorted coin image where a circular coin is represented elliptically.
Surface relief is best captured with high tonal contrast.
Tonal contrast employs shadows and highlights to communicate 3D surface form. The most common form of numismatic documentation achieves high tonal contrast by
positioning the photographic light source such that the subject is struck with obliquely angled, incident illumination
producing an image that accents the structure, detail, and
three dimensional quality [Hob1981]. The result is that
some areas of the coin are in shadow and some areas are
highlighted. Data is frequently obscured in both shadow and
highlight areas.
Flatbed scanners are also used to generate numismatic
images [FLAAR], but these images, varying in incident light
angle according to scanner design, are subject to essentially
the same limitations as photographic images.

As with cuneiform epigraphy, many surface features on
coins are only visible when illuminated from certain directions. Photographs of the same subject, illuminated from different directions, can produce images possessing
significantly different sets of information. The documentary
photographer taking a single, or small number, of photos of
a coin is then faced with the editorial choice of which information to capture and which information to omit.
Photographers do their best to minimize data loss in shadows and highlights. Sometimes, part of the extant illumination is reflected back to the subject to soften the shadowing
that produces the tonal contrast, thereby retaining some of
the information in the shadowed regions. Often the metals
used to make coins, gold for example, are characterized by
high levels of specular reflection leading to highlight data
loss. Polarizing the camera lens can reduce the size of the
area subject to data loss due to reflection [Hob1981]. Double polarization adds a linear polarizer to the light source as
well as the lens. The rotation angles of the filters are
adjusted to ‘cross polarize’ the light. Double polarization,
used with good results with many materials, can eliminate
all specular reflections but is also known to produce birefringent effects. Our experiments indicate that birefringent
effects are frequently encountered when photographing
coins. Birefringency produces unexpected, strong colors and
tonal effects not normally associated with the perception of
numismatic material. Compounds commonly present in
many coin patinas can cause this effect and, aside from the
study of patination chemistry, render the technique impractical for numismatic documentary purposes, see Figure 2.
.

Figure 2: Birefringency
In sum, the limitations of photographic documentation
including its editorial nature, the access restrictions imposed
by security concerns, the physical dispersion of numismatic
resources in collections around the world, and the remoteness of many of these collections, pose a serious impediment to numismatic scholarship and the enjoyment of these
resources by the interested public.

c The Eurographics Association 2005.


Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

3.2 3D Numismatic documentation
As with 2D documentation of coins, the attributes of
metallic composition, small size, and often intricate and
finely detailed designs necessitate scanning resolutions able
to capture high sampling densities.
Their metallic composition create issues relating to specularity and fragile surface patination. Highly specular surfaces are problematic for 3D scanning methods. Laser based
range acquisition tools generate low signal to noise results
when confronted with shiny surfaces. Similar issues are
present in structured light scanning processes. If the structured light pattern is obscured by reflection, no data can be
captured. The projection of the pattern inherently requires
direct incident illumination, preventing the use of diffusion
techniques. Range data spiking and data voids resulting
from reflectivity are usually countered by attempts to reduce
the subject’s shininess through the use of matting agents
such as powders and dulling sprays. Unfortunately, use of
such agents on numismatic materials is usually impossible.
On coins, the surface patinas and metallic lusters are often
extremely fragile. Further, surface characteristics are major
determinants of the coins monetary and aesthetic value.
These considerations make operations that might alter the
coin’s physical surface highly undesirable.
Our team used structured light scanning to create 3D models of coins beginning in 2002 [Mud2004], [CHIweb1]. We
employed the same basic projection equipment and software
methods previously described in connection with the 3D
scanning of the cuneiform tablet. However, no PTMs were
captured.
Specular reflections during range data acquisition were
eliminated with double polarized light. The Structured light
projector was equipped with a linear polarizer. A second linear polarizer was placed over the camera lens and its angle
rotated until all specular reflections were extinguished.
Registered texture images were captured with each range
image. The texture images were illuminated with a linearly
polarized ring flash. As the incident illumination angle of
the ring flash was parallel to the axis of the camera lens, the
images were shadowless. The polarizer over the flash and
rotationally aligned camera lens polarizer eliminated specularity. This texture acquisition method, relying on the use of
double polarization, had the disadvantage of producing birefringent effects. Figure 2 shows the result of double polarization during range data acquisition, the left image is a
detail of an unenhanced PTM of the coin. The right image
shows the birefringent effect and projected structured light
grid pattern
These effects required desaturation and tonal adjustments
in post processing, introducing undesirable editorial subjectivity and extra work.

c The Eurographics Association 2005.


33

Our research suggested that acquisition of high resolution
range data from highly reflective numismatic material was
possible. Given the need to remove highlights and shadows,
the associated textures suffered from the problems and limitations present in the photographic documentation of coins.
One possible solution was PTMs. If PTMs could resolve
these issues, they might serve as ideal texture maps on virtual 3D coins. This hypothesis prompted our experiments
with coins and PTMs.
4. Our Initial Numismatic Work with PTMs,
The first reflection transformation imaging of coins was
done in collaboration with Malzbender and Gelb using the
dome at HP Labs [Mud2004].
From the beginning, PTM images were understood to have
the attribute of muting sharp specularities and softening hard
shadows [MWG2001], but the extent of this property was
unknown. The goal of the first tests was to determine if the
highly specular characteristics of some coins would result in
poor quality PTM data.
Two coins were selected for the test. One coin was a silver
Athenian Tetradrachma with a mild degree of patination and
moderately high specularity, Figure 1. The other was a gold
stater from the time of Alexander the Great in near mint
condition and possessing extremely high levels of specularity, Figure 3.
In spite of the highly specular nature of the coins, particularly the gold stater, it was found that the smoothing introduced by PTM’s biquadratic polynomial function was
sufficient to accurately determine surface normals and color
values over the entire surface of both coins [Mud2004].
Figure 3 shows the stater under specular enhancement. In
the specular enhancement display, the relative proportions
of diffuse and specular components can be shown as
desired.The image on the left shows a 4:1 ratio of diffuse to
specular. On the right the ratio is 1:1

Figure 3: Gold Macedonian Stater, 336-323 BCE
These preliminary experiments established that PTM
imaging was able to overcome the data loss problems associated with shadowing and specular reflection in numismatic

34

Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

documentation. This ability of PTMs to robustly capture the
features of coins and provide a more complete numismatic
documentary record laid the foundation for our work at the
Hospice of the Grand St. Bernard
5. Project Background

bit TIFFs. During a Photoshop batch process, contrast and
saturation parameters were determined and applied, followed by 8 bit conversion, and cropping. All operations performed on the images were performed identically on each
constituent light direction image in the PTM group. The
constituent images for each group were then synthesized by
the PTM-Fitter program into a PTM. .

The Hospice of the Grand St. Bernard has an extraordinary
location and history. The monastery is located atop the
Grand St. Bernard Pass 2472 meters above sea level. The
summit of the pass forms the border between Italy and Switzerland and has been a major route between Northern
Europe and the Italian Peninsula since the mesolithic period
[Hun2005]. Harsh weather frequently isolates the Pass and
has made its crossing a perilous and often deadly activity.
The valley descending from the summit on the Swiss side is
named in french ‘Combe des Morts’, in english, ‘Valley of
Deaths’. Today the road over the Pass is open for only three
months a year and the average annual snowfall exceeds fifteen meters. The monastery was founded almost one thousand years ago by St. Bernard. Beginning with a single one
room rock shelter, St. Bernard began a tradition of refuge
and welcome continued by the Augustinian monks of his
order today.
The Monastery possesses an extensive archeological collection excavated from the Grand St. Bernard Pass including
material from a 17th century BCE bronze age tomb. The
numismatic collection, ranging from the time of Alexander
the Great to the 19th century CE, includes six hundred
Celtic, two hundred Roman Republic, and fifteen hundred
Roman Imperial coins.
6. PTM Acquisition and Processing
We designed a manual, easily transportable, and low cost
system. The system consisted of a light position template,
fiber optic directional light source, and computer controlled
camera. The light position template indicated twenty four
known spatial locations in a hemispheric array surrounding
the coin. The camera, an 11 megapixel Canon 1Ds, was
positioned at the apex of the hemispheric light array and
triggered remotely by computer.The light was moved by
hand to each of the twenty four positions and a corresponding image captured. The coins were photographed on a calibrated 18% Kodak gray card and care was taken to isolate
the coin and card from the surrounding template to mute
vibration. The set of twenty four constituent PTM light
direction images for each face of the coins required an average of 30 minutes to captureProcessing the images and
Archiving the PTM building process
Each constituent PTM light direction image was captured
in RAW format. Capture One software was used to set white
balance for each group and convert the RAW images into 16

Figure 4: PTM System: 1) 5500K Solarc illuminator 2)
liquid light guide 3) optical condenser and diffusing unit
4) 50 mm Canon FD lens 5) height indexed light stand
6) Template 7) 18% gray card 8) Canon 1Ds 9) Canon
100 mm macro lens 10) Dual focusing sliders 11) tripod
with extension arm
12) Ever useful gaffers tape
The original Raw images and their conversion settings
were archived as were the Photoshop batch processes, the
light position file, associating images with light position
coordinates, and the PTM-Fitter command line string, indicating the specific set of PTM attributes employed. At the
end of the process, each PTM possessed an associated
archive enabling anyone to review the image processing
employed to build it. The same information can be used for
confirmatory replication. In this way the quality of the PTM
image can be independently assessed
7. Results
PTMs were captured of both sides of seven selected coins
from the collection. These PTMs were bundled with the
Java based Viewer written by Clifford Lyon [Lyo2004] and
are displayed, with descriptions in French, on the website
maintained by the Congregation of the Grand St. Bernard, in
order to, in the words of the Congregation, “allow people to
consult some of the seminal objects of our civilization”
[GSBweb]. The PTMs have also been included in a Powerpoint presentation given to members of the Congregation
and youth groups traveling over the Pass, again in the words

c The Eurographics Association 2005.


Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

of the Congregation, “to explain to them that they are part of
a long chain of human beings which crossed the Alps going
back millennia.” They are also displayed, with English
descriptions, on the Cultural Heritage Imaging website
[CHIweb2]. The coins from the Grand St. Bernard collection whose PTMs are featured in this paper are described
below.
7.1 Silver Roman Republican Denarius

Figure 5: Denarius
Obverse: Goddess Roma head right., denomination indicated by ’X’ left of head, ‘ROMA’ written on field at right.
Reverse: Jupiter driving a Quadringa with Laurel branch
above, ‘CN.DOMI’ in exergue. Struck in Rome 116-115
BCE, Silver, 3.91g, 20.6x19.4mm, Catalog Number: GSBf
617
7.2 Bronze Roman Imperatorial Aes depicting ‘Julian
Star’

35

tus. It commemorates the divinization of Julius following
his assassination in March of 44 BCE. The comet, the Sidus
Iulium or Julian Star, appeared in July 44 BCE during the
funnery games held to honor Julius following his assassination in March of that year and remained for a period of days.
When it departed, historians relate that it was thought to
carry the soul of Julius and was said to have been a sign of
his divinity [RL1997].
Gold Merovingian Triens

Figure 7: Triens
Obverse. Bust of Merovingian ruler wearing pearl diadem,
partial legend ‘OMUIT’ in field. Reverse: Latin cross with
“V’ and ‘II’ surrounded by a crown of pearls, remnant of
‘ACAUNO FIT’ in field. Minted at Agaune (St. Maurice),
Switzerland, Merovingian, Clotaire II (613-628 CE) or
Dagobert I (628-638 CE), Gold, 1.14g, 11.3x11.0mm. Catalog Number: RGa F4C1. Only known example.
7.3 Evidence of complete surface information for gold
coins
Our work confirmed previous observations that PTMs
were able to robustly document the surfaces of highly specular materials and specifically gold.

Figure 6: Aes
Obverse: Julius Caesar left, Octavian, Caesar’s adopted
son (later Augustus), right, Sidus Illium comet above their
heads, IMP CAESAR DIVI F DIVI IVLI in field. Reverse:
Prow of Roman ship adorned with dolphin. Minted in Lyon,
France, Imperatorial period, (39 or 36 BCE), Bronze,
21.43g, 30.8x30.2mm. Catalog Number: GSBf 698. One of
four known examples.
This coin was issued by Octavian, the adopted son of
Julius Caesar, who became the first roman emperor Augus-

c The Eurographics Association 2005.


Figure 8 shows two images of the obverse of the Merovingian Triens. The image on the left of Figure 8 is one of the
twenty four input images for the PTM. It clearly shows two
areas, indicated with arrows, where specular reflections
have caused data loss. The image on the right of Figure 8 is
captured from the resultant PTM under specular enhancement and shows complete surface information for the areas
in question
Given a sufficient number of empirically captured incident
illumination direction samples to generate accurately
formed parametric geometry in light space, occasional
instances of missing data due to shadows and specular highlights are replaced by accurately interpolated values. The
presence of complete surface normal information in the

36

Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

PTM permits reflectance transformation operations on the
entire virtual coin.

.

Figure 8: Specular Reflection
7.4 Data discernment advantages to reflectance
transformation documentation
Diffuse gain provides contrast enhancement that, in combination with a dynamically changing light direction, aids
the perception of bas-relief morphology. Figure 9 shows two
views with identical lighting. the image on the left is unenhanced. The image on the right employs diffuse gain. The
additional contrast from the diffuse gain enhancement
‘brings out’ the subtleties of the Quadringa and horses’
form.

Figure 10: Unenhanced ‘Julian Star’

Figure 9: Diffuse Gain Enhancement
The established utility of specular enhancement to disclose
surface information, sometimes even invisible during direct
physical examination, is of great value to the field of numismatics
Figures 10 and 11 show two images of the coin commemorating Julius Caesar’s Funeral Games. The Comet is circled
in its full context and then magnified. Due to heavy wear,
there is significant abrasion of the coin’s surface features. In
Figure 10, the coin is seen in its natural, primarily diffuse
mode. In Figure 11, the coin is seen in its specular enhancement mode. The Specular enhancement mode shows, with
superior clarity, the four rays of light surrounding the comet
along with the triangular comets tail extending behind it at
approximately the ‘five o’clock’ position.

Figure 11: ‘Julian Star’ with Specular Enhancement

c The Eurographics Association 2005.


Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

8. Conclusions
8.1 PTMs provide an interactive experience of a more
complete data set than traditional numismatic
documentation
Using PTM viewing software to interactively experience
the virtual documentation of the coins, clearly demonstrates
the value of reflection transformation imaging to the field of
Numismatics.
The ability to light coins from any desired direction permits features visible only from specific illumination directions to be seen. Areas which are obscured by shadows and
bright reflections when lit from a given direction can be
observed by changing the illumination direction. With a virtual PTM image, the traditional experience of examining a
static photograph image is transformed into a dynamic process of intentional viewing, exploring, in an intuitive manner, a far richer data set.
8.2 PTMs offer a more informed and tool rich method to
select and generate images that convey numismatic
ideas.
The ability of maximum entropy analysis to disclose
which PTM illumination directions provide the greatest
amounts of information in combination with specular
enhancement, diffuse gain, and the flexibility of dynamic
illumination direction, enables scholars and other users to
determine which image or set of images will most effectively communicate their ideas. The PTM image can be analyzed both algorithmically and pragmatically to find the
most advantageous combination of illumination angle(s) and
enhancement effect(s) that illustrate the characteristics
under discussion. The optimized image(s) can then be digitally captured and included in the desired communication
medium. This offers significant advantages over the fundamentally arbitrary and editorial nature of even the most aesthetically accomplished traditional single image
photography.
8.3 PTMs as 3D Textures with normals for virtual coins
The ability of PTMs to document numismatic surfaces
without data loss, generate surface normal information, and,
through the use of reflection transformation functions, to
enable 3D virtual representations to carry and communicate
additional useful information, strongly recommends their
use as textures for 3D virtual coins.
8.4 The Advantages of PTMs Captured in combination
with Structured Light Range Data
Our exploration of 2D and 3D reflection transformation
imaging documentary applications indicate a strong affinity

c The Eurographics Association 2005.


37

between structured light 3D acquisition and reflection transformation imaging with PTMs. Variants of this relationship
between structured light and PTMs also exist between structured light and other methods of measuring bidirectional
reflectance functions (BRDFs) which capture normal vector
information. The following advantages of this affinity suggest that very high quality numismatic documentation can
be produced through the joint application of these techniques. This opportunity for the generation of high quality
documentation may exist in other cultural heritage applications and inform the discussion of empirically acquired 3D
virtual representations generally.
The shape information contained in PTMs permits the use
of less dense geometric descriptions. As was mentioned previously in the discussion of PTMs and photometric stereo
[BMMR2002] and our 3D work with cuneiform tablets
[Mud2004], the presence of empirically derived per pixel
surface normal information serves as an excellent bump
map. When registered and anchored to a surface in cartesian
space, the normal information present in PTMs offer a
densely sampled surface description that can span a less
densely sampled, structured light acquired, geometry.
When a denser geometric description is desired, a structured light system that generates per-pixel range data can be
employed. Many such systems exist and examples of such
structured light systems can be found in [RBMT1998], and
[STY*2003]. Multiple scans can be aligned, merged, and
full model texture maps synthesized using software such as
[CCG*2003]. Each photographic structured light pixel generates information containing a Cartesian positional X,Y,Z
triplet. When used in conjunction with a PTM acquisition
system, the generated PTM image has a corresponding pixel
possessing an R,G,B color triplet, a surface normal vector,
and implied knowledge of any surface inter-reflection, subsurface scattering, and self-shadowing effecting the pixel.
Given the registration between the pixels of the structured
light source image and PTM source image, the surface normal information can be analyzed in combination with the
surface range information to produce more precise 3D
geometry [NRDR2005]. As have seen, when geometry in
Cartesian space is generated from BRDF or PTM surface
normal data, it is subject to low frequency (over relatively
large distances) warping but can be highly accurate at high
frequencies (over relatively small distances). On the other
hand, scanned range data, whether from structured light or
other sources, is well known to be quite accurate at the low
frequencies but possess high frequency noise which distorts
the location of surface points by a distance which is related
to the scan’s sample density. The more points acquired in a
square millimeter surface area, the smaller the distance positional data will be distorted by noise. Combined analysis of
registered normal and range data sets uses the best information from both worlds to create a synthesized result corre-

38

Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

sponding more closely with the ‘real’ world documentary
subject than either constituent data set alone.
The combination of structured light scanning and PTM
texture generation and the resulting geometry/texture relationship creates an empirically robust virtual representation.
Structured light geometry and PTMs are inherently registered. This inherent registration increases the evidential reliability of the virtual representation because an explicit and
necessary relationship exists between surface range data and
color/normal vector information. The ability to refine surface shape through analysis of range and normal information
sets acts as a cross-check on the coherence of the data and
adds to evidential reliability. The archiving and association
of image generation process history provides the in depth
analysis and possibility of replication necessary to evaluate
the documentary images’s quality. In turn, this evidential
reliability accelerates the widespread acceptance and use of
virtual representations by cultural heritage workers in their
professional activities.
8.5 Summary
2D PTMs of coins communicate as much, if not more,
visual information as direct physical examination. The ease
of their digital communication overcomes numismatic
access and display limitations. This information can be algorithmically analyzed and pragmatically explored using an
enhancement tool kit to create the most useful images to
convey numismatic ideas.
Through digital communication, ancient coins from the
very remote, and often snow-bound collection of the Hospice of the Grand St. Bernard have become available to the
whole world. This successful proof of concept project can
lead to wider access of other numismatic collections.
The integration of PTMs with per-pixel structured light
range data will lead to more accurate 3D virtual coins which
can be examined using the PTM enhancement tool kit.
The evidential reliability of both 2D PTM and 3D PTM/
Structured light virtual representations of coins can encourage the adoption and development of similarly empirically
accountable virtual imaging methods in other cultural heritage areas of study and the field of 3D object acquisition
generally.
9. Future Work
We have built an automatic thirty two light fiber optic
PTM capture device able to document coins, and with adaptation, larger objects. It can limit photon damage through
control of the illumination wavelength band, enabling the
documentation of wax and lead seals on ancient documents
and other fragile, light sensitive materials. We intend to document cultural heritage objects through the camera cali-

brated capture and display of PTMs and multi-row object
movies, creating an encompassing environment for aligning
sets of PTMs with varying viewpoints. Integrated 3D geometry derived from visual hulls extracted from these object
movies will also be explored. We also plan to capture
numismatic and other material using a combined PTM/
structured light apparatus.
10. Acknowledgements
The generous assistance of Tom Malzbender, Dan Gelb,
and HP Labs, made this work possible. The congregation of
the Hospice of the Grand St. Bernard offered their welcome,
encouragement, vision, and production help. Dr. Patrick
Hunt, Director of the Stanford Alpine Archaeology Project,
provided important archeological analysis and logistical aid.
Thanks to the Cultural Heritage Imaging Board of Directors,
supporters, and volunteers for funding and labor.
11. References
[CCG*2003] Callieri, M., Cignoni, P., Ganovelli, F., Montani, C., Pingi, P., Scopigno, R. 2003. VCLabb’s Tools for
3D Range Data Processing VAST 2003, Virtual Reality,
Archaeology, and Intelligent Cultural Heritage, Eurographics Symposium Proceedings. pp. 13-22.
[CIG*2003] Cosmas, J., Itegaki, T, Green, D., Joseph, N.,
Van Gool, L., Zalesny, A., Schindler, K., Vanrintel, D., Leberl, F., Grabner, M., Karner, K., Gervautz, M., Hynst, S.,
Waelkins, M., Vergauven, M., Pollefeys, M., Cornelis, K.,
Vereenooghe, T., Sablatnig, R., Kampel, M., Axell, P.,
Meyns, E., 2003. Providing Multimedia Tools for Recording, Reconstruction, Visualization and Database Storage/
Access of Archeological Excavations. VAST 2003, Virtual
Reality, Archaeology, and Intelligent Cultural Heritage,
Eurographics Symposium Proceedings pp. 166-167.
[CHIweb1] http://c-h-i.org/examples/3d/3d_tetra_obj.html
[CHIweb2] http://c-h-i.org/examples/ptm/ptm.html
[FLAARweb] FLAAR Information network,
http://www.flatbed-scanner-review.org/
gold_coin_collecting_scanner/collecting_silver_coins.html.
[GSBweb] http://www.gsbernard.ch/histoire/musee.htm
[HBMG2002] Hammer, O., Bengston, S., Malzbender, T.,
Gelb, D. August 2002. Imaging Fossils Using Reflective
Transformation Imaging and Interactive Manipulation of
Virtual Light Sources. Palaeontological Association.
Palaeontologia Electronica, 2002, issue 1

c The Eurographics Association 2005.


Mudge, Voutaz, Schroer, Lum / Reflection Transformation Imaging and Virtual Representations of Coins from the GSB

[Hob1981] Hoberman, G., 1981. The Art of Coins and their
Photography. Spink and sons Ltd. in cooperation with Lund
Hunphies Publishers Ltd. ISBN 0853314500, pp. 323-363
[HPLweb]http://www.hpl.hp.com/research/ptm/
[Hun2005] Hunt, P., 2005. Archaeology and History of the
Great St. Bernard Pass. L'Erma di Bretschneider, Rome,
publisher. ISBN pending.
[KSK1988] Klette, R., Schluns, K., Koschan, A. 1988.
Computer Vision, Chapter 8: Photometric Stereo. Springer
Verlag, publisher. ISBN 981-3083-71-9, pp. 301-345
[Lyonweb] http://materialobjects.com/ptm/
[MGW2001] Malzbender, T., Gelb, D., and Wolters, H.
2001. Polynomial texture maps. Proceedings of ACM Siggraph, 2001
[MGWZ2000] Malzbender T., Gelb, D., Wolters, H., and
Zuckerman, B. 2000. Enhancement of shape perception by
surface reflectance transformation. Hewlett-Packard Technical Report HPL-2000-38.
[MO2005] Malzbender, T., Ordentlict, E. 2005. Maximum
Entropy Lighting for Physical Objects. Hewlett-Packard
Technical Report HPL-2005-68. http://www.hpl.hp.com/
personal/Tom_Malzbender/papers/papers.htm
[[Mor2003] Morton, S. Feb. 2003. Forensic Document
Examiner for the San Francisco Police Criminalistics Laboratory, 850 Bryant Street, San Francisco, Ca 94103. Presentation to the Criminalistics Section of the American
Academy of Forensic Sciences.
[Mud2004] Mudge, M. 2004. SIGGRAPH 2004 Conference
Presentations, Web Graphics/Special Sessions/Panels, Cultural Heritage and Computer Graphics Panel. Soma Media,
publisher, ISBN1-58113-950-X
[NGweb] National Gallery, London.
http://www.tate.org.uk/collections/ptm/technical.htm
[NRDR2005] Nehab, D., Rusinkiewics, S., Davis, J.,
Ramamoorthi, R., 2005. Efficiently Combining Positions
and Normals for Precise 3D Geometry. ACM Transactions
on Graphics, Siggraph, 2005. http://www.cs.princeton.edu/
gfx/pubs/Nehab_2005_ECP
[Pho1975] Phong, B.T. 1975. Illumination for computer
generated images. Communications of the ACM, 18(6):311317.
[RBMT1998] Rushmeier, H., Bernardini, F., Mittleman, J.,
Taubin, G. 1998. Acquiring Input for Rendering at Approc The Eurographics Association 2005.


39

priate Levels of Detail: Digitizing a Pieta. Rendering Techniques ’98, pp. 81-92.
[BMMR2002] F. Bernardini, I. Martin, J. Mittleman, H.
Rushmeier, G. Taubin. Building a Digital Model of Michelangelo's Florentine Pieta'. IEEE Computer Graphics &
Applications, Jan. 2002, pp. 59-67
[RL1997] Ramsey, J., Licht, A. 1997. The Comet of 44 B.C.
and Caesar’s Funeral Games. American Philological Association American Classical Studies, Number 39. Scholar’s
Press, Atlanta Ga., publisher.
[STY*2003] Stumpful, J.,Tchou, C., Yun, N., Martinez, P.,
Hawkins, T., Jones, A., Emerson, B., Debevec, P. 2003.
Digital Reunification of the Parthenon with its Sculptures.
VAST 2003, Virtual Reality, Archaeology, and Intelligent
Cultural Heritage, Eurographics Symposium Proceedings.
pp. 41-50.
[WD2003] Wang, J., Dana, K., A Novel Approach for Texture Shape Recovery. ICCV 2003, Volume 2, pp. 1374-80.
[WGT*2005] Wenger, A., Gardner, A., Tchou, C., Unger, J.,
Hawkins, T., Debevec, P., 2005. Postproduction Relighting
and Reflectance Transformation with Time-Multiplexed
Illumination. Proceedings of ACM Siggraph, 2005. http://
gl.ict.usc.edu/research/LS5/index-s2005.html

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Detail-Preserving Surface Inpainting
G.H. Bendels, R. Schnabel, R. Klein
Institut für Computergraphik, Universität Bonn, Germany

Max Planck model with missing data. The hole is inpainted by copying appropriate fragments of other parts of the object to the hole region.

Abstract
Inpainting is a well-known technique in the context of image and art restoration, where paint losses are filled up
to the level of the surrounding paint and then coloured to match. Analogue tasks can be found in 3D geometry
processing, as digital representations of real-world objects often contain holes, due to hindrances during data
acquisition or as a consequence of interactive modelling operations. In this paper we present a novel approach to
automatically fill-in holes in structured surfaces where smooth hole filling is not sufficient. Previous approaches
inspired by texture synthesis algorithms require specific spatial structures to identify holes and possible candidate
fragments to be copied to defective regions. Consequently, the results depend heavily on the choice and location
of these auxiliary structures, such that for instance symmetries are not reconstructed faithfully. In contrast, our
approach is based on local neighbourhoods and therefore insensitive with respect to similarity transformations. We
use so-called guidance surfaces to guide and prioritise the atomic filling operations, such that even non-trivial and
larger holes can be filled consistently. The guidance surfaces are automatically computed and iteratively updated
during the filling process, but can also incorporate any additional information about the surface, if available.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Comp. Geometry and Object Modeling]:

1. Introduction
Creating digital 3D copies of real-life objects is becoming
a standard procedure for various application fields - ranging from cultural heritage applications and medicine to automotive, artistic, and entertainment industries. Despite all
technological progress, models resulting even from the most
careful acquisition process are generally incomplete, due to
occlusion, the object’s material properties or spatial constraints during recording (among others), i.e. they contain
undersampled regions and/or holes. In some applications,
holes are also deliberately introduced into an object, as removing damaged, undesired or unnecessary parts of an object is an important tool in interactive modelling.
In order to derive complete and visually appealing models,
c The Eurographics Association 2005.

these holes have to be filled plausibly, i.e. the basic geometry has to be smoothly patched and the (unknown, yet assumed) detail geometry has to be restored or extrapolated,
taking into account the context of the object. That this illposed task has hope of being solved at all is based on the
observation that real-life objects often exhibit a high degree
of coherence in the sense that for missing parts one can find
similar regions on the object. This observation has been exploited extensively in the field of 2D texture synthesis and
disocclusion, and also in 3D surface completion. The problem with previous approaches, though, is that they require
specific spatial structures to identify holes and possible candidate fragments to be copied to defective regions. Consequently, the results depend heavily on the choice and location of these auxiliary structures. In contrast, we propose

42

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

in this paper a surface inpainting method that analyses the
neighbourhood of a hole, finds best matching local neighbourhood patches represented in local frames (the 3D analogue to what is called a fragment in image processing), and
fills the hole with copies of these. By finding best matches
hierarchically on several scales, the hole is filled in conformance with the context with respect to all considered scales.
The two key challenges for such a 3D fragment-based approach are

l = 0, . . . , L

• to identify symmetry, similarity and coherence relationships in the scene or on the object
• to exploit these relationships to inpaint surface regions.
Before we describe in detail how these challenges can be
tackled, we’ll shortly review the relevant literature in the following section, covering the inspiring works on 2D image
processing, but also previous approaches to automatic hole
filling for boundary representations in 3D. Then we describe
our algorithm in a one-level, non-hierarchical way in section 4, and extend the algorithm in section 5 to exploit Guidance Surfaces in a multi-scale approach.
2. Previous Work
The work most relevant for our paper can be subdivided into
two basic categories, 2D image completion and 3D surface
completion.
2.1. Image Completion
Image completion aims at filling-in holes in an image that
are generated erasing defective, damaged or undesired parts
of an image, by extending information available in the remaining image. Here, in addition to the overall visual properties of the image, the larger and highly irregular structures
of the image have to be preserved. With this requirement
in mind, Ballester et al. [BBC∗ 01] fill images by explicitly
propagating lines of equal brightness (so-called isophotes)
by solving variational problems, whereas Jia et al. [JT03]
segment the image and propagate segment borders into the
hole region, before filling colours in a pixel-based approach.
Using isophote-propagation to guide what is otherwise a
pure texture synthesis approach, Criminisi et al. [CPT03]
presented an approach that is similar to ours in the sense
that, in order to propagate larger scale structures to hole regions, we also prioritise our hole filling steps according to
the detection of feature lines on the surface – lines that can
be considered as the 3D analogue to isophotes in images.
Our approach also benefits from work presented by Drori
et al. [DCOY03], who assign iteratively updated confidence
values to each pixel in the image and exploit these confidence values for guiding the filling steps.
2.2. Surface Completion
As 3D data-acquisition generally leads to incomplete surfaces, the need to fill holes in 3D surfaces is traditionally
part of surface reconstruction algorithms (see [CL96] as an

Pl

=

Hierarchy level (L for coarsest)

{pli }

Point sets (hierarchy level l)

Bl = {bli } ⊂ P l

Sets of border points

C l = {cli } ⊂ P l \ Bl

Candidate sets

α:

Pl

7→ [0, 1]

Confidence value

N (pli ) ⊂ P l

Local neighbourhood of p

Gl

Guidance surface

n, nl

Number of points in P l

N
χ:

Pl

→

R

N

Number of points in the descriptor
Descriptor

Table 1: Notation and Symbols

example), but has also achieved recent research attention in
its own right [DMGL02, VCBS03, Lie03, CDD∗ 04].
Lifting the 2D-surface into a 3D volumetric representation,
Davis et al. [DMGL02] extend a signed distance function
that is initially only defined close to the known surface to the
complete space using volumetric diffusion, thereby completing the surface even for non-trivial hole boundaries. Clarenz
et al. [CDD∗ 04] cover surface holes minimising Willmore
energy functionals, leading to smooth surface patches with
guaranteed continuity properties.
Smooth completion is also the result of very recently published hole filling algorithms, where templates – constructed
from some known basis mesh [KS05] or from a partly manual selection from a shape data base [PMG∗ 05] – have been
exploited.
In some applications, however, smooth filling of holes is
not sufficient; this is particularly the case in cultural heritage applications, where virtual museums require visual
appealing reconstructions of cultural heritage objects. In
such applications, so-called surface inpainting algorithms
are needed that do not only reconstruct the basic geometry of the defective object, but also their fine scale geometric features. Although the problem of completing 2D images
appears conceptually almost identical to completing 2D surfaces in 3D, transferring successful techniques from image
completion to 3D is far from trivial. The lack of a regular
grid deprives us from the universal parameter domain that
is so extensively exploited in 2D image processing. As a
consequence, already the construction of multi-scale hierarchies representing different frequency bands – apparently
a key ingredient to many image completion approaches –
proves to be challenging, as the vertices’ positions at the
same time encode both, signal and domain of the function to
be analysed [GSS99, Tau95, PG01]. To our best knowledge,
there are yet only few publications that address the problem of detail preserving during hole filling [SK02,SACO04].
Adapting technologies from exemplar-based image synthesis methods and similar in concept to our approach, Sharf et
c The Eurographics Association 2005.

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting
Fill (Point Set Hierarchy P L , . . . , P 0 )
compute initial guidance surface G L [optional]
for all l = L, . . . , 0 do
Bl ← find boundary points in P l
C l ← find candidate points in P l
compute descriptors χ(C l ) and χ(Bl ) using G l
Q ← prioritise Bl
while Q not empty do
b ← top(Q)
find best matching candidate c ∈ C l
copy N (c) to N (b)
update Bl and Q
end while
G l−1 ← MLS(P l )
end for

al. [SACO04] fill holes by copying existing surface patches
from the object to the hole region. The fundamental problem
of this algorithm is that it is completely octree-based: Holes
in the surface are detected by checking for near-empty octree cells, different scales in their hierarchical approach are
represented through octree levels, descriptors are based on a
regular sampling of a distance field, and, most importantly,
patches to be copied can be generated from other octree cells
only. As a consequence, even perfectly symmetrical objects
can only be reconstructed if the symmetry axis of a symmetrical feature happens to coincide with an octree axis (or one
of the considered, discrete rotations thereof). Furthermore,
due to the resulting non-invariance with respect to rotation,
translation and scaling, very careful parameter tuning is required to successfully reconstruct real-world examples.

43

surface in the hole region on coarse scales first and exploits
the result to derive the guidance function for the next levels.
Hence, the first step in our algorithm is to compute a point
set hierarchy, consisting of L point sets P 0 , . . . , P L , where
P 0 is the original point set and P 1 , . . . , P L are smoothed and
(optionally) subsampled copies thereof. For clarity of presentation, though, we describe a non-hierarchical, 1-levelformulation of our approach first, before we motivate and
present the hierarchical formulation in section 5.
4. Non-Hierarchical Formulation

R

Suppose we are given a point set P = {p1 , . . . , pn } ⊂ 3 .
Following the notion from 2D-image synthesis, we define
for every point p ∈ P in conjunction with a local frame
a corresponding surface fragment
Fp and a radius ρ ∈
Nρ (p) ⊂ P as

R

Nρ (p) = { pi ∈ P | d(p, q(pi )) ≤ ρ } ,
where q(pi ) is the projection of pi into the plane defined by
Fp (see fig. 1). In order to establish the defining local frame,
we take the best fitting plane to the k nearest neighbours of
p, as suggested in [HDD∗ 92], and use it as parameter plane
for the fragment and its normal as surface normal in p. Given
this frame, the points in the fragment can efficiently be collected traversing a proximity graph as suggested in [KZ04].
Please note that we use the terms fragment and local neighbourhood synonymously throughout this paper, and that we
suppress the index ρ in unambiguous cases, as we do with
the index l.

3. Overview and Terminology

R

Given a point set P ⊂ 3 representing a manifold surface,
the goal of our algorithm is to fill any existing holes plausibly, i.e. taking into account the object’s local and global
context. This goal is achieved by first identifying Boundary
Points, i.e. points that are close to regions in the point set
with insufficient sampling, and then by copying appropriate
local neighbourhood patches (so-called fragments) from a
candidate set to the hole region. This way the hole is iteratively closed. As newly inserted points influence later filling
steps, we assign to every point in the point set a confidence
value, which is equal to 1 for all points in the original point
set and is in the interval [0, 1) for inserted points. By these
means, we are able to evaluate each point’s confidence and
adjust its influence on the algorithm appropriately.
In accordance with the terminology in texture synthesis approaches, we call the regions close to insufficient sampling
Target Fragments and regions from where points to be inserted are drawn are called Source or Candidate Fragments.
With the notation given in table 1, the basic workflow of our
algorithm can best be seen in pseudo-code (top of the page).
The overall approach is hierarchical, i.e. it reconstructs the
c The Eurographics Association 2005.

Figure 1: Illustration of a local point set neighbourhood
(triangulated for display purposes, centre) and its regularly
resampled counterpart (left). Hole regions in the original
surface (red square) lead to invalid descriptor components
(coloured in red).

4.1. Neighbourhood Descriptors
Unlike 2D image fragments, 3D surface fragments constitute an irregular and unstructured sampling of a surface.
As a consequence, there is no canonic distance measure to
quantify the alikeness of two fragments. Therefore, a neighbourhood descriptor (together with a corresponding distance
function) has to be defined. In a recent approach, Zelinka et
al. [ZG04] have shown so-called Geodesic Fans to faithfully
identify regions on a surface that are geometrically similar.
Their descriptor is a vector of N discrete samples of one or

44

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

Figure 2: Descriptor layout. Left: The number of sample
points per ring grows linear with respect to its length, i.e.
the sampling rate for each ring is identical (Four samples
per 2πr in the depicted case). Centre: Descriptor as suggested in [ZG04]; here, the number of sample points per ring
is constant, such that the sampling rate decreases linearly.
Right: 3D illustration of the local resampling.

several signals defined on the surface. The samples are taken
at N fixed sample positions according to some sampling pattern given in geodesic polar coordinates (fig. 2, middle).
Dealing with point sets, the computation of geodesics is an
ill-defined and expensive operation. Nevertheless, we adopt
the geodesic fan approach to our setting by deriving a locally
regular resampling of the point cloud that can then be used
to come up with a straight-forward descriptor. To this end,
we choose the sampling pattern depicted in fig. 2 (left) (as it
does not emphasise the regions close to the centre) and interpret these N sample positions as polar coordinates in the
parameter plane described above, scaled to fit into the parameter domain of the fragment. We define as the fragment
shape descriptor the vector of height values of this local regular resampling over the parameter plane:
p
p t
χ(p) = h1 , . . . , hN ∈ N .

R

Please note that although on coarser scales where fragments
are of larger size (see below) a considerable number of
points may have to be resampled, this can be performed
efficiently using natural neighbour interpolation techniques
that are performed directly on the GPU [KEHKL∗ 99]. Alternatively, interpolated height values for the new sample
points can also be computed quite efficiently by constructing a 2D Delaunay triangulation and using the barycentric
coordinates of the sample positions for interpolation.
An obvious choice for a distance metric on the space of
descriptors would be the weighted Mean Squared Error
1
p
q
wi ||hi − hi ||2 ,
(1)
N∑
i
with some appropriate weights wi . However, as the sampling
pattern is uniquely defined only up to the direction of the xaxis (see fig. 2), we allow for a set of transformations ϕ corresponding to discrete rotations and mirroring to be applied
to the descriptor before evaluation of eqn. 1 and perform linear interpolation along the rings where necessary [ZG04].
δ(χ(p), χ(q)) =

Obviously, for some points in the data set, the parameter
domain will stretch into regions containing points with con-

fidence value < 1 (points that have been inpainted in some
previous step) or no points at all. The basic idea is that descriptor values corresponding to these regions should contribute less to the selection of appropriate target-candidate
pairs. This is achieved by setting the weights in eqn. 1 to
wi = αi (p) · αi (q), where the αi (p) are the interpolated confidence values of the points in N (p). For descriptor values
corresponding to empty regions in N (p) we assign a confidence value of zero.
4.2. 1-Level Inpainting
Before we can start reconstructing the missing surface in
hole regions, we have to detect the hole boundary first. While
trivial in the case of triangle meshes, this boundary detection
is a difficult problem when dealing with point sets. To solve
this, we use the approach very recently presented in [Sch05],
that robustly identifies loops of boundary points. The basic
concept is now to find for every boundary point b ∈ B an appropriate candidate c ∈ P and to copy its neighbourhood to
the invalid (empty) parts of N (b). To guarantee that invalid
regions in N (b) can indeed be filled with the corresponding
regions in N (c), the candidate set C is built by collecting all
points p ∈ P, whose descriptors do not contain any invalid
components, i.e.

C = c ∈ P | hci valid ∀i .
With a suitable candidate set and a discriminative descriptor
at hand, inpainting simply consists of finding the best matching candidate cb for any boundary point b and co-aligning
cb ’s local frame with the frame of N (b).
In order to reduce the time required for searching a best
matching candidate, we apply the tree structured vector
quantisation method (TSVQ, [WL00]) to the candidate set.
By means of the TSVQ the search for a best matching candidate is significantly accelerated and renders the filling procedure interactive even for large candidate sets.
In addition to the minimising transformation from sec.
4.1, we also perform one ICP-step for the best matching candidate, taking into account the descriptor samples only and
using fixed correspondences in order to compensate for little
deviations that might result from the discreteness of our set
of considered rotations.
Finally, all points from N (c) corresponding to invalid regions in χ(b) are inserted into P, receiving the aggregated
confidence from eqn. 2 of the target descriptor used to compute the match. Afterwards the sets B and C are updated.
4.3. Structure-Driven Inpainting
So far, we have described an algorithm that performs filling
in random order. Analogue to what was noted for images by
Criminisi et al. [CPT03], this often has the adverse effect
that flat surface regions are unduely propagated into the hole
region. Our algorithm tackles this by assigning priority values to all possible targets and performing a best-first filling
c The Eurographics Association 2005.

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

algorithm. The priority values are computed to favour those
targets which are on the continuation of strong features and
are surrounded by a high confidence neighbourhood.
Unfortunately, feature detection on point sets in itself is a
non-trivial task, let alone in the presence of holes. We therefore use a simple heuristic to measure the expressiveness of
a fragment: We compute the standard deviation σ of the descriptor values along the sampling rings depicted in fig. 2.
By means of this criterion, regions of high curvature are preferred over flat regions. Please note that this criterion is welladapted to our hierarchical setting described in sec. 5, as here
the fragment size corresponds to the amount of detail contained in the fragment and therefore σ faithfully encodes the
probability of the presence of a feature on the resp. scale.
The confidence of the neighbourhood is measured by
means of the aggregated confidence value
1
p
(2)
α(χp ) = ∑ α(hi )
N i
that is computed for every target descriptor.
We experimented with several combinations of the two
criteria α and σ to prioritise the filling operations. According to our experiments, a threshold approach performed best:
Among those target descriptors that have the highest confidence value (quantised into ten bins), we choose the one with
highest σ to be filled first. This way, of those target fragments
with a high confidence we favour the most discriminative.
It is worth noting that our algorithm can of course trivially be modified to a semi-automatic approach, where a few
appropriate candidates are suggested to the user, who then
selects the one to be pasted into the target region.
5. Hierarchical Formulation
The essence of exemplar-based completion is to exploit coherence and similarity between the region of interest and appropriate candidate regions of the considered object. Geometric properties of the hole region, though, might be represented in different scales, and in many cases similarity
relations present in different scales correspond to very different regions on the object. It is therefore important to allow candidates to stem from the optimal object region per
scale, such that for instance the bunny’s missing left knee
(see figs. 6/7) is reconstructed on coarse levels by copying
the bunny’s right knee, whereas the fur structure, exhibiting
different similarity relations, is reconstructed from various
different locations on the bunny’s back.
Only in the presence of real symmetry, where similarity
on all considered scales happens to relate to the same candidate region, the one-level approach described in the previous section is sufficient. For instance, the missing left eye
and nose region of the Max-Planck-model (as shown on the
title page of this paper) can be reconstructed using transferred and mirrored copies of parts of the opposite side of the
face. This type of similarity relation ranging over all considered scales – rarely encountered when dealing with images
– is relevant for large classes of 3D objects. Nevertheless,
c The Eurographics Association 2005.

45

Figure 3: David’s head, subsampled to 300000 points, original (top left), iteratively smoothed once with k = 100 (top
right), after 5 (bottom left), and 8 iterations (bottom right).
The discs indicate the corresponding neighbourhood size.

in order to handle cases as described above, we propose a
hierarchical, multi-level approach, whose first step is to create a point set hierarchy P 1 , . . . , P L with according scales
ρ1 , . . . , ρL , each point set representing the (defective) object
and its geometrical properties up to the according scale.
5.1. Creating the Point Set Hierarchy
We approximate the scale-space representation of the input
model by iteratively applying Laplacian smoothing, deriving
coarser and coarser scales, corresponding to ever larger kernel widths. Specifically, to derive P l+1 from P l we compute
new point positions pl+1 = 1µ ∑kj=1 µ j plj , as the weighted
mean of all k nearest neighbours plj of pl , where µ = ∑ j µ j .
(Actually, we perform the smoothing in direction of the surface normal only, as we wish to smooth the surface itself,
rather than the distribution of sample points in the surface).
This corresponds to smoothing P l with a kernel width of
ρp = max d(p, plj ).
j=1,...,k

The average distance to the kth -nearest neighbour
1
ρp
n∑
is called the k-Ring Radius and describes a natural size of
the local neighbourhood patches, as it contains all the detail information up to the respective hierarchy level, with all
higher level detail information smoothed out (see fig.3).
ρ=

46

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

Target Descriptor (2-Layer)

Candidate Descriptor (2-Layer)

original
level h
level h+1

Defective Target Surface

Target
Descriptor
(level h / h+1)

Figure 5: The 2-Layer descriptor for the situation in fig. 4

Ideal Candidate

Candidate
Descriptor
(level h)

Candidate
Descriptor
(level h+1)

Figure 4: Defective target surface and an ideal candidate
(bold), together with two levels from the scale-space representation (dashed, level l+1 filled, level l incomplete). Updating target descriptor values invalid on level l using the
guidance surface from level l+1 leads to a descriptor (bottom left) that is not well comparable with either of the candidate descriptors (bottom centre / right).

We are aware that our smoothing scheme has two main drawbacks: On the one hand, it is a well-known fact that Gaussian
filtering causes shrinkage and ultimately converges to a single point — this causes no harm to our method, as all comparisons are evaluated on each level separately and consequently any potential shrinkage is cancelled out. On the other
hand, as the points contributing to the new, filtered point positions are a fixed number of nearest neighbours, the sampling density influences the smoothing. Strictly speaking,
claiming that a certain "scale" is represented in a smoothed
point set, therefore holds only for roughly uniformly sampled point sets.
To address this drawback, more sophisticated filtering methods could be applied, in the spirit of [KH98, CDR00] or
[Tau95]. However, in our setting the approximated scales are
used to guide the filling process only and therefore our simple approach proved to be sufficient.
5.2. Multi-Level Inpainting
Based on the point set hierarchy P 0 , . . . , P L , we formulate
the inpainting as a bottom-up process, filling the hole in the
coarsest scale representation P L first and then consecutively
on the finer levels up to the finest level P 0 . In each step
(aside from the first step, where P L is completed using the
non-hierarchical formulation of our algorithm as described
in sec. 4), we use the previous, next coarser level point set
to construct a guidance surface that can be used in the target descriptors for the filling step on the current level. This
way we can encode hints to the larger scale geometry into
the descriptor components that have been invalid till now
and hence neglected. Let the Guidance Surface G l be any
implicit representation of the (completed) point set P l+1 . In

our approach, we use the zero set of the MLS-approximation
of P l+1 , but any other locally evaluable representation could
also be applied. A straight-forward approach (that would
also resemble comparable approaches in 2D image processing) would then be to assign height values to invalid target
descriptor components by sampling G l (see fig. 4, bottom
left). This straight-forward approach, however, would have
the adverse effect that even ideal candidates would not be
considered a perfect match. The reason for this is that inserting samples from G l to the current level’s descriptor inherently causes two scales to be mingled. The resulting hybrid
descriptor – incorporating two scales at the same time – is in
fact comparable to descriptors on neither the current level l
nor the coarser level l+1. This fact is illustrated in fig. 4.
5.3. 2-Layer Descriptor
As a consequence, we define 2-layer descriptors as illustrated in fig. 5:
• The bottom layer χl+1 , is constructed as described in sec.
4.1, with the exception that height values are derived from
the zero level set of the MLS-approximation of P l+1 .
• The top layer χl is constructed using the same parameter
plane and the same sampling pattern, but capturing the
available local geometry from P l only, and assigning zero
confidence to the invalid descriptor components.
The distance function for the two-layer descriptor is then
simply a weighted sum of the distance functions per level:
δ(χ(p), χ(q)) = δ(χl (p), χl (q)) + τ δ(χl+1 (p), χl+1 (q)).
While the parameter τ is arbitrary in principle, a value of
0.3 has proven to produce good results in our experiments.
In cases where multiple hierarchy levels are reconstructed,
it is advisable to increase τ for finer levels, as they can be
expected to be already a reliable reconstruction.
As stated above, the coarsest level L is filled without guidance (formally setting τ = 0), as there is no previous reconstruction available for evaluation as guidance surface. Given
that the coarsest level’s scale size corresponds well to the
scale of the hole, this problem is sufficiently well-posed.
However, for very large holes, considerable filtering might
be necessary to this end. In this case a natural and trivial extension to our algorithm is to use any one of the available
smooth hole filling schemes for the coarsest level and use
the result as guidance surface G L .
c The Eurographics Association 2005.

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

47

Figure 6: Hierarchical reconstruction of the Stanford Bunny. First, a point set hierarchy (l = 0 to 3) of the defective bunny is
constructed (bottom). Starting with l = 3, each level is filled per-se, where in level l, the level l + 1 serves as Guidance Surface.

Figure 7: Reconstruction result.

Figure 8: Hierarchical reconstruction of the David Head Model.

6. Results
We applied our fragment-based inpainting algorithm to various data sets of point sampled geometry. The objects depicted in the images of this paper exhibit holes in structured
surface regions and are in addition to this comparably large
in size. Reconstructing the surface for these holes using traditional smooth hole filling algorithms would have lead to
disturbing visual artifacts.
The figure on the title page illustrates the basic workflow
of our algorithm. For target fragments (illustrated as green
discs) an optimal candidate fragment (red discs) is identified. The points corresponding to invalid target regions are
pasted into the point set after the according transformations
(translation, rotation, optional mirroring), which are deduced
from the descriptor comparisons, are applied. In near symmetric cases like faces, the non-hierarchical formulation of
our algorithm already gives satisfying results, given that the
required scale to cover the hole can be represented without
scale-space segmentation.
Figs. 6 and 7 demonstrate the use of our hierarchical formulation for the exploitation of similarities that are spread
over several scales. Using our approach we were able to reconstruct both the knee as a symmetrical large scale feature
and the fur structure that itself does not exhibit an analogue
symmetry, but is also well presented as a coherent feature on
the surface on finer scales. During the coarse level inpainting
steps, corresponding target-candidate descriptor pairs were
identified. In this example, prioritising the target fragments
for filling according to their discriminativity was particularly
useful. This way, the target regions close to the bunny’s knee
c The Eurographics Association 2005.

were selected for filling first. During the finer scale filling
operations, the fur structure was transferred to the hole region from various (other) regions on the bunny’s back.
Also, the david head model from the Michelangelo project
(fig. 8) could not have been filled using a 1-level approach, as
the model itself does not contain appropriate candidates that
correspond to the (unknown) hole regions’ full spectrum of
scales. By filling the hole for coarser regions, representing
the basic geometry, first, and adding more and more detail
with decreasing neighbourhood size at later stages, our algorithm was able to inpaint this hole in a visual believable
way, taking into account the objects global and local context.
In order to assess the influence of the automatically computed guidance surface and the candidate set on the inpainting results, we reconstructed the bunny data set with the help
of the complete point set itself as guidance surface and candidate set. The combination of both, perfect guidance and
perfect candidate set, resulted in the perfect reconstruction of
the bunny (fig. 9). As opposed to that, figs. 6 and 7 show the
hierarchical reconstruction of the incomplete bunny without
any additional knowledge.
7. Conclusions & Future Work
Inspired from exemplar-based techniques in 2D image processing, we have introduced in this paper a novel method
for the filling of holes in structured point set surfaces. In
order to be able to recognise and exploit similarity and coherence properties in the object, we derived target and candidate fragments, each living in their specific scale with a

48

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

Figure 9: From left to right: Original Bunny data set, data set with artificially introduced hole, iterative repair steps, resulting
reconstruction. In this example, the candidate set and the guidance surface have been built from the original bunny data set.

Figure 10: Original Aphrodite data set (left), an incomplete scanning session and its iterative completion.

naturally defined fragment size that is well correlated to the
respective scale of the filling operations. In addition to that,
the fragments are defined in local frames, thereby making
our algorithm insensitive to similarity transformations as rotation, translation and scaling.
Being based on a scale-space representation of the object,
our hierarchical algorithm is able to robustly identify and exploit similarity relations between the region of interest and
possibly various other locations on the surface, depending
on the respective scale.

tensor voting. In Conf. on Computer Vision and Pattern Recognition (CVPR 2003)
(Madison, WI, USA, 16-22 June 2003), IEEE Computer Society, pp. 643–650.
[KEHKL∗ 99] K ENNETH E. H OFF I., K EYSER J., L IN M., M ANOCHA D., C ULVER
T.: Fast computation of generalized voronoi diagrams using graphics hardware. In
SIGGRAPH ’99: Proc. of the 26th annual conf. on Computer graphics and interactive
techniques (New York, NY, USA, 1999), ACM Press, pp. 277–286.
[KH98] K ARBACHER S., H ÄUSLER G.: New approach for the modeling and smoothing
of scattered 3d data. In Proceedings of the Conference on 3D Image Capture and
Applications (San Jose, CA, USA, January 1998), Ellson R. N., Nurre J. H., (Eds.),
vol. 3313 of SPIE Proceedings, SPIE, pp. 168–177.
[KS05] K RAEVOY V., S HEFFER A.: Template based mesh completion. In Symposium
on Geometry Processing (2005), pp. 13–22.

References

[KZ04] K LEIN J., Z ACHMANN G.: Proximity graphs for defining surfaces over point
clouds. In Symposium on Point-Based Graphics (June 2004).

[BBC∗ 01] BALLESTER C., B ERTALMIO M., C ASELLES V., S APIRO G., V ERDERA
J.: Filling-in by joint interpolation of vector fields and gray levels. IEEE Transactions
on Image Processing 10, 8 (August 2001), 1200–1211.

[Lie03] L IEPA P.: Filling holes in meshes. In SGP’03: Proceedings of the Eurographics/ACM SIGGRAPH symposium on Geometry processing (2003), Eurographics Association, pp. 200–205.

[CDD∗ 04] C LARENZ U., D IEWALD U., D ZIUK G., RUMPF M., RUSU R.: A finite
element method for surface restoration with smooth boundary conditions. Computer
Aided Geometric Design 21, 5 (2004), 427–445.

[PG01] PAULY M., G ROSS M.: Spectral processing of point-sampled geometry. In
Proceedings of the 28th annual conference on Computer graphics and interactive techniques (2001), ACM Press, pp. 379–386.

[CDR00] C LARENZ U., D IEWALD U., RUMPF M.: Anisotropic geometric diffusion
in surface processing. In Proceedings of the conference on Visualization ’00 (2000),
IEEE Computer Society Press, pp. 397–405.
[CL96] C URLESS B., L EVOY M.: A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer
graphics and interactive techniques (1996), ACM Press, pp. 303–312.

[PMG∗ 05] PAULY M., M ITRA N. J., G IESEN J., G ROSS M., G UIBAS L.: Examplebased 3d scan completion. In Symp. on Geometry Processing (2005), pp. 23–32.
[SACO04] S HARF A., A LEXA M., C OHEN -O R D.: Context-based surface completion.
ACM Trans. Graph. 23, 3 (2004), 878–887.
[Sch05] S CHNABEL R.: Detecting holes in surfaces. Central European Seminar on
Computer Graphics (CESCG05) (TO APPEAR), May 2005.

[CPT03] C RIMINISI A., P ÉREZ P., T OYAMA K.: Object removal by exemplar-based
inpainting. In Conference on Computer Vision and Pattern Recognition (CVPR 2003)
(Madison, WI, USA, 16-22 June 2003), IEEE Computer Society, pp. 721–728.

[SK02] S AVCHENKO V., KOJEKINE N.: An approach to blend surfaces. Advances in
Modeling, Animation and Rendering (2002), 139–150.

[DCOY03] D RORI I., C OHEN -O R D., Y ESHURUN H.: Fragment-based image completion. ACM Trans. Graph. 22, 3 (2003), 303–312.

[Tau95] TAUBIN G.: A signal processing approach to fair surface design. In Proc. of
the 22nd annual conf. on Computer graphics and interactive techniques (1995), ACM
Press, pp. 351–358.

[DMGL02] DAVIS J., M ARSCHNER S. R., G ARR M., L EVOY M.: Filling holes in
complex surfaces using volumetric diffusion. In Proceedings of the 1st Int. Symp. on
3D Data Processing Visualization and Transmission (Padova, Italy, june 29–21 2002),
Cortelazzo G. M., Guerra C., (Eds.), IEEE Computer Society, pp. 428–438.

[VCBS03] V ERDERA J., C ASELLES V., B ERTALMIO M., S APIRO G.: Inpainting
surface holes. In IEEE International Conference on Image Processing (ICIP 2003)
(Barcelona, Spain, September 2003).

[GSS99] G USKOV I., S WELDENS W., S CHRÖDER P.: Multiresolution signal processing for meshes. Computer Graphics Proceedings (SIGGRAPH) (1999), 325–334.
[HDD∗ 92] H OPPE H., D E ROSE T., D UCHAMP T., M C D ONALD J., S TUETZLE W.:
Surface reconstruction from unorganized points. In Proc. of the 19th annual conf. on
Computer graphics and interactive techniques (1992), ACM Press, pp. 71–78.
[JT03]

J IA J., TANG C.-K.: Image repairing: Robust image synthesis by adaptive nd

[WL00] W EI L.-Y., L EVOY M.: Fast texture synthesis using tree-structured vector
quantization. In SIGGRAPH ’00: Proceedings of the 27th annual conference on Computer graphics and interactive techniques (2000), ACM Press/Addison-Wesley Publishing Co., pp. 479–488.
[ZG04] Z ELINKA S., G ARLAND M.: Similarity-based surface modelling using
geodesic fans. In SGP’04: Proceedings of the Eurographics/ACM SIGGRAPH symposium on Geometry processing (July 2004), Eurographics Association.

c The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

3D Modeling for Non-Expert Users
with the Castle Construction Kit v0.5
Björn Gerth1
1Institut

René Berndt1

Sven Havemann2

Dieter W. Fellner2

für ComputerGraphik, TU Braunschweig, Germany

2ComputerGraphik

& WissensVisualisierung, TU Graz, Austria

Abstract
We present first results of a system for the ergonomic and economic production of three-dimensional interactive
illustrations by non-expert users like average CH professionals. For this purpose we enter the realm of domaindependent interactive modeling tools, in this case exemplified with the domain of medieval castles. Special emphasis is laid on creating generic modeling tools that increase the usability with a unified 3D user interface, as well
as the efficiency of tool generation. On the technical level our system innovates by combining two powerful but
previously separate approaches, the Generative Modeling Language (GML) and the OpenSG scene graph engine.
Categories and Subject Descriptors (according to ACM CCS): H.5.1 [Information Interfaces]: Multimedia Information Systems, I.3.6 [Computer Graphics]: Interaction Techniques

1. Introduction
Interactive three-dimensional illustrations have a huge potential for engaging the public audience in museums and exhibitions, as well as for the exchange of scientific hypothesis about the past among researchers. But expectations are
high due to the ubiquitous use of 3D, which is slowly but
steadily becoming a standard in the entertainment and edutainment sectors. ’Serious games’ are expected to deliver the
same level of quality as known from X-Box and Playstation,
even if realized with only a fraction of the budget.
A central issue is the content authoring problem, in case of
3D also known as the modeling bottleneck. In principle there
are two ways to create 3D objects, namely shape acquisition
(3D scanning) and shape modeling. The focus of our paper
is the latter, for which there are two main applications:
c The Eurographics Association 2005.


• Virtual reconstructions of destroyed cultural heritage
sites to let users explore their ancient form at a certain
point in history, or a hypothesis about it
• 3D Illustrations as a straight generalization of the familiar 2D drawings in museums exhibitions, e.g., to emphasize the context of the findings and their surroundings
Archeologists, museum curators and art historians usually
have a background in human sciences rather than engineering or computer science. These persons are untrained in
3D modeling, but they have distinct three-dimensional ideas
about the appearance of historic sites and monuments. But
is there a way to let them express their ideas in 3D? – It is
a very demanding task to provide this particular community
with easy-to-use tools that (i) require no/not much learning,
but (ii) still guarantee high-quality results.

50

Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

Figure 1: Castles in recent computer games: Stronghold2 is a castle and medieval warfare simulation (a), sophisticated placement tools of The Settlers (b), and the game map editor of The Battle for Middle Earth with integrated castle designer (c,d).

1.1. Options for Solving the Modeling Bottleneck
Three-dimensional objects are usually created with sophisticated 3D modeling software, with procedural modelers like
3D Studio Max or Maya, or with parametric CAD software
like AutoCAD or Catia. These tools require serious learning efforts, and since they are all-purpose tools they do not
provide any particular support for creating CH content.
Custom tools exist for the architectural domain, for instance house planning software. It permits average users
to create standard houses easily by using libraries of predefined intelligent 3D components such as staircases, walls,
and windows, and catalogues of furniture. Drawbacks of
house planning software are that (i) the components are for
modern but not for ancient architecture and (ii) due to the
proprietary format new intelligent components can not be
added by normal users but only by programmers.
Modelers such as AutoCAD and Maya are extensible
through their built-in scripting languages (AutoLisp and
MEL) to let users add specialized modeling functionality by
programming. This is a way to speed up the modeling, but
it leads to another problem, the separation of modeling from
viewing: The finished models are exported to some exchange
file format (DXF, VRML) and loaded into a runtime viewer,
but there any specialized modeling functionality is gone. The
export throws away the high-level semantic modeling information, which precludes any on-line high-level editing – but
the possiblity to change objects is a major asset when choosing interactive 3D graphics as a media form!
The perceived level of engaging dynamic interaction can
be greatly enhanced using game engines. They provide fluent interaction, best possible rendering quality, animated
characters (virtual enemies), event-driven interactions, and
possibly even physical simulations. But there are some serious issues with using game engines, among others:
• authoring cost: the game industry counts in man years
• sustainability: short lifecycle of game technology
• re-usablity:
proprietary game data structures are a
dead end, no later re-use of models created
• generality:
rendering is optimized for low-poly,
multires-models, no custom renderers supported

The last issue is of great importance in the CH context (and
especially for Epoch) where many sophisticated surface representations have been developed that need to be displayed
together. This is witnessed by the publications of previous
VAST conferences, just to mention densely sampled triangle meshes [BC∗ 04], point clouds [DD∗ 04], or high-quality
BTF rendering [MMK04], e.g., for precious artifacts.
To summarize the requirements of a CH authoring system: An extensible set of CH specific modeling tools, represented in a non-proprietary, sustainable format standard,
combined with a CH presentation system, into which also
non-standard rendering methods can be integrated.

2. Related Work on Domain-Dependent Modeling Tools
The problem of creating custom tailored modeling tools has
not received much attention so far as a research topic of
its own. Extensive texts on modeling like the Handbook of
CAGD only mention the fact that domain-dependent tools
are important for practical shape design, but do not explain ways to realize them [HJA02]. There is no clear picture or systematic survey, the aforementioned proprietary approaches (MEL, AutoLISP, etc) clearly dominate. Formats
such as 3DS (3DStudioMax) or Collada [Col] allow for storing animations and for attaching some high-level information. – One lesson learned from VRML, however, is that any
sort of non-trivial interaction requires programming, i.e., to
resort to a second formalism, such as JavaScript.
The absence of any sort of standard format for domaindependent, or procedural, modeling tools has also been
identified as a major problem for the manufacturing industry: Models containing intelligent 3D parts (’features’) are
’frozen’ when they are transferred from one parametric modeler to another, e.g., from SolidWorks to Catia: Intelligence
is lost. Some interesting background information about the
fundamental obstacles experienced by the STEP consortium
trying to solve this problem was given by Michael Pratt on
SMI 2004 [Pra04]; a detailed discussion is part of [Hav05].
A thrilling new aspect was recently added to the whole
subject by a new generation of computer games from the
simulation genre. The title Stronghold2 terms itself the ultic The Eurographics Association 2005.


Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

51

Figure 2: User-defined world in The Settlers. High quality and great variety is obtained at the same time: The game provides
about 60 different pre-defined building types, each with a number of configuration options on several levels of extension.

Fig. 1 shows such streamlined modeling tools in action, the
quality of a typical world can be judged in Fig. 2. Similar
as with Lego vs. clay, modeling efficiency is obtained by
radically reducing the degrees of freedom (DOF), or, more
suitably, by exposing only the essential DOFs of the model.
An important maxim for domain-dependent modeling tools
is: Not everything can be changed, but everything that can
be changed can be changed efficiently.

3. Technical Foundations: GML and OpenSG
Our approach combines the strengths of two technologies
that were previously unrelated:
• The Generative Modeling Language (GML) does not
have a scene graph, so all models exist in the same coordinate system, and there are no multiple shape instances.
• The OpenSG scene graph engine provides the ’hooks’ to
change all aspects of the scene at runtime, but it has no
scripting language. Hitherto, all types of dynamic changes
must be programmed in C++, i.e., defined at compile time.

Figure 3: Several typical castles from [Koc00]. They are all
composed of similar elements, but each has a different style.

It turns out that both technologies fit surprisingly well together. Their tight integration opens a number of very interesting options, some of which are sketched in this paper. But
first a short introduction into the ingredients.

3.1. The Generative Modeling Language
mate castle simulation [Str05]. The latest Settlers from BlueByte or the Lord of the Rings series from Electronic Arts
play in a fantasy world that can be designed to a large extend by the user [Set05, Lor05]. Interesting with respect to
domain-dependent modeling is that greatest emphasis is put
on usability: Gamers are unpatient and not willing to spend
precious game time on impractical modeling tools:
•
•
•
•

generality is sacrificed for clarity of actions
only few but very well-chosen options
no WIMP-style GUI but freely floating graphical objects
optimized with respect to mouse movements

c The Eurographics Association 2005.


The GML has been sucessfully employed in a CH context
for the procedural construction of Gothic window tracery
as shown on VAST 2004 [HF04]. Its important novel feature is that (i) it can encode procedural modeling tools and
(ii) it contains a runtime engine to apply these tools interactively. So the GML bridges the gap between modeling and
viewing, it can be seen as a viewer with integrated modeling
capabilites. This permits for an extremely concise encoding
for the web-based transmission of highly complex models of
a procedural kind [BFH05]: Instead of 3D objects transmit
only the modeling operations that create these 3D objects.

52

Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

Figure 4: Interactive dragging of Arkade CVs. It uses a customized version of the polygon editor with geometric error checks.

The GML is a stack-based programming language, similar to Adobe’s PostScript, which solves the nasty code generation dilemma: With procedural tools modeling becomes
very similar to programming. But it is not tolerable to replace interactive shape design by literal programming using an ascii text editor; furthermore not all good artists are
also good programmers. PostScript, on the other hand, is
the ’invisible programming language’. Code can be – and
is! – generated automatically with ease: Whenever printing
a PostScript document the printer actually executes a computer program that produces the bitmap that is printed on
paper – technically, as a side effect of program execution.

3.2. The OpenSG Scene Graph Engine
OpenSG is an open source scene graph system with builtin support for (i) multi-threading and (ii) cluster rendering.
This feature will be of greatest importance in the near future
to assure scalability: Parallel computing on multi-core CPUs
(e.g., with Hyperthreading), is the only option to increase the
processing power further when the clock speed comes close
to physical barriers, which is the case already today. OpenSG
clusters are driving transparently and efficiently tiled projection screens and multi-projector units, including several
CAVE systems.
OpenSG can also digest dynamical changes to the scene
graphs: All scene relevant data exist in several aspects that
are replicated among the render clients. Changes are logged
in a change list that helps to synchronize the data periodically, typically once per frame.
OpenSG uses very consistently the node-field paradigm:
A single field is an atomic piece of data, e.g., an integer,
float, string, or a reference, etc. A multi field is a (dynamic)
uniform array of single fields, i.e., an array of integers, or of
floats, or of strings etc. A field container is very much like
a class in object oriented programming, or like a record in
a relational database: It is just a list of several named fields,
which can be single or multi fields. The scene graph itself
is a tree made up of nodes. A node is a field container with
fields parent, children, and core. The single field parent and
the multi-field children are references to other nodes, and
the single field core references a node core. A node core

1
2
3
4
5
6
7
8
9
10
11
12

osg-getroot
/Transform osg-corednode
(10,0,10) osg-translate
dup
/Cylinder osg-primitive
dup begin
/sides
20 def
/height
15 def
/botRadius 2 def
end
osg-addchild
osg-addchild

%
%
%
%
%
%
%
%
%
%
%
%

push scene graph root
create node+core
change transform
stack = root trans trans
create node+core
push cylinder scope
fake dict
fake dict
fake dict
pop cylinder scope
cylinder child of trans
trans child of root

Figure 5: Creation of a Cylinder primitive in GML and insertion into the OpenSG scene graph. The osg-translate operator can be called at any time to move or animate objects.

is a field container that contains actual data, e.g., a transformation matrix, or some geometry that is to be rendered.
The separation between (lightweight) node and (potentially
heavy) node core is important for multiple instancing of the
same geometry for, e.g., all the identical chairs in a theatre.
3.3. The Combination of GML and OpenSG
The combination has two parts, (i) exposing the OpenSG
API to the GML language, and (ii) integrating the GML runtime engine into the (larger) scene graph engine. Both parts
are only quickly sketched here, for details see [Ger05].
All functionality in the GML comes from operators. Only
two higher-level data structures are built in, (heterogenous)
arrays and dictionaries. A dictionary is a list of (name,token)
pairs, where the token can either be an atomic value, or refer
to an array or another dictionary. Note the striking similarity to the node-field paradigm in OpenSG from above. So
the idea was to treat OpenSG field containers in the GML as
fake dictionaries. This could even be realized in a generic
way with the reflectivity capability of OpenSG: Every field
container type registers itself at startup with a field container
factory that records the type, numerus (single/multi), and
name of the fields in the container. The VRML-like field
c The Eurographics Association 2005.


Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

53

Figure 7: Castle wall editor. In insertion mode corner points
can be added by clicking on the red polygon gizmo (1a,b).
In move mode they are represented by red discs that can
be interactively dragged until the desired shape is reached
(2a,b). In tower mode, towers are automatically inserted so
that all portions of the wall can be defended well (3a,b).

Figure 6: Ground layout of typical castles adapting to the
local ground topography. Examples from [Koc00].

container type Cylinder for instance, a node core, can be understood as a GML fake dictionary with entries sides, height,
and radius. It can now be created and integrated in the scene
graph with a GML code snippet as shown in Fig. 5. Note that
this technique makes it possible to translate VRML files fully
automatically to GML+OpenSG; but this will be described
in detail in a separate paper.
The main GML shape representation are combined
B-reps. They can represent both polygonal and smooth
free-form shapes in the same data structure using one
sharp/smooth flag per mesh edge: Faces with a smooth edge
are rendered as Catmull/Clark subdivision surfaces [HF01].
The GML interpreter operates only on a single mesh. In principle this is not a limitation since a mesh can contain an arbitrary number of connected components (3D objects). But
all these objects live in the same global coordinate space,
so the only way to move one object is to modify the positions of its vertices. – This problem has been solved by allowing the GML to switch the current mesh. An OpenSG
c The Eurographics Association 2005.


scene graph can therefore now contain any number of separate cB-rep meshes, stored in geometry node cores of type
BRepCombinedFC (field container). Any such node can be
made current, and may be interactively modeled, at runtime.
The low-level integration of combined B-reps into OpenSG
asserts that all mesh changes are even propagated through a
cluster of render client. So all client PCs compute and display a consistent tesselation of the visible combined B-rep
meshes even if they are modified.
4. The Castle Construction Kit
The new possibilities from the last section are a perfect
match to the technical requirements from section 1.1 for
authoring/presentation software for CH content. Domaindependent modeling tools are defined with respect to a domain. As example domain we chose medieval castles, and
our Castle Construction Kit (GML-CCK) was realized as
part of another diploma thesis [Ber05]. – First we had to
understand the domain. We found valuable material on castles in literature on architecture. Very helpful was the book
from Koch on construction forms, where all the building elements and components from different era and styles are ex-

54

Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

Figure 8: Staufer castles, e.g., in Sicilia, have a very clean
structure. Images are from [Koc00].

Figure 9: Very simple Staufer castle with a few mouse clicks.

plained [Koc00]. Some of its pencil illustrations are reproduced here to show the input material we had: Perspective
drawings (Fig. 3) and ground plans (Fig. 6).
Our idea was to create modeling tools that permit to
quickly produce 3D look-alikes that are not necessarily precise, but that contain the same structure as the original. Interestingly, interest on castle (re-)construction was also expressed from the fantasy role playing community. The workflow is depicted in the cover illustration on the first page:
(a) historical original castle (Scharfeneck from [Koc00]), (b)
adaptation for role playing from [Rad98], (c) 3D look-alike,
and (d) tesselation detail. Historically, castles have developed out of simple wooden ring walls on a small hill (Motte
in German, see Fig. 14).

Figure 10: The crenellation style is a wall attribute.

Most important for the shape of a castle is the shape of the
landscape. The location can greatly support a fortification.
Height isolines can often be recognized in the ground plan
of a castle adapting to a hill, see images Fig. 6 (3b,4a) in
row 3 (right) and row 4. But at first, to keep things simple,
we chose to start with castles in the plane.

ent editing modes. A gizmo is a 3D object that is artificially
set into a 3D scene to represent a certain operation. The wall
editor provides an insertion gizmo (red closed polygon), a
motion gizmo (points as discs), and an option gizmo that
presents a 3D menu with captions floating space, using balls
as switches (Fig. 7, 3a).

4.1. Castle Wall Editing

4.2. Re-usable Interactive Tools

As suggested by ground layout examples (Fig. 6) a good first
representation of the castle walls is a closed simple polygon:
A sequence of straight wall segments is connected with elements such as corners, towers, semicircular towers (with
open gorge), that are placed in the polygon vertices. Which
connection to build depends on the rules of warfare: For the
defenders it must be possible to strike every portion of the
wall, e.g., by bow and arrow. This is in fact a geometrical
problem, which we have built into our wall editor. A typical
wall editing session is shown in Fig. 7.

As it turns out a whole number of objects can be suitably represented by polygons, e.g., arkades, houses, and palisades.
So it is sensible to further develop and provide more sophisticated polygon editing modes, as all entities that are based
on polygons will immediately benefit from improvements in
the polygon editor. From the user’s point of view the re-use
of existing 3D gizmos/editors is highly desirable as well,
since it reduces the learning effort for the user interface.

The wall editor uses gizmos to clearly indicate the differ-

So the challenge is to design re-usable gizmos/editors in
a uniform but flexible, customizable fashion, suitable for
many different elements: Care must be taken to avoid erroc The Eurographics Association 2005.


Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

55

Figure 11: Creation of houses and palisades using variants of the polygon editor. A new house is created from a list of available
house types (1a). It can be moved (1b) and rotated (1c) as a whole prior to editing the outline polygon (1d). Row 2: Adaptation
of the ground polygon by alternating insertion and movement actions. A palisade is created very much like an arkade (3b-d).

The house and palisade editors are shown in action in Fig.
11. Although the house appears to be classical rectangular,
its ground polygon can in fact be freely edited. The roof is
constructed fully automatically (2d,3a) from the straight line
skeleton of the house polygon [AA96]. Interestingly, the roof
is created with the same extrudestable operator as the window tracery in [HF04]. The house editor adds two more operations, move and rotate, to the polygon editor that made
not much sense for the wall polygon. The move operation is
also customized, since it checks whether a house is going to
penetrate a wall, in which case the house snaps to the wall
and aligns with it.
Figure 12: Movements become efficient only with a scene
graph. It permits to animate elements like a water wheel.

neous states during editing and, therewith, user frustration.
Any editor for simple polygons should always avoid selfintersections (8-shape). The arkade gizmo in Fig. 4 additionally needs to avoid very acute angles and very short edges,
i.e., the freedom to move the points must be suitably limited. The wall editor in Fig. 7 must additionally take into
account the wall width, and the feasible wall length depends
on whether it has no, one, or two towers at the ends.
c The Eurographics Association 2005.


Note the great benefit of using OpenSG at this point: The
move and rotate operations are realized only on the scene

graph level, which means that also large portions of the geometry can be moved without overhead. We have used the
scene graph also on the object level to add a constantly turning water wheel to a mill, see Fig. 12.
4.3. GML Dictionaries as Element Classes
Recall that our original goal was an extensible architecture
for domain-dependent modeling tools. The danger in a collaborative environment, especially in an open CH context,
is that a huge bunch of incompatible tools may result since

56

Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users
1 dict dup !wall begin
2
/polygon
:polygon
def
3
/cornedTowers
0
def
4
/rotateMidpoint
(0,0,5)
def
5
/crenellationStyle /style-4
def
6
/wallWidth
2
def
7
/wallHeight
5.0
def
8
/gizmo
{}
def
9
10
/model CastleConstructionKit
11
/castle_wall get
def
12
/model-update
{
13
/construction clearmacro
14
model gizmo
15
} def
16
Model begin
17
18
/current-wall :wall def
19
end
20
/construction newmacro def
21
22
model gizmo
23 end

Figure 13: Castle element class in GML: Semantic information about a wall is collected in a dictionary that contains
static data as well as functions to create, modify, and update
the wall. The 3D geometry of the wall and the gizmos is created in line 22 by generic functions operating on the local
wall data thanks to begin, end in lines 1, 23.

the tool developer community is so heterogeneous. This can
only be avoided with interface standards, ’interface’ understood in the API sense, with respect to GML code.
We have solved the interface problem by using a unique
feature of the GML, namely the fact that it is a functional
language that can not only store literal data, but also functions in a dictionary. As mentioned before a GML dictionary
can behave like a class object in the OOP sense, even as a
class with a dynamic set of members. – A generic API for
interactive procedural elements was quickly found; in the
simplest case it consists of only the three functions gizmo,
model, and model-update shown for the wall in Fig. 13. This
function also enters the wall dictionary as the current-wall in
the Model dictionary, where it is globally visible (line 18).
Note that the wall is in fact a special case, since it has
no single gizmo, but a mode dependent gizmo. The gizmo
member of the wall is by default set to the empty gizmo:
The interaction mode function can dynamically replace the
empty gizmo function by the gizmo function for the chosen
interaction mode! – Examples are shown in Fig. 11!

Figure 14: Origin of castles, from [Koc00]: Wooden ring
wall that was gradually transformed into a stone castle.

5. Conclusion and Future Work
We have presented the first working prototype of our Castle Construction Kit. It is very rudimentary, especially compared to what is possible in Stronghold2, but we believe that
it exhibits already all the features that are essential to achieve
what is not possible using games technology, namely to create open libraries of CH specific modeling tools that bring
full modeling capabilites to a CH 3D-presentation, but can
still be rendered with multi-threading on a cluster with a
rendering engine that supports custom render node types.
With our architecture models and modeling tools can be
exchanged, in fact our models contain their own customized
3D modeler. These fundaments laid directly lead to a long
wish-list of improvements, only to mention:
•
•
•
•
•

Differentiate attributes per sub-element (wall, tower)
Integration of terrain heightfields
Better appearance, so far only shape was in focus
Systematically reduce number of authoring mouse clicks
High-precision construction tools for advanced users,
including AutoCAD-like tools for polygon editing.

There are also several long-term goals we will pursue. First,
we would like to understand the castle domain better and
come up with an exhaustive list of sophisticated intelligent
building elements that permit a much better match between
reality and look-alike. Second, we would like to develop
methods to match the model to given data automatically,
or at least semi-automatically. Our third goal is particularly
fancy: We would like to drag a complete castle interactively
over a hilly landscape, and the castle shall immediately adapt
to its new location so that the ’spirit’ of the original castle
is preserved, but also the laws of medieval warfare are respected, so that a plausible castle results in each time step.
c The Eurographics Association 2005.


Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

References
[AA96] A ICHHOLZER O., AURENHAMMER F.: Straight
skeletons for general polygonal figures in the plane. Proc.
2nd Annu. Internat. Conf. Computing and Combinatorics
(1996), 117–126. 7
[BC∗ 04] BALZANI M., C ALLIERI M., ET AL .: Digital
representation and multimodal presentation of archeological graffiti at Pompei. In Proc. VAST 2004 (Brussels, Belgium, 2004), Chrysanthou Y. et al., (Eds.), Eurographics
Association, pp. 93–103. 2
[Ber05] B ERNDT R.: Automatische Codegenerierung mit
der GML (in German). Master’s thesis, Institute of Computer Graphics, Braunschweig Technical University, Germany, 2005. 5
[BFH05] B ERNDT R., F ELLNER D. W., H AVEMANN S.:
Generative 3d models: A key to more information within
less bandwidth at higher quality. In Proc. Web3D ’05
(Bangor, UK, 2005), ACM Press, pp. 111–121. 3
[Col]

Collada project website. collada.org. 2

∗

[DD 04] D UGUET F., D RETTAKIS G., ET AL .: A pointbased approach for capture, display and illustration of
very complex archeological artefacts. In Proc. VAST 2004
(Brussels, Belgium, 2004), Cain K. et al., (Eds.), Eurographics, pp. 105–114. 2
[Ger05] G ERTH B.: Generative Scene Manipulation in
OpenSG. Master’s thesis, Institute of Computer Graphics, Braunschweig Technical University, Germany, 2005.
4
[Hav05] H AVEMANN S.: Generative Mesh Modeling.
PhD thesis, TU Braunschweig, Germany, 2005. 2
[HF01] H AVEMANN S., F ELLNER D. W.: A versatile 3d
model representation for cultural reconstruction. In Proc.
VAST ’01 (New York, USA, 2001), ACM Press, pp. 205–
212. 5
[HF04] H AVEMANN S., F ELLNER D. W.: Generative
parametric design of gothic window tracery. In Proc.
VAST 2004 (Brussels, Belgium, 2004), Cain K. et al.,
(Eds.), Eurographics, pp. 193–201. 3, 7
[HJA02] H OFFMANN C., J OAN -A RINYO R.: Parametric
modeling. In Handbook of Computer Aided Geometric
Design. Elsevier, 2002, ch. 21, pp. 519–541. 2
[Koc00] KOCH W.: Baustilkunde : das Standardwerk zur
europäischen Baukunst von der Antike bis zur Gegenwart,
22 ed. Bertelsmann, Gütersloh, 2000. 3, 5, 6, 8
[Lor05] Lord of the rings: The battle for middle earth.
Electronic Arts Inc, 2005. www.eagames.com. 3
[MMK04] M ÜLLER G., M ESETH J., K LEIN R.: Fast environmental lighting for local-pca encoded btfs. In Proc.
CGI 2004 (June 2004), IEEE, pp. 198–205. 2
[Pra04]

P RATT M.: Extension of iso 10303, the step stan-

c The Eurographics Association 2005.


57

dard, for the exchange of procedural shape models. In
Proc. SMI04 (Genova, Italy, June 2004), pp. 317–326. 2
[Rad98] R ADDATZ J.: Armorium Ardariticum, eine DSASpielhilfe. Fantasy Productions, Erkrath, 1998. 6
[Set05] The settlers: Heritage of kings. Ubisoft Entertainment (BlueByte), March 2005. www.ubi.com. 3
[Str05] Stronghold2: The ultimate castle sim. Firefly Studios, 2005. www.fireflyworld.com. 3

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

3D Face Modeling from Ancient Kabuki Drawings
Weiwei Xu1 Ryo Akama2 Hiromi T. Tanaka1
1

Department of Human and Computer Intelligence, Ritsumeikan University, Japan
2 Art Research Center, Ritsumeikan University, Japan

Abstract
In this paper, we describe a system to reconstruct 3D face model from ancient Japanese Kabuki drawings. Because
of the limitation of input, we deform the face model, which is compatible with MPEG-4 face animation standard,
according to ancient drawings to get the 3D geometry, and then a texture mapping algorithm is used to map Kabuki
make-up onto the reconstructed 3D face model. The deformation and texture mapping algorithms are based on
multi-level radial basis function network, which is an extension of original radial basis function to achieve the
smoothness and precision simultaneously. Experimental results show that the multilevel RBF method can solve
the deformation and texture mapping problems quite well.
Categories and Subject Descriptors (according to ACM CCS): I.3.5 [Computer Graphics]: Computational Geometry
and Object Modeling: Modeling Packages I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism:
Animation

1. Introduction
Kabuki is a traditional form of Japanese theater. It was
founded early in the 17th century, and over the next 300
years developed into a sophisticated, highly stylized form
of theater. There are abundant cultural legacies to describe
the long history of this outstanding art, such as beautiful pictures of Kabuki dance which is called UKIYOE in Japan and
Kabuki make-up. However, they are all traditional media and
lack of interactivity: One can only view the static pictures.
The aim of our research is to make use of such legacies to
reconstruct and animate 3D Kabuki face model by means of
Computer Graphics techniques. It is a new way to preserve
the cultural heritage, and we think that it provides an interesting way for people to learn about Kabuki art.
To reconstruct the 3D face model of ancient Kabuki players, we need to consider how to generate precise 3D mesh
and high quality texture mapping of Kabuki face. There are
already abundant researchs on face modeling. They can be
roughly categorized according to their input: multiple photographs [Par72,Par82,AS93,LKT97,NFN00,HL96], range
data [LTW93, LTW95, Tan95], video [LZJC98], face model
database [BV99].
Face modeling based on two orthogonal or multiple photographs try to make use of the correspondence of differc The Eurographics Association 2005.

ent photographs to compute the precise 3D shape and high
quality face texture. Parke [Par72, Par82] presented to put
a skin grid on the face and compute 3D coordinate of grid
point to generate animation by interpolation. Many methods
are introduced to automate Parke’s method. Face detection
technique is used to detect feature points on face [AS93],
and deformation techniques, such as direct free form deformation and radial basis function, are also applied to
adapt a generic facial model according to detected feature
points [LKT97, PJHSS98, NFN00]. Besides 3D shape, researchers also stress the importance of creating high quality face texture. Basically, it’s a blending of multiple views.
According to [PJHSS98], face texture extraction can be divided into view-independent blending and view-dependant
blending [PJHSS98, AS93, LKT97, PJHSS98, HL96], and
its consideration includes self-occlusion, smoothness, positional certainty and view similarity. Face modeling from
video provides not only the way to construct 3D shape of
face but also the way to generate high quality texture map.
Furthermore, they provide way to capture the motion of face
to create high quality animation [LZJC98].
Range data is another important source for facial modeling. Lee et al. [LTW93, LTW95, Wat87] proposed a method
to adapt a physics-based face model with animation structure to specific range data. Blanz et al. [BV99] presented

60

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

to construct a face database and reconstruct 3D face model
by the linear combination of the face models in database.
All face models in this database are embedded into a vector space and PCA is used to find the principal eigenvectors
to compress the data. Their method can deal with face recon
struction from single photograph or multiple photographs by
statistical gradient optimization.
Since our research is also related to texture mapping, we
will make a brief introduction of the related work in texture
mapping field. Basically, there are two kinds of texture mapping in CG now: 2D texture mapping and 3D texture mapping. 2D texture mapping is to map 2D texture image to the
3D geometry model to enhance the visual effect, so it needs
2D parameterization of 3D geometry model. 2D parameterization can be solved as a optimization problem or a scattered
interpolation problem [MYV93,Lev01,TWBP03]. To map a
2D texture map to a 3D face model, some point constraints,
such as eyes, mouth, and so on, should be considered. Levy
[Lev01] presented a global optimization method to compute
the texture coordinates for face texture mapping problem.
Ying et al. [TWBP03] proposed to use single level RBF with
regulation term to map face texture to 3D face model. Our
paper also uses RBF to map face texture to 3D face model.
However, we enhance the precision of single level RBF with
multilevel RBF based on the adjustment of the length of radiuses. So, the precise alignment of feature points can be
guaranteed, and the quality of mapping result is then enhanced.

matable structure automatically after deformation, and many
subsequent applications can benefit from this, such as Virtual
Environments, web applications, and so on [PF02, Bal98].
Radial basis function (RBF) [Orr96] is the base of our deformation and texture mapping algorithm, and we present a
multilevel approach to enhance the precision of original RBF
network to make it suitable to both deformation and texture
mapping.
The remainder of this paper is organized as follows. Section 2 will describe the principle of multilevel RBF method.
In section 3, we will introduce how to reconstruct 3D face
model from ancient drawings. Kabuki make-up mapping is
described in section 4, and we provide a conclusion and discuss future work in section 5.
2. Multilevel radial Basis Function Algorithm
RBF has been widely used in scattered interpolation, deformation, animation and so on, and we will show that it
is also suitable to texture mapping. In the theory of RBF,
a regulation term is usually used to guarantee the smoothness of the constructed surface [Orr96]. However, this also
leads to the imprecision at data points. To solve this problem,
we present a Multilevel RBF approach to achieve the precision and smoothness at the same time. Lee et al presented a
multilevel B-Splines method to do scattered data interpolation [LWS97]. They built multilevel method by increasing
the density of knots. However, our method is based on the
adjustment of the length of radius in RBF.
2.1. Radial Basis Function Network
The definition of RBF network is the following:
y = ∑ wi hi (x)

(1)

i

Figure 1: Examples of Ancient Kabuki Drawings
Unfotunately, the ancient players were from several hundred years ago. We can not find such kind of input mentioned above from the cultural legacies of Kabuki. Firstly,
it is impossible to get their photographs. We can only get
drawings of their face. Secondly, Kabuki player often use a
special make-up style (see details in Section 4). Since this
special make-up style is sometimes independently recorded,
we must provide way to map make-up to the 3D face model.
That means we can’t always get 3D shape and texture at
same time. To deal with the reconstruction problem, We
design a procedure to deform the 3D face model compatible with MPEG4 face animation standard according to the
UKIYOE to get the 3D geometry information, then a texture mapping algorithm is used to map the our special texture to the reconstructed 3D facemodel. Since our facemodel
is compatible withMPEG4 standard, we can obtain the ani-

where w = i is the weight coefficient and hi is the kernel function. The kernel function is usually a Gaussian-like
function determined by the center ci and its radius ri .
Given known pairs: (xi , yi ) i = 0, 1, ..., n − 1, we need to
compute coefficients wi so that the following least square
function is minimum:
g = ∑(yi − ∑ w j x j )2 + λ ∑ w2i
i

j

(2)

i

Taking derivative to wi , We get:
(HT H + λI)W = HT Y

(3)

In equation 3, H is called design matrix, where Hi j = h j (xi ),
and W, Y are column vectors of wi and yi respectively. To select a good parameter λ, we adopt the global-ridge algorithm
introduced in [Orr96] and the generalized cross-validation in
[GHW79] as error criteria.
The second term in Eq. (2) is called regulation term. It
c The Eurographics Association 2005.

61

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

is used to limit the values of coefficients so that the resulting RBF network doesn’t fluctuate too much and generate a
smooth curve or surface. This is suitable to machine learning or generate a smooth approximation from noisy input.
However, in our case, we need the result from RBF network
can pass the data points precisely. That means we need RBF
network to interpolate, not approximate, in our application.
For example, in texture mapping, the RBF network should
compute precisely at corresponding feature points.
2.2. Multilevel Radial Basis Function Network
The center c and radius r of the kernel function h are the
parameters to control the behavior of the RBF network. In
many applications, the center is located at data points for
convenience. We do the same in our reconstruction procedure. So, the radius r is the parameter that we can use to
control the RBF network. We will analyze the influence of
the radius to the RBF network first, and then explain why we
choose the multilevel approach.

Local Support

(a) Influence of small radius

$SSUR[LPDWLRQ(UURU

(b) Influence of large radius

Figure 2: Influence of radius length to RBF
As illustrated in figure 2, there is a straight line and two
points on it will be moved to new positions(two vertical
green lines stand for the point movement and red curve represents the resulting curve computed by RBF). A small radius is selected first, and the result from RBF (Figure 2.a)
shows that the middle regions between these two points are
not affected. This is not a surprising result according to the
local support property of RBF. But we need to point out that
this is not suitable to interpolation application, and we can
not use small radius to do texture mapping.
In figure 2.b, we choose large radius instead. Notice that
the radius is large enough to let two points to influence each
other. At this time, the middle region is influenced and interpolation is much better than the previous case. However,
there are still approximation errors at the data points due to
c The Eurographics Association 2005.

the regulation term. That means the resulting curve can not
model the data points precisely. It will cause mismatching
error, especially in texture mapping.
According to the discussion above, we can design a multilevel approach. First, large radiuses are selected to get an
approximation and compute the approximation errors remained at data points, and then reduce the length of radius
gradually to reduce the error to get the precise result at data
points.We reduce the length of radius base on following reasons: a. the error can only be measured at data points, b. After we get a good approximation from large radius, we only
need to adjust the curve or surface locally, and small radius
means a small influence region, which means error at one
data points will not spread to other data points. It leads to
a more stable algorithm. The multilevel RBF can be written
as:
!
y=∑
j

∑ wi j hi j (x) + λ j ∑ w2i j
i

, ri j1 > ri j2 , i f j1 > j2

i

(4)
In other words, our multilevel RBF algorithm can be described as following:
• Step 1: input data points and a error threshold
• Step 2: Calculate initial radiuses for each data point according to the distribution of data points(We adopt the
following rule to compute the initial radius: Firstly, for
each data point, compute the maximum distance between
this point and other data points, secondly, use half of the
maximum distance as the initial radius for this data point
)
• Step 3: Levels = 0
• Step 4: Loop
• Step 5: compute coefficients wi j for current level RBF network (with regulation term) and store the coefficients for
current level
• Step 6: Compute the sum of the approximation error at
data points, if the sum is less than the error threshold, stop
the algorithm
• Step 7: Reduce the length of radiuses with formula:
ri j+1 = a ∗ ri j (a < 1, j = Levels)
• Step 8: Levels = Levels + 1, goto step 4
Figure 3 shows the result of multilevel RBF for the same
problem. Now the resulting curve (red curve in the figure)
can model the data points precisely and smoothness is also
guaranteed.

Figure 3: The resulting curve from Multilevel RBF
We would like to point out that this multilevel RBF approach is suitable for both compact support and global sup-

62

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

port radial basis function. Figure 3 shows the result of using
compact support RBF in multilevel RBF approach. However, we can also adopt global support radial basis function
in multilevel RBF method. It also improves the precision and
smoothness is still guaranteed. Figure 4 illustrates some results of using global support radial basis function (Cauchy
function) for multilevel RBF method in 2D case. In figure 4,
black points represent the feature points and dash lines stand
for the correspondence. With that correspondence, RBF can
be used to find the corresponding point in right two figures
to the red point in leftmost figure. The red points in right
two figures are the result from multilevel RBF method and
original single level RBF method.
It is obvious that our multilevel RBF approach can preserve the relationships between feature points better than the
original RBF in an irregular case (notice that the feature
points in right two pictures are arranged differently to the
leftmost picture). In the first row of figure 4, multilevel RBF
comes up with a correct result which is still inside the region
of surrounding feature points, while original one level RBF
generates a result outside of surrounding region. In second
row, multilevel RBF also comes up with a better result. This
feature of multilevel RBF builds a good fundament for both
deformation and texture mapping.
Corresponding Feature Points

be projected onto the picture to calculate the texture coordinates. Since our standard face model is compatible with
MPEG-4 face animation standard, there are already feature
points information in it, and each feature point is associated
with influence region [Bal98]. Thus, it is easy for us to get
the parameters for RBF network. There are also some other
works in the deformation of MPEG4 face model [LP99], but
our work is to use the deformation method to reconstruct 3D
Face model of ancient player from UKIYOE.
After the user inputs a UKIYOE and a face model,
our system starts with registering the face model to the
UKIYOE. The purpose of registration is to get a rough
match between UKIYOE and 3D face model, and the parameters of translation, rotation and scale will be estimated
at same time. There are already many research papers on this
topic [PJHSS98, BM92]. Since we let user to select feature
points manually, the feature points in our case can‘t be too
many. We choose traditional optimization algorithm, conjugate gradient, to solve this problem. Figure 5 illustrates
the result, and the system also enables the user to adjust the
transformation manually.

(a) Define feature points

Feature Points

Feature Points

Multilevel RBF Single level RBF

Multilevel RBF Single level RBF

Figure 4: Comparison in 2D case

(b) Registration result

Figure 5: Estimating transformation
Multilevel RBF is used to deform the 3D face model to
match the outline, eyes, nose and mouth of the face. The user
can also specify arbitrary feature points in the region when
the originalMPEG4 feature points are not enough to control the shape of 3D face model. Figure 6 illustrates the deformation result. As we can see, the deformation of mouth,
nose, eyes and outline generate a precise match between face
model and 2D UKIYOE. Final result after texture mapping
is showed in figure 7.

3. 3D FACE RECONSTRUCTION FROM UKIYOE
Because of the limitation we described in the introduction,
our reconstruction strategy is to generate the 3D face model
as precise as possible. The 3D face reconstruction procedure
described in this paper is actually a registration and deformation procedure. Since the registration and deformation procedure can generate a 3D face model which matches the face
in ancient drawing very precisely, the reconstruction result
should be very similar to the real 3D shape of the face of
Kabuki player in UKIYOE. After we get a precise match between the face model and the UKIYOE, the face model will

Figure 6: Deformation Result ( Red points in the picture are
feature points of the face model, user can drag these points
to do deformation)
By using the symmetry constraint in face model, we can
c The Eurographics Association 2005.

63

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

Figure 7: Reconstruction result

also reconstruct 3D face model from side view pictures (Figure 8). The feature points defined in MPEG4 face animation
standard also facilitate the identification of symmetry constraint, for example, feature point 4.3 should be symmetric
to feature point 4.4. Ref. [PF02] lists a lot of constraints between MPEG-4 feature points. They are also the constraints
we consider in the reconstruction.

define feature points at eyes, mouth, nose and some other
feature points to roughly surround the Kumadori (see figure 10. green points are corresponding feature points). To
use multi-level RBF to solve the texture mapping problem,
we only need to treat the feature points on the reconstructed
3D face model as xi and their corresponding feature points
on the Kumadori picture as yi in Eq. (4). Then, multi-level
RBF can be used to compute texture coordinates for every
3D point on the face part of the reconstructed 3D face model.

The face part of 3D face model

Kumadori picture

Figure 10: Feature points for Kumadori mapping

Figure 8: Reconstruction from side-view kabuki drawings

4. Kumadori Mapping
Kumadori is the name of the special face make-up style in
Kabuki, which is usually viewed as the most distinctive feature associated with Kabuki. Kumadori uses bold lines to
highlight the eyes, cheekbone and jaw line which helps to
emphasize the emotional responsiveness of the character,
and its color implies the personality of the character. Figure
9 illustrates some examples of Kumadori.

Figure 9: Examples of Kumadori
Mapping Kumadori to the 3D face model also starts with
defining corresponding feature points. The user only need to
c The Eurographics Association 2005.

Ref. [TWBP03] also adopts RBF to solve texture mapping
problem, and they point out that RBF network with regulation terms is suitable for texture mapping. However, our
mapping problem is more diffi- cult since we need to preserve the curve shape in Kumadori, and the correspondence
is not obvious in our case. Furthermore, our multilevel RBF
can enhance the precision of RBF network, which is very important to texture mapping problem. Another reason for us to
adopt RBF is that we can base our system on the same algorithm, which brings us cleaner software architecture. Fig. 11
shows that multilevel RBF can generate precise match and
the mapping result is much better. Figure 12 illustrates the
results of our texture mapping algorithm by using two Kumadoris in figure 9 as texture.
To demonstrate the ability of our texture mapping algorithm, we also apply it to map some special make-up patterns
in Peking opera to the reconstructed 3D face model. Figure
13 illustrates the results.
There are some Kumadoris that were acquired from the
face of ancient famous Kabuki players. They are treated as
treasure of Japanese culture. Figure 13 shows the procedure
of how to acquire the Kumadori and one Kumadori acquired
from the eighth Danjuro who was a famous player about
three hundred years ago. Mapping such kind of kumadori to
the 3D face model is of special meaning to people. Since this
ancient Kumadori is acquired by putting a paper on the face
of the player (see figure 14), it is quite similar to cylindrical
projection. So, we decide to project the deformed 3D face
model to a cylindrical plane. This step can also be viewed
as a 2D Parameterization of 3D Face model. Figure 15 illustrates the projection result. The red triangles are of face
part of the face model. Please notice the irregular deformation at the mouth part. In Kumadori picture of figure 15, the
mouth is very close to the nose. It is caused by the acquisi-

64

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

tion method of this Kumadori. The result shows that multilevel RBF approach can handle this quite well.

Many problems are still remained to be solved. The hair
of kabuki player will be added to improve the visual effect,
and animation techniques will be applied to mimic the expressions in Kabuki. We also plan to find way to render the
Japanese traditional costume used in Kabuki realistically.
References
[AS93] AKIMOTO T., SUENAGA Y.: Automatic creation of 3d facial models. IEEE Computer Graphics &
Application 3, 5 (1993), 16–22.

(a) Multilevel RBF

(b) Single level RBF

Figure 11: Comparing mapping result

[Bal98] BALCI K.: Xface: MPEG-4 Based Open Source
Toolkit for 3D Facial Animation. Technical Report, 1998.
[BM92] B ESL P. J., M C K AY N. D.: A method for registration of 3d-shapes. IEEE Transactions On Pattern Analysis and Machine Intelligence 14, 2 (1992), 239–256.
[BV99] B LANZ V., V ETTER T.: A morphable model for
the synthesis of 3d faces. In Proceedings of SIGGRAPH
(1999), pp. 187–194.
[GHW79] G OLUB G., H EATH M., WAHBA G.: Generalized cross-validation as a method for choosing a good
ridge parameter. Technometrics 21, 2 (1979), 215–223.

(a)

[HL96] H ORACE H. S. I., L IJUN Y.: Constructing a 3d individualized head model from two orthogonal views. The
Visual Computer 12, 9 (1996), 254–266.
[Lev01] L EVY B.: Constrained texture mapping for polygon meshes. In Proceedings of SIGGRAPH (2001),
pp. 417–424.
[LKT97] L EE W.-S., K ALRA P., T HALMANN N. M.:
Model based face reconstruction for animation. In Proceeding of MMM’97 (1997), pp. 323–338.

(b)

Figure 12: Kumadori mapping results

5. Conclusion
A 3D reconstruction method from ancient drawings of
Kabuki player has been described. It is based on multilevel
radial basis function algorithm. The reason we present multilevel RBFmethod is to achieve the smoothness and precision
simultaneously, which is very important in texture mapping.
Experimental results prove the effectivity of our method.
There is a new research hotspot on investigating how to
apply CG techniques to the field of cultural heritage. Our
face modeling system can be classified into that research,
which provides a new way to interact with cultural heritage.
The reconstruction result is not a precise 3D reconstruction
of original face comparing to other reconstruction methods,
but it provides interesting results and other applications can
be based on the reconstruction result of our method.

[LP99] L AVAGETTO F., P OCKAJ R.: The facial animation engine: Toward a high-level interface for the design
of mpeg-4 compliant animated faces. IEEE Transactions
on Circuits and Systems for Video Technology 9, 2 (1999),
277–289.
[LTW93] L I Y., T ERZOPOULOS D., WATERS K.: Constructing physics-based facial models of individuals. In
Proceedings of Graphics Interface (1993), pp. 1–8.
[LTW95] L I Y., T ERZOPOULOS D., WATERS K.: Realistic modeling for facial modeling. In Proceedings of SIGGRAPH (1995), pp. 55–62.
[LWS97] L EE S., W OLBERG G., S HIN S.: Scattered
data interpolation with multilevel b-splines. IEEE Transactions on Visualization and Computer Graphics 3, 3
(1997), 228–244.
[LZJC98] L IU Z., Z HANG Z., JACOBS C., C O HEN M.:
Technical Report MSR-TR-2000-11.
http://research.microsoft.com/ zhang, 1998.
[MYV93] M AILLOT J., YAHIA H., V ERROUST A.: Interactive texture mapping. In Proceedings of SIGGRAPH
(1993), pp. 27–34.
c The Eurographics Association 2005.

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

[NFN00] N OH J., F IDALEO D., N EUMANN U.: Animated deformations with radial basis functions. In ACM
Symposium on Virtual Reality Software and Technology
(2000), pp. 166–174.
[Orr96] O RR M.: Introduction to Radial Basis Function
Networks, Technical report. Center for Cognitive Science,
1996.
[Par72] PARKE F. I.: Computer generated animation of
faces. In Proceedings of the ACM National Conference
(1972), pp. 451–457.
[Par82] PARKE F. I.: Parameterized models for facial animation. IEEE Computer Graphics & Application 2, 9
(1982), 55–62.
[PF02] PANDZIC I. S., F ORCHHEIMER R.: MPEG-4 Facial Animation, The Standard, Implementation and Applications. John Wiley & Sons, 2002.
[PJHSS98] P IGHIN F., J. H ECKER D. L., S ZELISKI R.,
S ALESIN D. H.: Msynthesizing realistic facial expressions from photographs. In Proceedings of SIGGRAPH
(1998), pp. 75–84.
[Tan95] TANAKA H. T.: Accuracy-based sampling and
reconstruction with adaptive meshes for parallel hierarchical triangulation. Computer Vision and Image Understanding 61, 3 (1995), 335–350.
[TWBP03] TANG Y., WANG J., BAO H., P ENG Q.: Rbfbased constrained texture mapping. Computers & Graphics 27, 3 (2003), 415–422.
[Wat87] WATERS K.: A muscle model for animating
three-dimensional facial expression. Computer Graphics
21, 4 (1987), 17–24.

c The Eurographics Association 2005.

65

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Viewpoint quality and scene understanding
Dmitry Sokolov
sokolov@msi.unilim.fr

Dimitri Plemenos
plemenos@unilim.fr

Université de Limoges, Laboratoire MSI
83 rue d’Isle, 87000 Limoges, France

Abstract
Virtual worlds exploration techniques become nowadays more and more important. The methods are used in wide
variety of domains — from graph drawing to robot motion. This paper is dedicated to virtual world exploration
techniques which have to help a human being to understand a 3d scene. Improved methods of viewpoint quality
estimation are presented in the paper. Using these methods, a real-time technique allowing to choose a “good”
viewpoint for a virtual scene is also proposed.
Keywords: Scene understanding, Automatic virtual camera, Good point of view, Visibility.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Computer Graphics]: Viewing algorithms

1. Introduction
The last decades have been marked by the development of
the computer as a tool in almost every domain of human
activity. One of the reasons for such a development was the
introduction of human-friendly interfaces which have made
computers easy to use and learn.
The increasing exposure of the general public to technology means that their expectations of display techniques have
changed. The increasing spread of the internet has changed
expectations of how and when people are to access information. The whole way in which heritage is viewed is undergoing a change.
Virtual Reality technology broadens the role of the museum, it can provide several advantages:
1. Simulation of the physical layout of the museum and the
artefacts,
2. Removal of time and distance constraints on access to
displays,
3. Transformation of abstract data into a virtual artefact,
4. Ability to interact with artefacts.
Visitors are not limited by opening hours for observing
artefacts in the virtual museum. Internet access also ensures
that visitors can view the artefacts without having to travel
great distances. Virtual worlds are not limited by physical
laws, so it’s possible to transform general information to an
c The Eurographics Association 2005.

artefact, which is easy to view and understand, but which
has not any prototypes in real world. Also, an exact copy of
the real artefact can be handled by the user without fear of
breakage or loss.
As a consequence, a lot of problems raised, one of them
is automatic exploration of a virtual world. During these last
years people pay essentially more attention to this problem.
They realized necessity to have fast and accurate techniques
for better exploration and clear understanding of various virtual worlds. However, there are few papers which consider
this problem from the computer graphics point of view. On
the other hand, many papers have been published on the
robotics artificial vision problem.
The purpose of a virtual world exploration in computer
graphics is completely different from the purpose of techniques used in robotics. In computer graphics, the purpose
of the program which guides a virtual camera is to allow a
human being, the user, to understand a new world by using
an automatically computed path, depending on the nature of
the world. The main interaction is between the camera and
the user, a virtual and a human agent, and not between two
virtual agents or a virtual agent and his environment.
Ability to automatically compute a good exploration path
could allow to create a virtual guide, a councillor, which will
not limit a user to a precomputed trajectory, but will give

68

D. Sokolov & D. Plemenos / Viewpoint quality and scene understanding

recommended direction of moving, taking into account a set
of already visited sights. Figure 1 shows an example.

2. Compute a good direction in the pyramid defined by the
3 chosen directions, taking into account an importance of
each of the chosen directions (see figure 2).

d3
Good view direction
Scene

d2
φ2

φ1
d1
Figure 1: A user walks in a virtual city, the arrow indicates
a recommended direction.

Figure 2: Direct approximate computation of a good direction of view.

In this paper we are mainly concerned by global virtual
world exploration, where the camera remains outside the
scene, but the proposed methods may help to the indoor exploration researches too.

The viewpoint is considered to be good if it shows high
amount of voxels.

Viewpoint quality is an important criterion for good comprehension of a scene. In this paper we shall try to propose new methods for more accurate estimation of viewpoint
quality, in order to improve scene understanding.
The rest of the paper is organized as follows: in section 2 related works are reviewed. Their drawbacks, which
we would like to eliminate in our approach, are under discussion in section 3. Section 4 presents our approach and
invents new criteria of quality. Section 5 shows results of applying new technique in comparison with previous methods.
Finally, we address the problem of rapid estimation qualities
for a set of viewpoints in section 6.

2. Background
Kamada and Kawai [KK88] have proposed a fast method
to compute a point of view, which minimizes the number
of degenerated edges of a scene. They consider a viewing
direction to be good if parallel line segments are as far away
one from another as possible at the screen. Obviously, this
means minimizing an angle between a direction of view and
a normal of a considered face. Or, for a complex scene, this
means minimization of the maximum angle deviation for all
faces of the scene.
Colin [Col88] has proposed a method, initially developed
for scenes modeled by octrees. The method is to compute
a good viewpoint for an octree. The main principle of the
method can be described as follows:
1. Choose three best directions of view d1 , d2 and d3 among
the 6 directions corresponding to 3 coordinate axes passing through the center of the scene.

In [PB96] Plemenos and Benayada have proposed heuristic that extends definition given by Kamada and Kawai.
Their heuristic considers a viewpoint to be good if it minimizes maximum angle deviation between direction of view
and normals to the faces and gives a high amount of details.
The viewpoint quality according to [PB96] can be computed
by the following formula:
n

C(p) =

P (p)

i
]
∑ [ Pi (p)+1

i=1

n

n

∑ Pi (p)

+ i=1

r

,

(1)

where:
1. C(p) is the viewpoint quality for the given viewpoint p,
2. Pi (p) is the number of pixels corresponding to the polygon number i in the image obtained from the viewpoint
p,
3. r is the total number of pixels of the image (resolution of
the image),
4. n is the total number of polygons in the scene.
5. [a] means the ceiling function, i.e the smallest integer
number ac ∈ N : ac ≥ a.
In [VFSH01] Vázquez et al. have provided an information
theory-based method evaluating viewpoint quality. To select
a good viewpoint they propose to maximize the following
function, they have called a “viewpoint entropy”:
Nf

Ai
A
· log i ,
A
A
t
t
i=0

I(S, p) = − ∑

(2)

where N f is the number of faces of the scene, Ai is the projected area of the face number i, A0 represents the projected
area of the background in open scenes and At is the total area
of the projection.
For more details, a state of the art paper [Ple03] on virtual
c The Eurographics Association 2005.

69

D. Sokolov & D. Plemenos / Viewpoint quality and scene understanding

world global exploration techniques is available, whereas
viewpoint quality criteria and estimation techniques are presented in [PSF04].
3. Discussion
Directly or implicitly, all the proposed methods use only two
global parameters as input:
1. Size of a visible surface (projected area, amount of voxels, angle between direction of sight and normal to the
face),
2. Number of visible faces.
In other words, all of them consider a viewpoint quality as a
sum of qualities of separate faces, but don’t take into account
how a polygon is connected to the adjacent ones.
The number of visible faces is quite weak criterion for
a viewpoint quality estimation, because it may give nonaccurate results. For example, if we consider a very simple
scene, that consists of one square (figure 3(a)), then equation 2 gives us I(S, p) = 0 for the viewpoint p lying at a perpendicular to the square’s center. If we subdivide the square
(figure 3(b)), topology of the scene does not change, but
I(S, p) grows.

knowledge. Intuitively, this choice is good since more mesh
bends are visible from the viewpoint, more details of a scene
could be seen. Unfortunately, we can’t apply differential geometry definitions due to a discrete nature of our scene, so,
the equations should be redefined. There are many ways to
define a total curvature for a discrete mesh — for example,
a curvature along edges, so-called extrinsic curvature (equation 3) and intrinsic curvature for a set of vertices (equation 4). A concave mesh brings to a user the same amount
of information as a convex one, so, we propose to consider
absolute values instead of signed ones. Then the first one can
be written as:
Cext =

∑ (1 − cos Θe ) · |e|,

(3)

e∈E

where E is the set of edges of the scene, |e| is the edge length,
Θ is the angle between normals to the faces sharing the edge
e. The second one can be defined as follows:
Cint =

∑

v∈V

2π −

∑

αi ,

(4)

αi ∈α(v)

where α(v) is the set of angles adjacent to the vertex v (see
figure 4).

αi
v
e
Θe

(a) A scene
consisting of a
single square.

(b) The square
is subdivided
into 4 faces.

(c) The square
is subdivided
into 8 faces.

Figure 3: Three scenes represent the same square (a), it is
divided into 4 parts (b) and 8 parts (c). Equation 2 gives us
I(S, p) = 0 for (a), I(S, p) = log 4 for (b) and I(S, p) = log 8
for (c).
Thus, the methods using a number of faces to evaluate a
viewpoint quality, depend on initial scene subdivision. Using
the projected area of a face as a criterion of quality, the dependence will appear also if we don’t use an additive metric,
i.e., a sum of areas.
4. New heuristic
In this section a new heuristic using a new criterion of a
viewpoint quality is presented. As we have said before, it
would be good if the estimation routine could be broadened
by the knowledge of how a face is connected to the adjacent
ones. We propose to use a curvature of a surface as such a
c The Eurographics Association 2005.

(a) Extrinsic curvature is
equal to the product of
edge length and angle
Θ between faces sharing
the edge.

(b) Intrinsic curvature is
equal to the sum of angles adjacent to the vertex minus 2π.

Figure 4: Two ways to define total curvature for a discrete
mesh: curvature along edges and curvature in vertices.
Thus, our heuristic could be expressed as follows:
I(p) = C (F(p)) ,

(5)

where F(p) means the set of visible faces for the viewpoint
p, C(F(p)) is the total curvature for the mesh visible from
p. In order to calculate the curvature, both equations 4 and 3
may be used. We prefer to use intrinsic curvature equation,
because it requires significantly less amount of calculations
than the other one. It implies point-to-point visibility problem, while the usage of extrinsic curvature requires point-tosegment visibility computations.
The proposed heuristic is invariant to any subdivision of a

70

D. Sokolov & D. Plemenos / Viewpoint quality and scene understanding

scene keeping the same topology. Indeed, if we subdivide a
flat face to several ones, then all inner edges and vertices will
not be taken into account due to zero angles. The heuristic
works for the outdoor and indoor explorations equally. An
important property of such a kind of viewpoint quality definition is the
possibility to extend it, using the total integral
R
curvature |K|dA, into the class of continuous surfaces, such
Ω

as NURBS etc., which nowadays become more and more usable.
If we are allowed to use point-to-region visibility calculations, the projected area could be considered as an input
parameter also:
I2 (p) = C (F(p)) ·

∑

(a)

(b)

Figure 6: The best views of the candlestick are computed by
entropy-based method (a) and our method (b).

P( f ),

f ∈F(p)

where P( f ) means the projected area of the face f . This definition is invariant to the scene changes keeping the topology,
but it is more expensive than the previous one due to large
amount of computations necessary to get properly projected
areas. As a cheaper solution keeping an eye on projected
sizes, we could propose to use extrinsic curvature in formula 3 with projected edge lengths instead of absolute ones.
Sometimes it would be convenient — distant objects will
give less contribution to the viewpoint quality than closer
ones.

5.3. Viewpoint complexity versus our heuristic
A martini cup from SGI Open Inventor toolkit was tested.
The viewpoint complexity [PB96] and the viewpoint entropy
[Vaz03] techniques give the same result, which is shown at
figure 7(a). The result of our method application is given at
figure 7(b).

5. Comparison with previous results
5.1. Projected area versus our heuristic
Figure 5 shows us that maximum of projected area may give
to a user large amount of pixels drawn, but few details of a
scene.
(a)

(b)

Figure 7: The viewpoint complexity and the entropy-based
methods give the same result (a); our method application
(b).

(a)

(b)

5.4. Stand-alone results

Figure 5: Maximum area (a) versus our method of viewpoint
selection (b).

5.2. Viewpoint entropy versus our heuristic
A candlestick from SGI Open Inventor models is drawn at
figure 6. Figure 6(a) is taken from [Vaz03], this viewpoint
maximizes the viewpoint entropy. A view direction, maximizing our heuristic is shown at figure 6(b).

Figures 8 and 9 present results of viewpoint quality estimations for two models: the Utah teapot and a virtual office model. The figures show qualities of viewpoints lying
on surrounding sphere. To distinguish values in black-andwhite picture, we have connected viewpoints by lines; more
the line is dark and thick, more the viewpoints are good. The
figures show three images in order to represent better the 3D
data: the first image (the large one at the top of the figure)
gives a scene from the best point of view. The small images
allow to see how good each viewpoint is.
c The Eurographics Association 2005.

71

D. Sokolov & D. Plemenos / Viewpoint quality and scene understanding
Z

X

Z

Z

Y
Z

Z
Y

X

X
Y

X

Y

Y

X

Figure 8: The figure shows the scene from different points of
view. First image gives the scene from the best viewpoint, the
second and third ones show the viewpoint qualities (see the
section 5.4).

Figure 9: The figure shows the scene from different points of
view. First image gives the scene from the best viewpoint, the
second and third ones show the viewpoint qualities (see the
section 5.4).

y

A
F

Spherical triangle

x
6. Selection of good views for scene understanding
Having defined a viewpoint quality measure we would like
to find view directions, maximizing the heuristic.
6.1. Previous approaches
Plemenos in [Ple91] and [PB96] has proposed an iterative
method of automatic calculation of a good viewpoint. The
scene is placed at the center of a sphere whose surface represents all possible points of view. The sphere is divided into
8 spherical triangles (see figure 10). The best spherical triangle is chosen by the viewpoint qualities of the triangle vertices. Then, the selected spherical triangle is recursively subdivided and the best vertex is chosen as the best point of view
at the end of the process (figure 11).
The good view criteria in this approach were the number
of visible polygons and the projected area of visible parts of
a scene.
Vázquez in [Vaz03] proposes similar technique, applying
c The Eurographics Association 2005.

D

E
H

Scene

z
Figure 10: The sphere of
points of view is divided
into 8 spherical triangles.

B

C

Figure 11: Recursive
subdivision of the “best”
spherical triangle.

a heuristic designed to predict new viewpoint entropy using
values of already processed points.
Both works operate with quite expensive point-to-region
visibility and calculate visible parts using the Z-Buffer
method.
In these papers, the scene is rendered from a viewpoint,
coloring each face with a unique color ID and using flat
shading. In the resulting rendered scene, each pixel represents the color code of the face that is visible in that pixel —
OpenGL allows to obtain a histogram which gives informa-

72

D. Sokolov & D. Plemenos / Viewpoint quality and scene understanding

tion on the number of displayed colors and the ratio of the
image occupied by each color. See [BDP99] and [NRTT95]
for more detailed description.
Quality estimation routine for a single viewpoint, applying these methods, runs in O(N f · Z) time, where N f is a
number of faces in a scene and Z is the resolution of ZBuffer. For Ns viewpoints the running time is O(N f · Ns · Z).
Note that Z should be significantly greater than N f in order
to have at least few pixels to display each face. Complexity of the algorithm forces the authors to use adaptive search
algorithms which may give inexact results. In the following
section a new approach, which allows us to use even bruteforce method in real-time, is presented.

6.2. New approach
Our new heuristic does not use visibility of faces, it uses visibility of vertices instead. This allows us to reduce the problem of computing viewpoint quality from point-to-region to
point-to-point visibility. Using algorithm 1, we√reduce
 the
running time from O(N f · Z · Ns ) to O N f · Nv · Ns operations, where Nv means the number of vertices of a scene,
Z  Nf .
Let’s rasterize our surrounding sphere; each element represents a viewpoint. To evaluate viewpoint quality, we have
to find all vertices of a scene which are visible from each
element of the sphere. In order to perform it rapidly, we
consider a reverse problem by finding all visible viewpoints
from each vertex of the scene. It allows to use structuredness of the rasterized sphere for fast elimination of hidden
areas. Without loss of generality we can consider a rasterized plane instead of the sphere and a triangulated scene (we
can triangulate polygonal mesh in linear time using the algorithm presented in [Cha91]). Now we iterate through each
vertex of a scene and find all pixels of the plane which are
visible from a given vertex (see figure 12).

Now description of the main step of the algorithm will
be given. A vertex of the scene is given; for each triangle of the scene all its vertices are to be projected into the
plane. Then, its boundary can be drawn on the rasterized
plane using famous Bresenham’s algorithm [Bre65] of digital line drawing. Having initially white plane, projection of
each triangle is drawn with black color. Then we can easily delete inner parts of the projections. If our plane consists of
√ Ns pixels, then the maximal boundary drawing time
is O( Ns ) for a selected triangle. Having N f triangles and
Nv vertices
√in a scene, the total running time of the algorithm
is O(N f · Ns · Nv ) operations.
This technique can be applied to the rasterized sphere as
follows (see algorithm 1): let us suppose that there is a tessellated sphere; then a graph G = (V, E) can be constructed,
where the set of vertices V corresponds to the tessellation
parts. Edge (v1 , v2 ) ∈ E if and only if the part v1 is adjacent
to v2 . For example, each pixel of usual screen has 8 neighbors. If there is such a kind of graph and two vertices of
Input: Set of faces F, rasterized
n sphereo S
Output: Set of qualities Q = q{s∈S}

qs ← 0 ∀s ∈ S
Construct graph G = (S, E) as it is shown in section 6.2
for each vertex v of the scene do
B←∅
for each f ∈ F do
Find projection Pf of vertices of face f
for i from 1 to |Pf | − 1 do
Find shortest path γ in G from pi to pi+1 .
S
Store vertices are found: B ← B γ
end for
end for
S ← S\B
Remove broken edges from E
Use depth-first search to remove from S inner parts of
the projections.
qs ← qs +Cint (v) ∀s ∈ S
Restore S and G = (S, E) to the initial state
end for
Algorithm 1: Rapid estimation qualities for a set of viewpoints lying at surrounding sphere.

Figure 12: The scene consists of two cubes and a cone, the
rasterized plane represents a set of viewpoints. The given
vertex of the scene is not visible from viewpoints colored
black.

the sphere are given, then the shortest path in the graph corresponds to geodesic curve (or shortest line) connecting the
vertices. Since the graph has very special structure, the shortest path of length l can be found in O(l) operations using an
adaptation of the Bresenham’s algorithm. So, when we have
projected 3 vertices of a triangle on the sphere, we can find 3
shortest paths representing the boundary of the projection.
The paths are stored and next triangle is projected. After
main loop, when the boundaries of all triangles have been
computed, it’s easy to remove from the graph all the vertices met, at least once, in the set of paths. Then, the graph
c The Eurographics Association 2005.

D. Sokolov & D. Plemenos / Viewpoint quality and scene understanding

73

is splitted into a set of linked components (for example, we
can split it by constructing a depth-first search tree) and the
components corresponding to internal parts of the triangles
are to be removed.

References

7. Conclusion and future works

[Bre65] B RESENHAM J. E.: Algorithm for computer control of a digital plotter. IBM Systems Journal 4, 1 (1965),
25–30.

In this paper we have presented the new criteria of evaluating a viewpoint quality, which do not depend on changes in
a scene keeping the original topology. The method is extendable into the class of continuous surfaces such as NURBS.
A fast method of best viewpoint choosing is presented too.
The proposed techniques allow to get a good comprehension of a single virtual artefact or a general comprehension
of a scene. In the future it would be interesting to extend our
definitions. For example, we can consider not only curvature
or number of faces, but also a number of visible objects. An
example is shown at figure 13. The display is almost completely hidden by the case, but we clearly recognize it. If we
could split properly (in human perception) a scene into a set
of objects, then the number of visible objects becomes a very
important criterion of a viewpoint quality estimation.

[BDP99] BARRAL P., D ORME G., P LEMENOS D.: Visual understanding of a scene by automatic movement of
a camera. In GraphiCon’99 (Moscow (Russia), September 1999).

[Cha91] C HAZELLE B.: Triangulating a simple polygon
in linear time. Discrete Comput. Geom. 6, 5 (1991), 485–
524.
[Col88] C OLIN C.: A system for exploring the universe
of polyhedral shapes. In Eurographics’88 (Nice (France),
September 1988).
[KK88] K AMADA T., K AWAI S.: A simple method
for computing general position in displaying threedimensional objects. Comput. Vision Graph. Image Process. 41, 1 (1988), 43–56.
[NRTT95] N OSER H., R ENAULT O., T HALMANN D.,
T HALMANN N. M.: Navigation for digital actors based
on synthetic vision, memory, and learning. Computers &
Graphics 19, 1 (1995), 7–19.
[PB96] P LEMENOS D., B ENAYADA M.: Intelligent display in scene modelling. new techniques to automatically
compute good views. In GraphiCon’96 (Saint Petersburg
(Russia), July 1996).
[Ple91] P LEMENOS D.: A contribution to the study and
development of scene modelling, generation and visualisation techniques. The MultiFormes project., Professorial
dissertation (November 1991).
[Ple03] P LEMENOS D.: Exploring virtual worlds: Current
techniques and future issues. In International Conference
GraphiCon’2003 (Moscow (Russia), September 2003).

Figure 13: The display is almost completely hidden by the
case, but we clearly recognize it.
This technique could be particularly helpful in virtual heritage projects, for example, in a virtual museum different
objects should have different importances. Obviously, the
artefacts should have significantly greater importances than
walls, chairs and so on. Having a proper division of a virtual
museum model into a set of objects we could obtain good
heuristics for an automatic (or guided manual) exploration.
It would be also interesting to develop automatic methods of a scene exploration and to consider possibilities to
develop methods allowing an interaction with a user, where
the user can point which parts of a scene he (she) would like
to explore in details.
c The Eurographics Association 2005.

[PSF04] P LEMENOS D., S BERT M., F EIXAS M.: On
viewpoint complexity of 3d scenes. In International Conference GraphiCon’2004 (Moscow (Russia), September
2004).
[Vaz03] VAZQUEZ P. P.: On the selection of good views
and its application to computer graphics. PhD thesis,
Barcelona (Spain), May 2003.
VAZQUEZ P. P., F EIXAS M., S BERT M., H EI W.: Viewpoint selection using viewpoint entropy. In VMV ’01: Proceedings of the Vision Modeling
and Visualization Conference 2001 (2001), Aka GmbH,
pp. 273–280.

[VFSH01]
DRICH

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Rapid Visualization of Large Point-Based Surfaces
Tamy Boubekeur∗ , Florent Duguet+ and Christophe Schlick∗
∗:

LaBRI - INRIA - CNRS - University of Bordeaux

+:

INRIA Sophia Antipolis - ENST Paris

Abstract
Point-Based Surfaces can be directly generated by 3D scanners and avoid the generation and storage of an explicit
topology for a sampled geometry, which saves time and storage space for very dense and large objects, such as
scanned statues and other archaeological artefacts [DDGM∗ ]. We propose a fast processing pipeline of large
point-based surfaces for real-time, appearance preserving, polygonal rendering. Our goal is to reduce the time
needed between a point set made of hundred of millions samples and a high resolution visualization taking benefit
of modern graphics hardware, tuned for normal mapping of polygons. Our approach starts by an out-of-core
generation of a coarse local triangulation of the original model. The resulting coarse mesh is enriched by applying
a set of maps which capture the high frequency features of the original data set. We choose as an example the
normal component of samples for these maps, since normal maps provide efficiently an accurate local illumination.
But our approach is also suitable for other point attributes such as color or position (displacement map). These
maps come also from an out-of-core process, using the complete input data in a streaming process. Sampling
issues of the maps are addressed using an efficient diffusion algorithm in 2D. Our main contribution is to directly
handle such large unorganized point clouds through this two pass algorithm, without the time-consuming meshing
or parameterization step, required by current state-of-the-art high resolution visualization methods. One of the
main advantages is to express most of the fine features present in the original large point clouds as textures in
the huge texture memory usually provided by graphics devices, using only a lazy local parameterization. Our
technique comes as a complementary tool to high-quality, but costly, out-of-core visualization systems. Direct
applications are: interactive preview at high screen resolution of very detailed scanned objects such as scanned
statues, inclusion of large point clouds in usual polygonal 3D engines and 3D databases browsing.

1. Introduction
Most of the visualization systems designed for large 3D objects (tens or hundreds of millions samples) are based on an
initial mesh. Even high-quality multiresolution systems such
as QSplat [RL00] or the Sequential Point Trees [DVS03],
regularly mentioned for their ability to use points in order
to display gigantic models, require such an initial mesh, and
do not directly handle point clouds. In the case of very large
scanned objects (e.g. Digital Michelangelo [LPC∗ 00]), a non
trivial surface reconstruction has thus to be performed. This
process is very time consuming (from hours to days of computation on a single workstation), and is followed by other
several expensive algorithms before interactive display is actually possible [CGG∗ 04, GBBK04]. Moreover, in addition
to the point data, a large memory overhead is required to
store the topology, which becomes challenging when working on standard workstations.
It is clearly out of the scope of this section to recall all
the approaches that have been proposed in recent years to
obtain an interactive visualization of large objects. Essentially, these works can be classified according to three differc The Eurographics Association 2005.

ent principles. The first one, distributed rendering [WDS04],
has shown its efficiency on models reaching one billion
samples. However, this method requires expensive hardware
configurations (PC clusters) and, since it is based on raytracing, is limited to low resolution rendering if an interactive framerate is required. The second one, out-of-core
visualization [Lin03, Tol99], proposes to use some cache
friendly data structures, both for storage and rendering,
which offer efficient disk-to-memory updates according to
the modification of viewing parameters. Adaptive Tetra Puzzle [CGG∗ 04] is currently one of the most efficient systems,
reaching the competitive framerate of 60 frames per second
with the well-known St Matthew model [LPC∗ 00]. Other
out-of-core methods have also focused on advanced realtime rendering effect, such as shadows [GBBK04]. Finally,
the last proposed principle tries to (dramatically) reduce the
amount of data, while preserving almost the same appearance for the rendered object. In [COM98], the very innovative idea of mesh simplification combined with simultaneous texture generation to capture the underlying details has
been proposed. Multi-resolution approaches, such as Progressive Meshes [Hop96], can also be enhanced by com-

76

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

bining a normal map with simplified geometry [SSGH01].
Nowadays, such a process can be directly implemented on
programmable GPU that allow the use of normal maps to
represent fine details within a per-pixel shading [TCS03].
But it should be noticed that a complete parameterization of
the model has to be performed to construct the normal map,
which is a challenging task for huge models.
Multi-resolution directly on point clouds is proposed in
the Layered Point Cloud [GM04], as well as an efficient
compression scheme in the DuoDecim system [KSW05],
but these methods require several hours for processing a
model like the St Matthew [LPC∗ 00]. We rather target one
order of magnitude faster preprocessing, and we would like
to take benefit from polygonal rendering. To our knowledge, the only appearance-preserving simplification that
does not require a meshing or parameterization, is the Phong
Splatting [BSK04], following [KV03], which uses surfels
[PZvBG00] (usually a point with a normal and a color) combined with normal mapping to render point sets with fewer
points but similar appearance. Unfortunately, surfel splatting [ZPvBG01] is not well adapted to high resolution display, and requires complex multi-pass rendering, and intensive use of vertex/fragment shaders [BSK05]. Moreover, the
technique proposed in [BSK04] to encode the normal distribution for each rendering primitive performs a strong lowpass filtering, and the intensive use of an underlaying kDTree makes it difficult to extend the technique to an out-ofcore implementation.

into hardware friendly rendering primitives, called Surfel Strips, organized in a bounding box hierarchy, named
the Stripping Tree, following the fast lower dimensional
meshing of [BRS05] (see Section 2.3).
3. the Stripping Tree is used during the out-of-core normal
mapping; all the points of the original model are streamed
through the tree and distributed to their corresponding
leaves, where the point normal is projected onto a quad
texture associated to each Surfel Strip (see Section 2.4).
This streaming process is the key step of our technique,
as it allows us to handle large models with limited memory. At the end of this step, each leaf of the Stripping Tree
contains a low definition Surfel Strips and a high definition (possibly sparse) normal map.
4. a normal map diffusion fills the holes of each normal map
by using diffusion algorithm, to get a continuous normal
field, interpolating the original normals of the huge model
(see Section 2.5).
The resulting object in each leaf (a coarse piece of mesh plus
a high resolution normal map), is what we call Normal Surfel
Strips. This collection of primitives, stored on the leaf of the
tree, are directly used to provide a high quality interactive
rendering by using conventional per-pixel shading.

In this paper, we propose a general pipeline for converting
a large point set into a coarse polygonal mesh with high resolution normal maps, which is exactly the kind of representation suitable for real-time hardware supported visualization.
We use as an input an unorganized set of samples where each
sample is a 3D point and its associated normal vector (we
will use indifferently “point” and “surfel” for such a sample). Our major goal is to avoid time-consuming steps such
as surface reconstruction or precise parameterization, while
keeping nice visual results. By introducing a new rendering
primitive, called Normal Surfel Strips, we show that in-core
appearance-preserving models can be created starting from
huge point sets through a very fast out-of-core process. Our
technique has been tested on various very detailed scanned
objects and statues, for which an interactive visualization has
been obtained with a global preprocessing time that represents only a couple of minutes.
2. Our Approach
2.1. Overview
Our algorithm can be decomposed in four steps (see Figure
1):
1. we perform an out-of-core simplification of the huge
model (see Section 2.2)
2. the resulting simplified point set is quickly converted

Figure 1: Overview of our approach for interactive visualization of large models. The usual expensive step, the meshing, is only performed on a very reduced point cloud, and
is no more the bottleneck. Most of the fine details are expressed through the normal maps, generated on a per-surfel
strip basis with diffusion, a faster process than geometric
reconstruction algorithms.

c The Eurographics Association 2005.

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

77

2.2. Out-of-Core Simplification
Out-of-core simplifications of meshes are usually based on
differential geometry properties, expressed as error metrics,
which drive the polygon decimation. Such a criterion is
not available on point clouds without some kind of surface
reconstruction. On the other hand, state-of-the-art downsampling methods [PGK02] for point-based surfaces are difficult to use with out-of-core models. Actually, a precise
down-sampling is not mandatory in our particular case: we
are not looking for the n best points to represent a given large
model, we just need a reasonably-looking point cloud that
can be quickly tessellated with the algorithm presented in
Section 2.3. This algorithm can generate a fast local triangulation for point sets up to few millions of points in a reasonable time (typically about one minute for two millions of
points).
But a final down-sampled point set of 2 millions points
is not interesting enough to take full benefit of our fast appearance preserving approach (Section 2.4). What we need
is to get very quickly a good looking coarse representation
of the point cloud. Since the target objects of our application (large point clouds acquired with 3D scanners) are usually very dense (see models in Figures 5 and 11), we propose to use a simple grid to filter the point cloud. Similarly
to Lindstrom’s filtering for polygons [Lin00], each surfel of
the model is read and tested against a 3D grid enclosing the
whole model. If the intersecting grid cell is empty, then the
surfel is simply stored in the cell where it becomes an accumulation surfel, and the cell cardinal is set to 1. If not, all
the properties of the current surfel (position, normal, color,
etc) are added to the accumulation surfel of the cell, and the
cardinal is increased.
At the end, for all non empty cells, we divide all the properties of the accumulation surfel by the cardinal of the cell,
and put the resulting surfel in the in-core surfel set. Note that
for semi-automatic applications, the user may enter the desired grid resolution; we have observed good results when
using 2blog10 (n)c where n is the total number of points. This
simple grid filtering is extremely fast, as it processes more
than 5 millions points per second.
2.3. Surfel Stripping
As our goal is to design a visualization system, watertight
surface reconstruction is not mandatory for the coarse mesh
generation. We propose to use an efficient lower dimensional
triangulation, the Surfel Striping, that have been recently developed in [BRS05]. This technique converts the point set
into a collection of overlapping triangle strips that offers a
convincing smooth visualization, despite the lack of geometric continuity (see Figures 5 and 11).
First, the in-core point set is partitioned into small point
sets, locally expressed as height maps, organized in an
octree-like structure: the Stripping Tree. This is done by recursively splitting the bounding box of the decimated point
c The Eurographics Association 2005.

(a)

(b)

(c)

Figure 2: Surfel Stripping. (a) The simplified point cloud
obtained after the out-of-core simplification. (b) The local
piece of meshes quickly generated thanks to the Stripping
Tree partitioning (in green). Each colored patch corresponds
to a surfel strip, locally generated in 2D. (c) The coarse mesh
obtained, made of local triangle strips interpolating the input points.
cloud, until having in each leaf i a set Si = {si0 ...sin } of surfels, that respects the following predicate:

δa ∈ [0, 1]

 ni j .ni > δa
∀si j ∈ Si
(1)

 |(pi j −ci ).ni | < δd δd ∈ [0, 1]
max (||p −c ||)
k

ik

i

with pi j the position of si j , ni j its normal vector, ni the average normal of Si , which can be computed as the normalized eigen vector associated to smallest eigen value of the
covariance matrix of Pi = {pi0 , ..., pin } (Principal Component Analysis) or simply by averaging the normals of Si ; ci
is the centroid of Si . The δa value corresponds to the normal
cone. In order to avoid distortion resulting from bent partitions, we set δa = 0.25 in all our tests. The δd corresponds to
the geometric displacement in the average normal direction.
It can be set according to the sampling density if available,
or by some heuristics. We have chosen in our experiments
δd = 0.25. To obtain regular leaves, we impose a maximum
population threshold for the leaves, which may thus be subdivided, even if they are already consistent with a height map
definition. The resulting partitioning is shown in Figure 2(b).
Before the generation of a local piece of surface and in
order to avoid holes in-between leaves, an inflation pass is
done on each leaf, by enlarging its associated point set with
the points located in its volumetric one-neighborhood (i.e. all
the points of the leaves adjacent to the current one). Actually,
this is on this inflated surfel set Si0 that must be tested the
predicate of Equation 1.
Then, a fast incremental 2D Delaunay triangulation of
each inflated point set Si0 (leaves of the tree) is done in the
average plane Π0i = {c0i , n0i } of each surfel set Si0 . In order to
remove redundant triangles in overlapping zones introduced
by the inflation process, a decimation pass is performed by
comparing triangles of neighboring surfel strips. In practice,
the decimation pass actually discards about 99% of the overlappings when using the simple rules described in [BRS05].

78

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

Finally, a fast stripping of the resulting local mesh is done
using a cache-friendly adjacency graph [RBA05].
All those operations are performed at coarse level, on the
decimated model and so require only a negligible processing
time, even with a non optimized implementation.
When rendering the resulting collection of Surfel Strips,
a visually continuous surface is obtained, almost equivalent
to a provable watertight surface mesh in practical cases, but
about one order of magnitude faster. Usually, residual overlapping does not produce visible artefacts in the shading,
even under multiple light sources (see Figure 2). Each interior node of the stripping tree provides an average position,
an average radius and a normal cone, in order to perform a
hierarchical culling in a similar fashion to [RL00].

or restricted to the amount of available GPU texture memory for very large models or less competitive graphics cards.
Similarly, its aspect ratio is equivalent to the bounding rectangle of its Surfel Strip. Using a flat parameterization of a
non-flat Surfel Strip may generate some distortions, especially in areas of high curvature that would result in a global
loss of details. But in practice, the constraints imposed during the construction of the stripping tree lead to close to planar Surfel Strips, which limit distortion. No artefacts were
visible in our experiments. Since the normal maps are generated on a quad basis (aligned to the surfel strip), they are
easy to pack in few large textures, an optimized way to store
textures on the GPU memory.

We have used this triangulation method because it naturally proposes a quad of projection for the normal map construction on a per surfel strip basis, simply by reusing the
average plane Π0i used for performing the Delaunay triangulation.
2.4. Streaming Normals
At this point, the Stripping Tree of the down-sampled surfel point set is available, and can be visualized as a coarse
representation of the huge model (see Figure 2). In order to
retrieve the original appearance of the large model, a normal map will be associated to each Surfel Strip. These normal maps are generated during a normal streaming process,
where all points of the initial huge point cloud are streamed
through the Stripping Tree, to quickly find the set of Surfels
Strips they belong to. This second reading pass of the model
requires only to deal with one of these points at the same
time in the main memory. Note that one point may belong
to more than one Surfel Strip, because of the inflate-anddecimate pass described above.
Then, for each intersected leaf, the local parameterization
of the point relative to its Surfel Strip is computed by projecting the point on the average plane Π0i of the strip. Actually,
we parameterize the projected point according to a bounding quad, including Pi0 , and aligned to the two eigen vectors
associated to the two highest eigen values of the covariance
matrix of Pi0 , previously computed and stored as leaf data
with n0i and c0i .
This parameterization is used to fill the relative pixel value
of the associated normal map with the normal vector of the
streamed point. We use floating point textures, so if more
than one normal is projected onto the same pixel, we just add
the normal vector value to the existing pixel, and normalize
all the normal maps after having processed all the points of
the original model. This also prevents from aliasing artefacts
that may occur.
The resolution of each normal map is proportional to the
number of points of its corresponding Surfel Strip, but can
also be specified by the user as a “global quality” parameter,

(a) Surfel strips

(b) Sparse normal map

Figure 3: After the normal streaming step, a sparse normal
map is attached to each Surfel Strip. (a) Coarse topology
computed from the sub-sampled point cloud. (b) Color visualization of the spare normal map: pixels color is set with the
XYZ coordinates of the normals. Black points corresponds to
pixels of the normal map where no surfel as been projected.
2.5. Normal Map Diffusion
After the normal streaming process, each surfel strip is enriched with a sparse normal map since several pixels may
not have been filled by projected normals (as shown in Figure 3). For using this map as a texture for our coarse surfel strips, holes need to be filled (black pixels in Figure 3).
Many approaches have been developed over the years to fill
holes in an image, which is a basic operation for image repairing. Exploration-based approaches such as [BWG03] directly compute an illumination value for a pixel given by exploring its neighborhood. On the other hand, iterative PDEbased approaches such as [PGB03], spread existing color in
the image using PDEs such as Poisson equation, or diffusion equation. We use the PDE-based diffusion technique
presented in [XP98], for its guarantees of continuity and
smoothness. The implementation is based on a multigrid resolution scheme that first solves the problem at a coarser resolution, and then uses this coarse result to initialize the algorithm at finer resolution:
Solve (h, Axh = b)
1.
2.
3.
4.
5.

Pre-smoothing steps: Ax = b
Downsample: xh−1 = Dxh
Solve (h-1, Axh−1 = b)
Upsample: xh = Uxh−1
Post-smoothing steps: Ax = b
c The Eurographics Association 2005.

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

where Ax = b corresponds to the matrix formulation of
discrete diffusion equation with finite differences, and h corresponds to the quadtree level associated with the resolution
of the processed image (see also the Push-Pull algorithm in
[GGSC]).
The approach of [PG01] corresponds to a multigrid iteration with no pre-smoothing step, a single post-smoothing
step and a specific down-sampling algorithm that only takes
into account existing samples. We inspired from [PG01] by
skipping the pre-smoothing, and only using existing samples
for down-sampling, but ran the post-smoothing iterations until the convergence criterion is met. Indeed, without these extra iterations, some blocky interpolations are present in the
texture we obtained, especially around holes.
The multigrid resolution algorithm proved to be very efficient in practice (see Table 6), only a few iterations (e.g.
5) were needed for convergence with 10−3 error bounds in
most cases. Note that the same approach can be used to create maps for other per-surfel attributes (e.g. color, geometric
displacement, etc). The Figure 4 shows the resulting set of

(a)

(b)

Figure 4: Normal diffusion. (a) The Omphalos model (11
664 466 points). (b) Close-up. Left: coarse surfel strips
quickly generated after the out-of-core decimation of the
large point cloud (random per surfel strip color). Right:
real-time rendering, with per-pixel illumination using the
high-resolution normal maps. High quality rendering at high
resolution, with minimal pre-process. Note the nice automatic filtering of the model, thanks to the intrinsic hardware
mipmapping of the normal maps.
c The Eurographics Association 2005.

79

high resolution normal maps attached to coarse surfel strips.
The hole-filling process provides normal maps that express
the essential part of the original large model appearance.
These maps are stored as texture in the GPU memory, and
benefit from the automatic filtering provided by the hardware mip-mapping. This property is quite interesting, since
it can be interpreted as both an anti-aliasing process and an
hardware supported multiresolution rendering, thanks to the
different levels of the mip-mapping.
3. Results
We have implemented our system under Linux on an Intel PIV 3.2 GHz, 1GB RAM, 160GB UDMA HD, NVidia
Quadro FX 4400. We use C++ and the OpenGL Shading
Language (for normal mapping). We consider input binary
files where points are encoded as an unorganized list of
chunks of 6 floats (3 for the position and 3 for the normal).
Table 6 summarizes the preprocessing times of our system.
Figure 5 shows the real-time rendering obtained on various point clouds with our approach. It appears that the normal map initialization is the main bottle-neck. Obviously,
tree-traversal and local projections involved in this out-ofcore streaming remain costly since they are performed on
the whole model. Nevertheless, all the different stages involved in our approach are highly parallelizable (each point
is treated separately), and can take benefit from recent dualcore CPUs (an improvement factor of 1.5 can reasonably be
considered for dual-core CPUs, more for multi-CPU workstations). Note also that we use a pointer-based implementation of the Stripping Tree, which could be enhanced. Our
resolution criteria for the normal maps works quiet well in
most of the cases. Actually, even when a high density variation occurs inside a leaf of the tree, aliasing is prevented in
the normal map thanks to the iterative diffusion step (see
Figure 7). Note that the use of compression for textures
could reduce the GPU memory footprint. The hard-drive
latency strongly influences the performances of simplification and normal mapping passes. Better performances can
be reached by using high-speed hard-drives (U-SCSI) and a
dedicated workstation, where useless processes are stopped
(usually between 20 and 30 on our Linux system). The excellent framerates given in Table 6 are reached thanks to
the highly optimized polygonal hardware graphics pipeline,
particularly adapted to display at high resolution polygonal
models with high definition textures.
Comparison The critical point in our work was to reduce as much as possible the pre-process time needed for
obtaining a convincing visualization of large point clouds.
Compared to QSplat [RL00], our preprocessing is faster (on
order of magnitude in the worst experimental case) and it
does not require a previous surface reconstruction (huge additionnal processing time). Compared to the Layered Point
Clouds [GM04], our preprocessing is about ten times faster.
Of course, these multiresolution methods are conservative,
and do not perform a low pass filtering on the geometry

80

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

(a) Omphalos (11 664 466 points)

(b) Drum 2 (22 877 845 points)

(c) Dancers (31 620 449 points)

Figure 5: Visual quality for various large models. Antialiased rendering with 3 color light sources on a 1600x1200 screen
resolution. From left to right in each image: the sub-sampled point cloud decimated at the first out-of-core reading pass,
the coarse piece of mesh locally generated, colored with random per-surfel strip colors and the interactive rendering of this
collection of piece of meshes, enhanced with normal map expressing the fine details, generated during the second reading pass
of the point cloud.
Models
Number of points
TIMINGS
Simplification time
Surfel Stripping time
Normal mapping time
Diffusion time
Total time
RENDERING
Number of surfel Strips
Number of triangles
Textures memory used in MB
FPS (frames per second)

Omphalos
11 664 466

Drum 2
22 877 845

Dancers
31 620 449

St Matthew
186 810 938

5s
2s
45 s
35 s
100 s

10 s
4s
151 s
35 s
201 s

14 s
5s
213 s
34 s
274 s

61 s
7s
667 s
152 s
887 s

1602
45 504
68
> 200

1721
51 012
71
> 200

2013
66780
94
198

2457
79030
185
165

Figure 6: Preprocessing time and rendering framerate for various large models. The total timing represents all the steps needed
for the preprocessing, starting from an unorganized point cloud on disk up to a ready-to-render data structure in memory. The
framerates are given for 1600x1200 screen resolution.
such as ours, but from the visualization point of view, we
keep the essential appearance thanks to high resolution normal maps reconstructed in 2D (see Figure 11). Note also
that our system provides a polygonal rendering, highly optimized on today’s GPU. This allows us to reach high framerates at high resolution. Actually, our approach provides results that confirm [PGK02]: performing decimation on the
point cloud and then applying reconstruction methods is definitely more efficient than meshing and optimizing the full
resolution point cloud, at least for our visualization purpose.
Finally, the diffusion process of normal maps can be seen
as a kind of surface reconstruction, where not the geometry, but the normal field is reconstructed from points, in the
lower dimension (the average plane of the leaf node). Figure 8 shows our normal mapping reconstructed directly from
original samples: the same order of visual quality is reached
when comparing to prior art methods where a full resolution
surface reconstruction and parameterization were necessary
before performing the appearance preserving simplification.

Figure 7: Upper part of the St Matthew model with our
method rendered at 165 FPS, without (left) and with (right)
the normal maps. The maps are recreated directly from the
point cloud, providing a convincing appearance, while using less than 80k triangles (left image). Most of the “appearance” information carried by the original point cloud
is directly stored through these normal textures on the GPU
memory (185 MB) and used for the per-pixel lighting.

Limitations: We have made the choice to use a very simple Sub-sampling scheme at the beginning of our algorithm
(see Section 2.2). The choice has been made after various
experiments with real data set, which show essentially that
c The Eurographics Association 2005.

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

81

4. Conclusion
We have proposed an efficient pre-process to obtain an interactive visualization of large 3D objects represented by point
clouds with appearance preserving. The main advantages of
our technique are:
Figure 8: Left: coarse surfel strips rendering. Right: normal
surfel strips rendering. High frequency details of large scans
models are preserved, using detailed normal textures instead
of huge polygons sets. Globally, our approach provides similar results to usual appearance preserving methods that require full resolution tessellation, parameterization and simplification, while we deal only with the original samples.
most of the time, large scans are dense enough to support this
decimation, and so allow for this fast pre-process. Nevertheless, one could imagine situations where small topological
features are lost during this first out-of-core sub-sampling.
In this case, more adaptative decimation scheme must be
used. Unfortunately, this usually means a much longer preprocess. The reader must also note that our approach is still
a “simplification” one, which exhibits drawbacks and advantages. On one hand, even if most of the fine visual details are kept thanks to the high resolution normal maps
(see Figure 9), a slight shrink effect can appear in silhouettes because of the coarse mesh definition. This is the price
for reducing the time preprocessing and improving the rendering framerate compared to “multiresolution” approaches
[RL00]. On the other hand, this low-pass filtering has frequently removed the registration noise present in our examples. Figure 9 shows the rendering of the St Matthew model
with the publicly available QSplat software. We can observe
that our method provides a globally equivalent appearance,
with a much higher framerate, even under a strong closeup. Note that we have compared with QSplat because it is
the only software publicly available for large dataset. Note
also that QSplat is not tuned for recent graphics hardware,
which explains the poor framerate obtained. One of our future work will be to compare our results with a combination
of [DVS03] and [BSK05], which should be the stat-of-theart point-based rendering system. In order to avoid the slight
shrink effects, we have also plane to compute displacement
maps on top of normal maps, using the dynamic GPU tessellation method proposed by Boubekeur and Schlick [BS05].

• Our system directly handles the unorganized point clouds,
avoiding any kind of surface reconstruction of the large
model.
• No complex data structure or complex processing is
needed on the large model.
• Our local quad-based approach for appearance preserving
does not require a globally consistent parameterization of
the model.
• The pre-process is very fast as it basically only requires
two out-of-core passes, which makes it usable in various
applications such as large model data base browsing.
• The final in-core model is entirely stored on the GPU
memory, large enough on today graphics devices to handle efficiently appearance attributes of hundred millions
of samples, through, for instance, normal textures.
• Since all details are stored as normal maps, the rendering
takes automatically benefit of the hardware mip-mapping
for antialiasing details at a given resolution.
One weakness of the presented approach may be in the initial
out-of-core grid down-sampling step, which clearly trades
speed for quality. It only uses the volume of the object and
not its surface, thus in very complex areas, some undersampling may appear which may generate visible artefacts.
However, these artefacts occur very rarely, and do not degrade the overall appearance of the object. Of course some
more accurate (but more expensive) down-sampling may always be employed as an alternative.
The whole pipeline is very easy to implement, and has
provided very convincing results when applied on a large
variety of acquired point clouds. We hope that it can become
a good complement to existing high quality but slower
visualization methods of large models, and that it will help,
following [DDGM∗ ], to use point-based surfaces for storing
and transmitting huge sampled models such as scanned
archaeological artefacts.
Acknowledgement : We are grateful to Electricit’e de France (EDF) and the
Digital Michelangelo Project (Stanford) for the 3D data sets. This work as been partially
done at UBC (Vancouver) and supported by LIGHT (INRIA associated team program).

References
[BRS05] B OUBEKEUR T., R EUTER P., S CHLICK C.: Surfel Stripping. Tech. rep.,
LaBRI - RR-1352-05 - (to appear in Proceedings of ACM Graphite 2005), 2005.
[BS05] B OUBEKEUR T., S CHLICK C.: Generic mesh refinement on gpu. In ACM
SIGGRAPH/Eurographics Graphics Hardware 2005 (2005).
[BSK04] B OTSCH M., S PERNAT M., KOBBELT L.: Phong splatting. Symposium on
Point Based Graphics 2004 (2004), 25–32.

Figure 9: Eye of the St Matthew. Visual quality comparison between our approach at 165 FPS (left) and the QSplat
rendering (right), obtained at 0.3 FPS. Even under a strong
close-up, our method keeps the fine details as well as the
QSplat system, but with a much higher framerate.
c The Eurographics Association 2005.

[BSK05] B OTSCH M., S PERNAT M., KOBBELT L.: High quality splatting on today’s
gpu. Symp. on Point Based Graphics (2005), 25–32.
[BWG03] BALA K., WALTER B., G REENBERG D. P.: Combining edges and points for
interactive high-quality rendering. ACM Trans. Graph. 22, 3 (2003), 631–640.
[CGG∗ 04] C IGNONI P., G ANOVELLI F., G OBBETTI E., M ARTON F., P ONCHIO F.,
S COPIGNO R.: Adaptive TetraPuzzles – efficient out-of-core construction of gigantic
polygonal models. ACM Trans. Graphics (SIGGRAPH 2004) (2004).

82

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

Figure 10: Real-time visualization of the St Matthew model (186 810 938 points). From left to right: the sub-sampled model,
the coarse mesh locally generated in the leaves of the stripping tree (in green), the coarse mesh with the high resolution normal
mapping with one white light source and 3 colored light sources.
[COM98] C OHEN J., O LANO M., M ANOCHA D.: Appearance-preserving simplfication. Proceedings of SIGGRAPH 98 (1998).

[PG01] PAULY M., G ROSS M.: Spectral processing of point-sampled geometry. ACM
Trans. Graphics (SIGGRAPH 2001) (2001), 379–386.

[DDGM∗ ] D UGUET F., D RETTAKIS G., G IRARDEAU -M ONTAUT D., M ARTINEZ J.L., S CHMITT F.: A point-based approach for capture, display and illustration of very
complex archeological artefacts. In VAST 2004.

[PGB03] P EREZ P., G ANGNET M., B LAKE A.: Poisson image editing. ACM Trans.
Graph. 22, 3 (2003), 313–318.

[DVS03] DACHSBACHER C., VOGELGSANG C., S TAMMINGER M.: Sequential point
trees. ACM Trans. Graphics (SIGGRAPH 2003) (2003), 657 – 662.

[PGK02] PAULY M., G ROSS M., KOBBELT L. P.: Efficient simplification of pointsampled surfaces. In Proc. of IEEE Visualization ’02 (2002), pp. 163–170.

[GBBK04] G UTHE M., B ORODIN P., BALÁZS A., K LEIN R.: Real-time appearance
preserving out-of-core rendering w/shadows. In EGSR 2004. 2004, pp. 69–79 + 409.

[PZvBG00] P FISTER H., Z WICKER M., VAN BAAR J., G ROSS M.: Surfels: Surface
elements as rendering primitives. ACM Trans. Graphics (SIGGRAPH 2000) (2000),
335–342.

[GGSC] G ORTLER S., G RZESZCZUK R., S ZELISKI R., C OHEN M.: The lumigraph.
In Proc. of ACM SIGGRAPH 1996, pp. 43–54.

[RBA05] R EUTER P., B EHR J., A LEXA M.: An improved adjacency data structure for
fast triangle stripping. Journal of Graphics Tools (JGT, to appear) (2005).

[GM04] G OBBETTI E., M ARTON F.: Layered point clouds. In Eurographics Symposium on Point Based Graphics (2004), Alexa M., Gross M., Pfister H.„ Rusinkiewicz
S., (Eds.), pp. 113–120, 227.

[RL00] RUSINKIEWICZ S., L EVOY M.: Qsplat: a multiresolution point rendering system for large meshes. ACM Trans. Graphics (SIGGRAPH 2000) (2000), 343–352.

[Hop96] H OPPE H.: Progressive meshes. Computer Graphics 30, Annual Conference
Series (1996), 99–108.
[KSW05] K RÜGER J., S CHNEIDER J., W ESTERMANN R.: Duodecim - a structure for
point scan compression and rendering. In Symp. on Point-Based Graphics (2005).
[KV03] K ALAIAH A., VARSHNEY A.: Modeling and rendering of points with local
geometry. IEEE Trans. Visualization v9, n1 (2003), 30–42.
[Lin00] L INDSTROM P.: Out-of-core simplification of large polygonal models. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques (2000), pp. 259–262.
[Lin03] L INDSTROM P.: Out-of-core construction and visualization of multiresolution
surfaces. In Symposium on Interactive 3D graphics (2003), pp. 93–102.
[LPC∗ 00] L EVOY M., P ULLI K., C URLESS B., RUSINKIEWICZ S., KOLLER D.,
P EREIRA L., G INZTON M., A NDERSON S., DAVIS J., G INSBERG J., S HADE J.,
F ULK D.: The digital michelangelo project : 3d scanning of large statues. In Proc.
SIGGRAPH 2000 (2000), ACM.

[SSGH01] S ANDER P. V., S NYDER J., G ORTLER S. J., H OPPE H.: Texture mapping
progressive meshes. In Proceedings of SIGGRAPH ’01 (2001), pp. 409–416.
[TCS03] TARINI M., C IGNONI P., S COPIGNO R.: Visibility based methods and assessment for detail-recovery. In Proc. of Visualization 2003 (2003).
[Tol99] T OLEDO S.: A survey of out-of-core algorithms in numerical linear algebra.
161–179.
[WDS04] WALD I., D IETRICH A., S LUSALLEK P.: An Interactive Out-of-Core Rendering Framework for Visualizing Massively Complex Models. In Proceedings of the
Eurographics Symposium on Rendering (2004).
[XP98] X U C., P RINCE J. L.: Snakes, shapes, and gradient vector flow. IEEE Transactions on Image Processing 7, 3 (March 1998).
[ZPvBG01] Z WICKER M., P FISTER H., VAN BAAR J., G ROSS M.: Surface splatting.
ACM Trans. Graphics (SIGGRAPH 2001) (2001), 371–378.

c The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Participating Media for High-Fidelity Cultural Heritage
Veronica Sundstedt1† , Diego Gutierrez2‡ , Fermin Gomez2 , and Alan Chalmers1

2

1 Department of Computer Science, University of Bristol, United Kingdom
Department of Computer Science and Systems Engineering, University of Zaragoza, Spain

Abstract
Computer graphics, and in particular high-fidelity rendering, make it possible to recreate cultural heritage on
a computer, including a precise lighting simulation. Achieving maximum accuracy is of the highest importance
when investigating how a site might have appeared in the past. Failure to use such high fidelity means there is a
very real danger of misrepresenting the past. Although we can accurately simulate the propagation of light in the
environment, little work has been undertaken into the effect that light scattering due to participating media has on
the perception of the site. In this paper we investigate how the appearance of the interior of the ancient Egyptian
Temple of Kalabsha is affected when including dust in the simulation. Given that the sun was a key feature of
Egyptian religion, the correct perception of the sun rays entering the temple and being scattered by the dust may
be important for a better comprehension of that culture.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Virtual Reality I.3.8
[Computer Graphics]: Applications J.2 [Physical Sciences and Engineering]: Archaeology

1. Introduction
In many graphics applications, including virtual archaeology, it is assumed that light travels through a nonparticipating medium, normally clear air or a vacuum. For
a great majority of synthesised images, this is a satisfactory assumption. However, in some situations it is necessary to include the participating media such as fog, smoke,
dust, humidity or clouds, to provide the required level of
realism within the images, see Figure 1. In archaeological
sites in particular the materials used to provide interior light,
for example candles and wood fires, would have generated
smoke, perhaps significantly affecting visibility in these environments [Rus95]. High-fidelity computer graphics allows
these effects to be investigated in a physically accurate and
a safe, non-invasive manner [DC01].

scattering also opens up new possibilities of exploring how
past environments might have been perceived, allowing ar-

Experimental archaeology and high-fidelity reconstructions, incorporating participating media in the lighting simulation, allow us to recreate the archaeological site and show
how it may have been perceived in the past. Predictive light
† veronica@cs.bris.ac.uk
‡ diegog@unizar.es
c The Eurographics Association 2005.


Figure 1: Photograph of a smoky medieval house in
Southampton.

84

V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

chaeologists to investigate hypotheses concerning architecture, art and artefacts.
In this paper we consider how physically-based participating media should be incorporated when undertaking a reconstruction of a heritage site with a view to investigating
how it may have appeared in the past. We also investigate
how the affect of participating media in the lighting simulation can alter the perception of a virtual environment. The
site we consider is the ancient Egyptian temple of Kalabsha [SCM04].
The rest of the paper is organised as follows. Section 2
presents related background work. In Section 3 we briefly
discuss the basic theory behind participating media. The importance of participating media in simulations for perception
is stated in Section 4. Our chosen case study, the Kalabsha
Temple in Egypt, is presented in Section 5 with the results
presented in Section 6. Finally, conclusions and future work
are outlined in Section 7 and 8 respectively.

2. Background
Lately computer graphics reconstructions have become commonplace in television documentaries, film and the publishing industries as part of presenting ancient cultures. Recent
advances in computer graphics, such as low cost, high performance hardware, tools for efficiently handling data sets
from laser scanners [CCG∗ 03], etc., also enables virtual reconstructions to become a valuable tool for archaeologists as
a way of recording, illustrating, analysing and presenting the
results.
The popularity of virtual archaeology has led to a significant number of virtual reconstructions ranging from nonphotorealistic presentations, Quicktime VR images, realistic looking computer models, augmented reality applications and even full reconstructed urban environments,
for example [FSR97, BFS00, VKea01, WWAD01, DEC03,
RD03, STH∗ 03, GSM∗ 04]. Currently, the value of threedimensional computer reconstructions is limited because
their level of realism cannot be guaranteed. The generated
images may look realistic, but their accuracy is not guaranteed, since they have no physical basis in reality. In order for
the archaeologists to benefit from computer-generated models and use them in a meaningful way, they must accurately
simulate all the physical evidence from the site being reconstructed [DC01]. The virtual reconstruction should not only
be physically correct but also perceptually equivalent to the
real scene it portrays [MCTR98,CDB∗ 02,Mar01]. In a word,
if computer reconstructions are to go beyond mere digital
images and models, becoming a true tool for archaeologists,
physically-based predictive rendering techniques have to be
used, along with adequate tone mapping mechanisms to ensure the image is perceptually accurate on the display device.

3. Modeling participating media
When rendering archaeological scenes which include dust,
smoke, fog, soot, etc. it is also necessary to take into account the scattering of light as it passes through the medium.
All these effects are essential for a physically accurate lighting simulation. This process involves solving the radiative
transport equation (an integro-differential equation), which
is more complicated than the traditional rendering equation
solved by global illumination algorithms [Gla95]. This section introduces the basics principles of light transport in
participating media, but it is not meant to be exhaustive.
For a more complete overview including different resolution
strategies the reader can refer to [PPS97].
Light travelling through participating media interacts not
only with the surface of the objects, but with the medium
itself as well, see Figure 2. Four new types of interaction occur: emission, absorption, in-scattering and outscattering. The coefficients that govern these interactions are
wavelength-dependent, and therefore the equation describing the variation of radiance L in a point x in the direction
w must be written in its wavelength-dependent form (equation 1):
∂Lλ (x,ω)
= αλ (x)Le,λ (x,ω) + σλ (x)Li,λ (x,ω) − (1)
∂x
−αλ (x)Lλ (x,ω) − σλ (x)Lλ (x,ω)
where α is the absorption coefficient, σ is the scattering coefficient, Le is the emitted radiance and Li is the in-scattered
radiance. We can group the last two terms of equation 1,
which refer to absorption and out-scattering, into a single extinction term κλ (x)Lλ (x,ω), where the extinction coefficient
is defined as:

Figure 2: Photograph of volumetric light in an Egyptian
temple.
c The Eurographics Association 2005.


V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

κλ (x) = αλ (x) + σλ (x)

(2)

The in-scattered radiance Li,λ depends of the radiance Lλ
coming from all possible directions ω over the sphere Ω.
This in-scattered radiance can therefore be obtained by integrating over the whole sphere, thus:

Li,λ (x,ω) =



Ω

pλ (x,ω ,ω)Lλ (x,ω )dω

(3)

Inserting equations 2 and 3 into equation 1 we obtain
the so-called integro-differential Radiative Transfer Equation (RTE), which governs light transport in participating
media [Gla95].
∂Lλ (x,ω)
= αλ (x)Le,λ (x,ω) +
∂x

+σλ (x)

Ω

(4)

pλ (x,ω ,ω)Lλ (x,ω )dω −

−κλ (x)Lλ (x,ω)

To describe the distribution of scattered light in the
medium, so called phase functions are used. They represent
the radiance scattered in a given direction divided by the radiance which would have been scattered in that direction had
the scattering been isotropic:

p(x,ω ,ω) =

dL(x,ω)
1 
ω )dω
4π Ω4π L(x,

(5)

Different from the BRDF for surfaces, the integral of the
phase function over the whole domain is adimensional and
normalized. The most common geometry to describe phase
functions is small homogeneous spheres, suspended in the
medium, each one reflecting light diffusely according to
Lambert’s law. However, desert dust is made of more than a
hundred different types of particles, each oriented in random
directions, thus making its characterization a much more difficult task. A good reference and source of data to model dust
properties can be found in [Kah03].
4. Participating media in perceptually-based rendering
One of the goals of recreating cultural heritage sites is to obtain an image that provokes the same sensation to the viewer
as if he/she were seeing the scene in the real world. This
raises the need for tone reproduction, since the range of luminance values in a real scene is almost always orders of
magnitude greater than the range a computer display can
show. Many different techniques have been developed to try
to solve this problem, although none is completely successful yet. A complete survey on tone mapping techniques can
c The Eurographics Association 2005.


85

be found in [DCWP02] and an evaluation of some of them
in [LCTS05].
The dynamic range of luminances in the real world
reaches up to fourteen log units, whereas the optical nerve
can only transmit 1.5 log units. We can adjust this limited
range to the range of the scene through a process known as
adaptation, perceiving data around that adaptation level. The
process is easily explained by the following example: when
we enter a dark room after having been exposed to normal
daylight luminance levels, we first cannot distinguish any detail in the room, since our adaptation level is that of the daylight scene but the room luminances are orders of magnitude
below that level. As minutes pass by, though, we can start
distinguishing the most salient features of the room, since
our adaptation level has changed to that of the room luminances. The inverse process happens when we go back to
daylight luminance levels and are momentarily blinded, until seconds later our adaptation level changes again and we
regain normal vision.
The human visual system is very good at judging brightness, which is a subjective sensation, instead of luminance,
which is the objective physical magnitude of the light energy. The perception of the brightness of an object depends
on the contrasts around it: a dimly lit object will appear much
brighter against a dark background than against a light background. This effect is closely related to the adaptation mechanism explained above.
The way participating media can alter the perception of
things can be obvious or more subtle. Objects might appear
blurry and some detail might be lost. Light scattering and absorption are wavelength-dependent processes, so color perception will also be changed. Under certain conditions, participating media can act as light sources themselves, or at
least have a great influence in the contrasts and luminance
gradients of a scene. As a consequence, the adaptation level
of the observer will effectively be altered, and therefore the
way he/she perceives the scene will change. Several other
effects that depend on luminance levels will be triggered as
well, such as color perception loss under low light levels or
a decrease of visual acuity.

5. Case study: the Temple of Kalabsha
The site chosen for our study is the ancient Egyptian Temple
of Kalabsha. For the convenience of the reader, we repeat the
historical background from [SCM04]. The temple of Kalabsha is the largest free-standing temple of Lower Egyptian
Nubia located about 50 km south of Aswan and built of sandstone masonry. The temple dates back to the Roman Emperor Octavius Augustus, 30 BC, but the colony of Talmis
evidently dates back to at least the reign of Amenhotep II in
1427 - 1400 BC [SN02]. The temple was dedicated to the
Nubian fertility and solar deity known as Mandulis and the
walls are covered with text and inscriptions depicting Egyp-

86

V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

Figure 3: Plan view of Kalabsha.

tian deities such as Isis and Osiris. The temple itself was
never finished.
The design of the temple is classical for the Ptolemaic period, Figure 3, with pylon (4), courtyard (1), hypostyle hall
and a three-room sanctuary. The courtyard just inside the
pylon once had columns on three sides. At either end is a
staircase that leads to the pylon. The pylon is offset, to the
courtyard behind, since it was built on the site of an earlier
structure built by Ptolemy IX. The small rooms in the surrounding wall were used for storage. After the hypostyle hall
(2) are the three chambers, the pronaos (5), the naos (6), or
sanctuary where statues of gods were located, and the adyton (7), which is the innermost or secret shrine. There is also
a nilometer (3), which was used to collect sacred water for
the gods. Two further elements of religious importance remain outside the enclosure wall, which is in turn narrowly
enclosed by another (outer) enclosure wall. At the South
West angle of this latter enclosure is the Mamisi where the
sacred birth of the Pharaoh is venerated. Finally one complete element of the earlier temple is preserved, the so-called
Ptolemaic Chapel. The first enclosure wall is meant to bind
an area of 66.08m x 33.04m. The original overall height of
the Pylon was probably 16.25m [CMRB65, Wri72].
5.1. Dismantling the Temple of Kalabsha
The temple was originally built at Kalabsha (Talmis) but
with the construction of the Aswan High Dam in 1959, it
became apparent that the temple would disappear under the
rising waters of the Nile. In order to save the monument it
was decided that Kalabsha was going to be dismantled and
moved to a new site.
The parties responsible for the transfer were: the German
Nubian Committee, the Foreign Ministry of the German Republic, a German semi-governmental body for administrating foreign aid (GAWI), a major German Civil Engineering
Firm (Hochtief) working with proper archaeological supervision [Wri72].
By August 1961 all the necessary contracts were signed
and those to be engaged were assembled and housed at
Aswan in September 1961. Work on preparing the new site

Figure 4: The temple of Kalabsha in water at the old site:
(top) exterior view of the pylon, (bottom) exterior view of the
courtyard [Wri72].

(Chellal) began immediately and the works flotilla for operations on the Temple proceeded upstream to old Kalabsha
on September 19th to begin the work of recording the monument. All necessary records were established so that the
first blocks were dismantled on November 17th 1961. The
various units of masonry were each identified with a number, and their position shown on a measured drawing. The
blocks were then broken from the bond and removed from
the wall. Dismantling continued day and night until December 9th when the rise of the Nile made further work impossible at the old site. All resources were then transferred to the
work on developing the storage place and the new site for
the re-erection of the Temple.
A second season of dismantling was carried out at the
old site during May to October 1962 and completed so that
all blocks scheduled for re-erection were transported and
ranged in order at the storage place by the new site, where
they were consolidated as necessary. Some 13,000 blocks,
and in total 20,000 tons of stone were dismantled. On October 30th 1962 the first blocks were reset at the new site
and the work of re-building was carried out continuously
with day and night shifts until it was completed in November 1963. The temple now stands 750m to the south of the
Aswan High Dam.
c The Eurographics Association 2005.


V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

87

ties of the card under a known light source were determined
previsouly using a Spectrophotometer. The illumination at
each pixel of the photograph could then be corrected based
on the equivalent pixel value of the green card photograph.
In total 21 different seamless textures were used for the environment, with the largest resolution being 2188 × 945 which
was also repeated for larger areas. An alternative technique
to the green card would have been to use a Macbeth Color
chart and the program, macbethcal, in Radiance [LS98] to
directly compute the correct color and brightness. Neither of
these techniques is perfect and will not work in the presence of highly specular or complex geometrical surfaces,
however, for the diffuse surfaces of the temple, it enables
us to acquire a good approximation of illumination-neutral
textures.
Figure 5: Exterior view of the virtual temple in Alias Maya
without textures.

5.2. Reconstruction of the Kalabsha Temple
The first task in the virtual reconstruction process was to
create a highly detailed geometric model of the temple of
Kalabsha, see Figure 5. Fortunately, when the temple was
dismantled, it was very well documented, including detailed
drawings and measurements, so that it could subsequently
be physically reconstructed. A three-dimensional geometrical model was created based on these architectural plans, visual measurements and historical literature [SR70, Wri72]
using the Alias Maya modelling package. Although a more
detailed model could have been captured if laser scanning
would have been undertaken, the model currently consists
of ∼ 2.6 million polygons. Typically it takes many months
to get laser scanning permission and for the purpose of our
simulation the model is sufficiently detailed.
Equally important to the geometric model, is the representation of the materials, which determines how the light
interacts with the geometry. The materials of the temple are
predominantly diffuse, so it was not necessary to accurately
determine their precise Bidirectional Reflectance- Transmittance Distribution Function (BRTDF) using samples and a
sophisticated device such as a gonioreflectometer. The materials were modelled directly in Lucifer without any significant loss of accuracy. Lucifer is a spectral renderer, described in-depth in [GMAS05].
The textures have been created based on the photographs
taken on site. Since all photographs taken for the textures
also would contain the illumination in which the pictures
were taken, in order to aquire illumination-neutral textures, a
piece of green card was introduced. This was important since
we introduced our own simulated light in the reconstructions [CDB∗ 02, SCM04]. The diffuse nature of the materials allowed us to take TIF photographs, with a static camera,
of materials with and without the card. The spectral properc The Eurographics Association 2005.


5.3. Sun worship
We chose the Kalabsha Temple as our case study due to its
direct relation with the visualization of participating media,
specifically sunrays entering the temple. The sun was a key
feature of ancient Egyptian religion. The worship of the sun,
although not peculiar to any one time or place, received its
greatest prominence in ancient Egypt. There, the daily birth,
journey, and death of the sun was the dominating feature of
life.
To express one aspect of the sun god, the Egyptians used
a humble analogy from their observation of nature, the beetle pushing a ball of dung along the ground. The scarab has
continued to be a solar amulet in life and burial and are commonly depicted on temple walls in Egypt [Qui01]. The gateway of the Kalabsha temple also contains inscriptions of the
disk of the sun as well as scenes of the king giving sacrifices
and praying [Wri72].
Astronomy was used by the Egyptians to accurately position their pyramids and temples. The ancient Egyptians also
built sun temples that were aligned so that at sunset of the
summer solstice, sunlight would enter the temple and make
its way along the axis of the building to the sanctuary. These
sun temples helped in determining the length of a year because the sun would only penetrate the temple in that way
once per year.
6. Results
The new location and orientation of Kalabsha means that
without computer graphics it would not be possible to vi-

LAT (DMS):
Lon (DMS):
Altitude:

Old location
23 33’ 0N
32 52’ 0E
172m

New location
24 4’ 60N
32 52’ 60E
141m

Table 1: Position coordinates for the Kalabsha Temple.

88

V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

Figure 6: High-fidelity reconstruction and sun simulation of
the ancient Egyptian temple of Kalabsha rendered in Radiance using a daylight data derived from Commission Internationale de l’Eclairage (CIE) standards [LS98].

sualise the effect of the sun on the temple as it would have
appeared to the ancient Egyptians. By knowing the carefully
chosen co-ordinates of the original location of the Kalabsha temple, see Table 1, this allows us to place the computer
model back to where it was originally built. A sun simulation can then be made to study how the light shines on the
temple and its interior during a day in 30 BC, see Figure 6.
But to accurately recreate how the sunrays looked inside
the temple, we also need to incorporate participating media
in our simulations, in the form of dust. The perception of the
interior changes dramatically when this usually overlooked
element is added, and it more closely reassembles the look it
might have had two millennia ago.
We have chosen one of the inner three chambers for our
investigation of how participating media might alter the perception of the Kalabsha Temple. The three rooms have only
got small windows high up on each wall for light to enter
through them. This makes these rooms especially interesting
since there is practically no direct sunlight entering there,
and thus the participating medium plays a key role in the
transport of light throughout the scene. Photographs from
other sites with similar architecture, see Figure 2, show how
the sunlight scatters through participating media when entering the chambers, greatly altering the visual sensation they
provoke.
Figure 7 shows a close up of one of the modeled windows, with and without participating media. It is evident
how the sunrays play a key role in the perception of the
scene. Figure 8 shows the difference in how the interior of
the Kalabsha temple might look with and without taking into
account participating media. The presence of participating

Figure 7: Visual comparison of two renderings of one interior window of the Kalabsha temple rendered in Lucifer:
(top) without participating media, (bottom) with participating media.

media in the simulation (dust from the sandy environment
of the temple) creates a whole new luminance distribution
and, what is more important, a very different gradient distribution, thus changing dramatically the perception of brightness. Colors and details are almost indistinguishable in the
background, since the eye is adapted to the higher luminance
levels of the foreground. As a tone mapper, we have used
SEKER [GSAM04], adding a model of the bleaching effect
in the human visual system as described in [GAMS05].
7. Conclusions
In this paper we have showed the importance of rendering
an archaeological site with physically-based lighting which
incorporates participating media. This technique provides
a safe and controlled process in which the archaeologists
can experiment with different hypotheses concerning the
lighting conditions present, with and without a participating
medium, and how this could have affected site utilisation.
c The Eurographics Association 2005.


V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

89

levels in the temple to validate the dust levels chosen for our
simulations. Since this work can be considered as a proofof-concept, heuristic values for scattering and absorption
coeffcients have been used, along with a generic HenyeyGreenstein phase function, with varying values for the g parameter. Future experiments will also include smoke from
torches or tapers which are known from representations on
the walls of the New Kingdom tombs and temples. Furthermore, the use of an eyetracking device would allow us
to measure involuntary eye movements and attention shifts,
which could tell us more about the areas of the temple that
are emphasised under different simulation conditions. We
also intend to extend our simulations to exterior images, including atmosphere and dust scattering.
Finally, we plan on extending our simulations to underwater archaeology. Water is a very thick participating medium,
where lots of interactions with light occur. Depending on its
characteristics, its hue and visibility can change greatly, thus
altering the perception of the underwater site. Once again, by
using a physically-based model of water we can extend the
scope of the simulations to predictive tasks, such as planning
dives or rescue missions.
9. Acknowledgements

Figure 8: Visual comparison of two renderings of the interior of the Kalabsha temple rendered in Lucifer: (top) without participating media, (bottom) with participating media.

It has been shown how modeling participating media in
certain environments can greatly affect the way something
is perceived. In the field of cultural heritage, where most of
the digital reconstructions are done to show the public how
a certain building or site may have looked like, or to better
understand certain aspects of them, misrepresenting the past
might lead to the wrong conclusions.
8. Future work
While physically-based participating media is a must in certain applications, simulating it is a very computation expensive process. Shortcuts that fail to reproduce the exact phenomena involved might again lead to misinterpretations of
the past. We are currently concentrating on perceptual techniques to distribute rendering accuracy based on a previous
knowledge of how the observer would see the real scene under the conditions of the simulation.
A future visit to Kalabsha is planned to measure the dust
c The Eurographics Association 2005.


This work was partially supported by the Rendering
on Demand (RoD) project within the 3C Research programme of convergent technology research for digital
media processing and communications. For more information please visit www.3cresearch.co.uk. This work
has also been partly funded by the Asia-Link Programme of the European Commission, under the contract
ASI/B7-301/98/679/051(072471) (Development of Multidisciplinary Management Strategies for Conservation and
Use of Heritage Sites in Asia and Europe).
References
[BFS00] BARCELÓ J. A., F ORTE M., S ANDERS D.: Virtual Reality in Archaeology. BAR International Series,
843, Archaeopress, Oxford., 2000.
[CCG∗ 03] C ALLIERI M., C IGNONI P., G ANOVELLI F.,
M ONTANI C., P INGI P., S COPIGNO R.: VCLab’s
Tools for 3D range data processing. In Proceedings of
VAST2003 (2003), Brighton, pp. 9–18.
[CDB∗ 02]

C HALMERS A., D EVLIN K., B ROWN D., D E P., M ARTINEZ P., WARD G.: Recreating the
Past. SIGGRAPH 2002 Course 27 (274 pages ACM SIGGRAPH, 2002).
BEVEC

[CMRB65] C URTO S., M ARAGIOGLIO V., R INALDI C.,
B ONGRANI L.: Kalabsha. Aziende Tipografiche Eredi
Dott. G. Bardi, Roma, 1965.
[DC01] D EVLIN A., C HALMERS A.: Realistic Visualisation of the Pompeii Frescoes. In Proceedings of Afrigraph
(2001), Cape Town, pp. 43–48.

90

V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

[DCWP02] D EVLIN K., C HALMERS A., W ILKIE A.,
P URGATHOFER W.: Star: Tone reproduction and physically based spectral rendering. In State of the Art
Reports, Eurographics 2002 (September 2002), Fellner
D., Scopignio R., (Eds.), The Eurographics Association,
pp. 101–123.

[PPS97] P EREZ F., P UEYO X., S ILLION F. X.: Global
illumination techniques for the simulation of participating media. In Rendering Techniques ’97 (Proceedings of
the Eighth Eurographics Workshop on Rendering) (New
York, NY, 1997), Dorsey J., Slusallek P., (Eds.), Springer
Wien, pp. 309–320.

D IKAIAKOU M., E FTHYMIOU A., C HRYSAN Y.: Modelling the Walled City of Nicosia. In Proceedings of VAST2003 (2003), Brighton, pp. 57–65.

[Qui01] Q UIRKE S.: The Cult of RA: Sun Worship in Ancient Egypt. Thames and Hudson Distributors LTD, 2001.

[DEC03]
THOU

[FSR97] F ORTE M., S ILIOTTI A., R ENFREW C.: Virtual Archaeology: Re-Creating Ancient Worlds. Harry N
Abrams, 1997.
[GAMS05] G UTIERREZ D., A NSON O., M UNOZ A.,
Perception-based rendering: eyes wide
S ERON F.:
bleached. In EUROGRAPHICS 2005 Short Presentations
(2005), J. Dingliana F. G., (Ed.), pp. 49–52.

[RD03] ROUSSOU M., D RETTAKIS G.: Photorealism and
Non-Photorealism in Virtual Heritage Representation. In
Proceedings of VAST2003 (2003), Brighton.
[Rus95] RUSHMEIER H. E.: Rendering participating media: Problems and solutions from application areas. In
Proceedings of the Fifth Eurographics Workshopon Rendering (1995), Springer-Verlag.

[Gla95] G LASSNER A.: Principles of digital image synthesis. Morgan Kaufmann, San Francisco, California,
1995.

[SCM04] S UNDSTEDT V., C HALMERS A., M ARTINEZ
P.: High fidelity reconstruction of the ancient egyptian
temple of kalabsha. In AFRIGRAPH 2004 (November
2004), ACM SIGGRAPH.

[GMAS05] G UTIERREZ D., M UNOZ A., A NSON O.,
S ERON F.: Non-linear volume photon mapping. In Eurographics Symposium on Rendering (2005), pp. 291–300.

[SN02] S HAW I., N ICHOLSON P.: The British Museum:
Dictionary of ancient Egypt. The British Museum Press,
2002.

[GSAM04] G UTIERREZ D., S ERON F., A NSON O.,
M UNOZ A.: Chasing the green flash: a global illumination solution for inhomogeneous media. In Spring Conference on Computer Graphics (2004), Pasko A., (Ed.),
pp. 95–103.

[SR70] S IEGLER K. G., ROMBOCK U.: Kalabsha: Architektur und Baugeschichte des Tempels. Gebr. Mann
Verlag, Berlin, 1970.

[GSM∗ 04] G UTIERREZ D., S ERON F., M AGALLON J.,
S OBREVIELA E., L ATORRE P.: Archaeological and cultural heritage: Bringing life to an unearthed muslim suburb in an immersive environment. Journal of Cultural
Heritage (ISSN 1296-2074) 5(1) (2004), 63–74.
[Kah03] K AHNERT F.: Reproducing the optical properties of fine desert dust aerosols using ensembles of simple
model particles. In Journal of Quantitative Spectroscopy
and Radiative Transfer (2003), vol. 85, pp. 231–249.
[LCTS05] L EDDA P., C HALMERS A., T ROSCIANKO T.,
S EETZEN H.: Evaluation of tone mapping operators using
a high dynamic range display. In ACM SIGGRAPH 2005,
LA. (August 2005), ACM Press.
[LS98] L ARSON G. W., S HAKESPEARE R.: Rendering
with RADIANCE: The art and science of lighting simulation. Morgan Kauffman, 1998.

[STH∗ 03] S TUMPFEL J., T CHOU C., H AWKINS T.,
M ARTINEZ P., E MERSON B., B ROWNLOW M., J ONES
A., Y UN N., D EBEVEC P.: Digital Reunification of
the Parthenon and its Sculptures. In Proceedings of
VAST2003 (2003).
[VKea01] V LAHAKIS V., K ARIGIANNIS J., ET AL .
M. T.: ARCHEOGUIDE: First results of an augmented
reality, mobile computing system in cultural heritage
sites. In Proceedings of VAST2001 (2001), Athens,
pp. 131–140.
[Wri72] W RIGHT G.: Kalabsha: The preserving of the
temple. Gebr. Mann Verlag, Berlin, 1972.
[WWAD01] W ILLMOTT J., W RIGHT L., A RNOLD D.,
DAY A.: Rendering of large and complex urban environments for real time heritage reconstrucitons. In Proceedings of VAST2001 (2001), Athens, pp. 111–120.

[Mar01] M ARTINEZ P.: Digital Realities and Archaeology: a difficult relationship or a fruitful marriage? In Proceedings of VAST2001 (2001), Athens, pp. 9–16.
M C NAMARA A., C HALMERS A., T ROS T., R EINHARD E.: Fidelity of graphics reconstructions: A psychophysical investigation. In Proceedings of the 9th Eurographics Workshop on Rendering
(1998), pp. 237–246.

[MCTR98]
CIANKO

c The Eurographics Association 2005.


The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Real-time Shader Rendering for Crowds in Virtual Heritage
Pablo de Heras Ciechomski, Sébastien Schertenleib, Jonathan Maïm, Damien Maupu and Daniel Thalmann
VRLab, EPFL
CH-1015, Lausanne, Switzerland
{pablo.deheras, sebastien.schertenleib, jonathan.maim, damien.maupu, daniel.thalmann}@epfl.ch
http://vrlab.epfl.ch

Abstract
We present a method of fully dynamically rendered virtual humans with variety in color, animation and appearance. This is achieved by using vertex and fragment shaders programmed in the OpenGL shading language
(GLSL). We then compare our results with a fixed function pipeline based approach. We also show a color variety creation GUI using HSB color space restriction. An improved version of the LOD pipeline for our virtual
characters is presented. With these new techniques, we are able to use a full dynamic animation range in the crowd
populating the Aphrodisias odeon (which is part of the ERATO project), i.e., a greater repertoire of animations,
smooth transitions and more variety and speed. We show how a multi-view of the rendering data can ensure good
batching of rendering primitives and comfortable constant time access.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Virtual Heritage]: Computer Graphics Animation

1. Introduction
We are working on the project - Identification, Evaluation
and Revival of the Acoustical heritage of ancient Theaters
and Odea - (ERATO) [ERA05], where our responsibility lies
in recreating the Aphrodisias odeon in Turkey and filling it
with a virtual humans audience that follows a play on stage
[dHCUDC04, UdHCT04, dHCSMT05, TCU∗ 04]. The work
on the Odeon is now finalized and some pictures of it can be
seen in Figure 1.
Since the beginning of the project, our focus has always
been on creating a graphical pipeline that could be applied
to many different scenarios and cultural heritage settings.
We would like the user of our library to be able to quickly
get a heritage application running from scratch, through the
use of customizable meshes, textures, colorings, behaviors
and scenarios. One of our contributions in this paper lies in
the use of HSB color constraining with a tool specifically
developed for our designer needs. These color constraints
are specifically designed for fast prototyping and final delivery of virtual heritage crowds with historically significant
c The Eurographics Association 2005.

constrained color ranges. We are able to display a full dynamic range of animations and transitions between animations computed with a minimal memory footprint. This is
achieved on the fly for an entire crowd of virtual characters,
thanks to an improved deformation pipeline using hardware
vertex and fragment shaders. Finally we show how a tool
from the OGRE3D distribution [Ogr05] can be adapted and
improved to handle virtual human LODs with texture seam
preservation.
Variety is defined as having different forms or types and
is necessary to create believable and reliable crowds in opposition to uniform crowds. For a human crowd, variation
can come from the following aspects: gender, age, morphology, head, kind of clothes, color of clothes and behaviors. We propose here a way to create as many color
variations as possible from a single texture. Several works
have been done on the subject. [TLC02] propose an imagebased rendering approach to display a crowd. Variety comes
from multipass rendering where different colors are assigned
to significant parts of the body such as clothes, hair and

92

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

Figure 1: The final Aphrodisias odeon version with sculptures

skin color. [dHCUDC04] propose to create several template meshes, which are at run-time cloned to produce a
large number of people. These clones can then be modified by applying different textures to create variety. Similar work that also treats meshes with variety uses a static
pre-deformed mesh ( [RFD04, DHOO05, CLM05]). The difference between these approaches is the use of one, two,
or three levels of geometrical details before starting to render the impostors. There is one exception to the rule of going from strict geometric rendering to impostors in [WS02],
where they use a smooth transition from triangle meshbased rendering to particle splats. However, none of these
approaches attack the problem of editing colors variety according to designer needs, nor do they address the problem
of using a large animation palette. Indeed, all crowd rendering papers to date employ only one walking animation
except for [UdHCT04, dHCUDC04, dHCSMT05, GSM04].
The main reason for this lack of animation variety is that
when using a billboarded crowd, this variety needs to be
constrained since billboarded approaches could be extended
to use more animations but would explode the memory requirements in the process. Having one or a few animations
is fine when one works with static meshes, but as can be
seen in [dHCSMT05], this leads to a memory explosion
when several template meshes are added with multiple animations each. This was solved in [dHCSMT05] by having
dynamically deformed meshes with animations and geometrical caches. We build our meshes upon this idea and improve the color editing, the animation variety and the overall
performance.
2. Variety Rendering
Our variety in rendering extends the way Tecchia et
al. [TLC02] use to create color variety from a single texture
to dynamically animated 3D virtual humans. Our goal is to
have a wide variety of colors and appearances by combining our texture and color variety (see Section 5). Each mesh
has a set of interchangeable textures and the alpha-channel
of each texture is segmented in several zones: one for each
body part. This segmentation is done using a desktop publishing software (in our case Adobe PhotoShop [Pho05]).
We present two possible approaches to use the alpha layer

Figure 2: Texture and alpha zone map of a patrician woman

information for creating color variety. The first approach is
software-based and runs on graphics cards with a fixed function pipeline. The second one uses a fragment shader.
2.1. A Software Approach: Fixed Function Pipeline
An alpha zone is denoting a part of the texture of the character that is to be modulated with a certain color. For example
an alpha zone is a specific part of a dress or some jewelry that
we want to be able to color to differentiate crowd members
from each other. Some triangles, denoted as "dirty" triangles, can span several such alpha zones that have to be colored differently. An example of such triangles can be seen
in Figure 2 where triangles close to the border between the
face skin color and the hair are overlapping. A solution consists in re-triangulating "dirty" triangles to have them cover
each zone with more triangles. Even if this could be easily
achieved, it is to be rejected, because there would be no control on the amount of new triangles generated and a nonsensical situation could appear: a lower LOD mesh having more
triangles then a higher LOD mesh. Our approach consists of
using alpha tests on a split mesh to reduce the work of the
graphics card. Meshes are quickly split at startup in several
sub-meshes: one per group of uniform triangles, plus one
for the "dirty" triangles. This splitting is achieved by plotting every triangle over the texture’s alpha layer. During the
plotting, if a change in the alpha value appears, the triangle is
then considered as "dirty". Once the mesh is split, multipass
rendering is done only on the "dirty" triangle sub-mesh.
The main drawback of the software approach is that the
c The Eurographics Association 2005.

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

rendering complexity depends on the amount of "dirty" triangles. If some alphas overlap a large amount of triangles,
the rendering will slow down accordingly, which makes the
frame rate depend considerably on the chosen texture. Moreover, it also depends on the way alpha zones are designed.
One willing to use this splitting mesh technique should take
into account this issue and try to reduce as much as possible
the amount of "dirty" triangles by being extra careful when
creating textures. A way to ensure this is to have zones with
uniform alpha values connected by a large pixel neighborhood. However, this approach is able to run on our crowd
generator on a large install-base of machines.

used here since we need separate bilinear and nearest neighbor filtering. In fact, the alpha layer itself has to be nearest
neighbor filtered separately, while the RGB layer must be
bi-linearly filtered when the Mipmaps are being built. This
is not yet implemented.
2.4. Color
The color variety presented here is based on texture color
modulation. Each fragment is colored by modulating the
pixel color by the texture color: thus, the value produced by
the texture function is given by:
Cv = Ct C f

2.2. A Hardware Approach: Shaders for Color Variety
In our continuing process to improve the execution speed of
our crowd inside the Aphrodisias odeon, we have decided
to explore the performance of high-end consumer graphics cards, specifically the nVidia Geforce 6 series [nVi05]
with support for GL Slang (GLSL) by 3DLabs [3DL05] and
shader model 3.0. Each character is able to have up to 256
different alpha key areas corresponding to a certain body part
like in [GSM04]. Using the software approach, this is completely unreasonable, as it would require 256 passes for the
uniform alpha triangles and another 256 passes over the dirty
ones. Although the hardware approach could manage such
a number of areas much better than the software approach,
in practice, we only use up to 10 different areas, such as
lips, eyes, hair, parts of the dress, jewelry etc. In the fragment shader, we have to determine for each pixel which area
it is part of and then color it with the appropriate modulating color. In our first implementation of the fragment shader,
we had an if-else statement check for determining the correct color to apply with a limitation of 8 alpha zones. Since
Shader model 3.0 allows nested if statements, we can do
with only 3 if evaluations per pixel. Another way to program the fragment shader is to send a one dimensional texture and map the alpha value to the color we want to modulate with, for a specific character. In order to batch drawing
calls [Wlo03], we put all the individual one dimensional textures into one 2D texture of 1024 times 256 size. Each row
in this texture atlas maps from an alpha value to a color value
and only one extra variable has to be sent to the mesh in the
form of an identifier for which row to sample from.

2.3. Filtering
Our color variety tool rests on a precise control of alpha key
values. While uploading textures to the graphics card, such
filtering as "nearest" filtering is preferable to a "linear" filtering. Indeed a "linear" filtering would create new alpha values at the border of two alpha zones and thus, pixels at these
borders would not be drawn. However, nearest neighbor filtering is very gross and, to soften the texture, Mipmaps are
required [Wil83]. OpenGL’s Mipmap creation tool cannot be
c The Eurographics Association 2005.

93

(1)

where f refers to the incoming fragment and t to the texture
image. Colors Cv , Ct , and C f are values between 0 and 1. In
order to have a large panel of reachable colors, Ct should be
as light as possible, i.e., near to 1. Indeed, if Ct is too dark,
the modulation by C f will give only dark colors. On the other
hand, if Ct is a light color, the modulation by C f will provide
not only light colors but also dark ones. This explains why
part of the texture has to be reduced to a light luminance,
i.e., the shading information and the roughness of the material. Passing the whole texture as luminance does not make
sense. First, there is no gain in memory : OpenGL will emulate an RGB texture based on luminance values, because
graphics cards are optimized for RGB textures. The drawback of passing the main parts of the texture to luminance is
that "funky" colors can be generated, i.e., agents are dressed
with colors that don’t match. Some constraints have to be
added when the modulating colors randomly. With the RGB
color system, it is hard to constrain colors effectively. That
is why we use the HSB system [Smi78], also called HSV,
meaning Hue, Saturation, Brightness (respectively Value).
This model is linked to the human color perception and is
more user-friendly than the RGB system. In Section 5, we
present a graphical user interface that has been created for
helping designers to set constraints on colors.
2.5. A Hardware Approach: Shaders for Deformation
By re-writing the vertex deformation step into a hardwarebased vertex shader, we can no longer re-use the result of a
computation on the vertices in a software memory caching
scheme, as in [dHCSMT05]. However, the benefits are twofold: first, as the graphics pipeline on our cards are made
of 6 vertex processors working in parallel [nVi05], we have
a major speed-up. Second, our variety coloring can now
be done in one pass per pixel in the fragment shader (not
as in the software approach, which is a multi-pass solution
[dHCSMT05]). Thus, we have 16 of these fragment shaders
working in parallel.
Each vertex needs the array of deformation matrices, indices and weights for these matrices, a normal, a position,
and a texture coordinate. A way of sending this data to the

94

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

graphics card is through attribute vectors [3DL05]. Instead,
we prefer to send the per vertex attributes embedded in the
standard components [SWND03], i.e., in our case : the color,
texture coordinate, position and normal. This is achieved by
storing up to three matrix indices in the RGB color component, the number of affecting bones in the alpha color component, the weights of bones one through three in the w component of the position and the third and fourth component of
a four dimensional texture coordinate. We also ensure that
for low LODs, the character will not use more than one bone.
This allows us to use the if statement in the vertex shader
that is part of shader model 3.0 instead of activating another
program. An if statement only costs 2 cycles according to
nVidia sources.
To better use the graphics card, we need to store the geometric data, the weights, the texture coordinates, the number of bones affecting each individual vertex and so on. This
is done by storing the glDrawElements call in a display list.
The OpenGL driver will store it in in an optimal form so that,
for rendering a character, we will only need to send a few attributes; in this case the transformations of each bone, which
row to sample from the 2D alpha to color texture, and a variable stating ( for a character supposed to use only one bone
per vertex). Thus, we decrease the communication to the
graphics card to a minimum and the rendering is achieved
in one pass.
2.6. Comparison of Shaders Versus a Software
Approach
In our first approach to create color variety for the virtual
humans, we were using a multi-pass rendering for each colored region and we optimized it so that only the triangles that
were fully inside a region were rendered in blocks. The triangle that passed several regions were rendered several times,
using alpha checks (see [dHCSMT05]). With a software approach, the decision process is done by the alpha function
which has to do up to 8 passes of sub-mesh rendering and
one pass per triangle group fully withing an alpha region.
This gives us 1 pass per sub-mesh and 8 passes for the dirty
triangles, which is still faster than to use 8 passes considering all triangles as dirty. In the shader approach only one
pass is done over the triangles though the fragment program
has to run on all rendered pixels. Since the fragment program
is only 6 GLSL lines using two texture look-ups to get the
final color it is extremely fast.
On the side of the vertex shader we only do one OpenGL
display list call for each human and upload the attributes
of bone matrices along with only two extra variables. This
gives us an enormous advantage in terms of rendering primitive calls comparing to the software approach which does
up to 16 glDrawElements calls. Storing the geometry and
variables of the mesh in graphics card memory even if it has
to be deformed on the hardware gives a considerate boost in
performance as seen in the results section.

3. System Architecture
Crowd simulations require addressing many problems from
different perspectives. In our experiments, a typical scenario
features more than a few hundreds distinct virtual characters. The number of assets to be created and controlled interactively within the simulation is becoming a predominant factor. Naturally, the real-time performance of such a
virtual environment depends on the appropriate usage of a
3D API such as OpenGL [Ope05]. These APIs come with
design particularities that oblige the developers to organize
their rendering pipeline in a very strict order. Unfortunately,
this organization is not intended to be easily controllable for
high-level simulation designers. Very often, developers have
to make some compromise between real-time performance
and the flexibility offered by their system. In order to overcome this barrier, we have improved our scene management
by extending the way the data can be fetched. The idea is to
provide different views based on the specific requirements
of every module.
3.1. Multi-View Scene Representation
By providing different view representations of the same
shared data, we can optimize both the access time and the
different components computation. For instance, the GPU
rendering pipeline and the semantic representation have distinct requirements. The 3D rendering module needs to classify 3D meshes among their rendering cost penalty (OpenGL
state sorting, shaders parameters bindings, etc), while the semantic view may access them by their unique ID. Our approach tends to overcome the limitations of classifying objects in a unique list or queue. To implement this multi-view
representation, we are relying on the multi-index container
from the [Boo05] library. This container maintains multiple
indices with different sorting and access semantics. It goes
beyond the single view representation that the STL map or
set may offer. The original idea of this concept came from
the indexing theory as used in relational databases. Thus,
we can simultaneously optimize data access for spatial, state
or semantic considerations, by fitting precisely their local
requirements. Figure 3 depicts such a container with three
view indices on the same shared data.
This design does not only allow a perfect balancing between the 3D rendering and semantics view, but it also offers
different possibilities to optimize every single component.
For instance, 3D rendering optimizations are often platform
specific : some hardware may be more efficient to fetch different textures than to modify their lighting computation
mode. By extending our multi-index container with additional views, we can adapt our system more easily. This is
particularly important for controlling the simulation, as different semantic indexing may be necessary. They may represent more logical or intuitive views for accessing the different objects could they be using unique ID or by classifying them in different categories (human, dynamic object,
c The Eurographics Association 2005.

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

95

Figure 3: Multi-view representation

static object). From a performance perspective, as our virtual worlds are populated with thousands of dynamic entities, it is really important that we keep a constant time access on every representation. The container implementation
is ensuring this requirement. Moreover, the spatial efficiency
becomes acceptable with regards to memory consumption as
the number of indices grows. Finally, the memory fragmentation remains minimal especially in comparison to manual
management, which can show up in more usable memory
available and better performance.
3.2. Multithreading for Crowd Simulations
Recent developments in CPU technology have extended the
install-base of computers capable of running multiple hardware threads simultaneously. However, taking advantage of
this advance in hardware technology involves significant
high level modifications within the system design. [Kru05]
have shown that providing an architecture that can scale into
dedicated processes will allow creating crowd simulations
that are currently only possible in a non interactive mode.
Our approach is based on separating the flow of control between three different threads, as described in Figure 4. Each
of them is responsible for one specific component. For instance, we use a dedicated process for the 3D rendering,
another one for the AI processing and a third one for the
event handling and script executions. In order to fully take
advantage of our multi-threaded approach, we need to keep
the shared data as minimal as possible. Most current PC
platforms like the Intel Pentium IV processor with HyperThreading or Dual-Core CPUs [Bin03] are capable of running two hardware threads simultaneously. By exploiting
three active threads, we are optimizing the resource usage.
One may argue that using one more active thread than the
number supported by the processors may be suboptimal, but
it occurs that the rendering thread is mainly bound on I/O operations, waiting for some acknowledgment from the GPU.
The system architecture and performance also scope better
with current trend in CPU hardware design, which features
multiple simpler in-order cores ( [Wan05], [Del]). By separating different components within our architecture, we are
c The Eurographics Association 2005.

Figure 4: Threading Work Flow Design Model

able to generate more complex simulations with the same
level of interactivity.
3.3. Animation Blending
To obtain a more believable and realistic crowd, every agent
in our system has a distinct behavior. To increase the variety,
we are using a bank of animations simulating the different
behaviors. Adding noise functions in the selection and the
execution of every animation provides the feeling that every animation is unique. However, the animations transitions
are clearly visible as the last and first frames of both animations do not match, introducing some jittering artifacts. To
solve this problem, we have extended our animation system
by introducing animation blending : rather than waiting for
an animation completion, every agent may get a notification
to blend their current animation with the next one. By varying the blending duration and by choosing different blending
modes, we obtain more variety within the simulation. Figure 5 shows the animation transition over time.
4. Geometric LODs
To increase the number of displayed virtual humans, we render them using more or less simplified polygonal representations, i.e., we choose the appropriate geometric LOD for
each virtual human depending on its distance to the camera.
The integration of a tool present in the OGRE3D [Ogr05]
distribution into our asset pipeline allows us to easily create
these LODs ( [dHCSMT05]). However we had to adjust the
tool to fit our needs. Indeed, we have observed some visual
artifacts produced by the OGRE3D tool on the textures of
our characters. We have fixed the tool by increasing the texture seam preservation, i.e., by associating an infinite cost
to the action of moving away from a seam. This fix allows
us to reduce the geometry of every template until a limit of
about 400 polygons. Going further than this limit introduces
another kind of artifacts, this time on the produced geometry.

96

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

Figure 7: HSB color space. Hue is represented by a circular
region. A separate square region may be used to represent
saturation and brightness, i.e., the vertical axis of the square
indicates brightness, while the horizontal axis corresponds
to saturation.

Figure 5: Animation Blending Pipeline

Figure 8: The HSB space is constrained to a three dimensional color space with the following parameters (a): hue
from 20 to 250, saturation from 30 to 80 and brightness from
40 to 100. Colors are then randomly chosen inside this space
to add variety on the eyes texture of a character (b).
Figure 6: Random color system (a) versus HSB control (b).

5. Designing Variety
In the process of designing Romans and more generally human color variety, we must deal with localized constraints :
some body parts need very specific colors. For instance, roman skin colors are taken from a specific range of unsaturated shades with red and yellow dominance, almost deprived of blue and green. Eyes are described as a range from
brown to green and blue with different levels of brightness.
These simple examples show that we cannot use a random
color generator as is. We need a tool that allows us to control the randomness of color parameters for each body part
of each roman (see Figure 6).
5.1. Color Models
The standard RGB color model representing additive color
primaries of red, green, and blue is mainly used for specifying color on computer screens. In order to quantify and
control the color parameters applied to the roman crowd, we
need a user-friendly color model. Ray Smith ( [Smi78]) proposes a model that deals with everyday life color concepts,
i.e., hue, saturation and brightness. This system is the HSB
(or HSV) color model. The hue defines the specific shade of
color, as a value between 0 and 360 degrees. The saturation

denotes the purity of the color, i.e., highly saturated colors
are vivid while low saturated colors are washed-out like pastels. We represent saturation as a value between 0 and 100.
The brightness measures how light or dark a color is, as a
value represented between 0 and 100. The color space represented by the HSB model is shown in Figure 7.
5.2. HSB Color Model as a Tool for Designing Variety
We have found in the HSB color model an intuitive and flexible manner to control color variety. Indeed, as shown in Figure 8, by specifying a range for each of the 3 parameters, we
are able to define a three-dimensional color space, that we
call HSB map.
We have built a GUI so that a designer can easily load,
modify and save HSB maps for different human templates
(see Figure 10). This GUI provides all the necessary tools to
efficiently :
• change the number of virtual humans rendered,
• select which virtual human template one wishes to work
with,
• choose which texture of the selected template one wishes
to work with,
• choose on which body part (alpha value) of the selected
texture one wants to define color ranges,
c The Eurographics Association 2005.

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

97

Figure 9: Decurion women with saturation from 40 to 80
and brightness from 50 to 70 (left); plebeian women with
saturation from 10 to 50 (right) .

Figure 11: Software and hardware rendering mode comparison (FPS on axis Y) using different number of visible characters (axis X).

6. Results and discussion

Figure 10: Real-time design of lower classes texture variety.
Dialog to select the template to edit and to choose the number of characters (left); the results displayed on the crowd
(middle); dialog to design the variety of the selected template with the possibility to choose the body part along with
the texture of the template to edit (right).

• select a saturation and a brightness range between 0 and
100, and choose a range in the hue circle between 0 and
360 (cycles are allowed), for the currently selected alpha
value.
The result of using these features is visualized in real time,
i.e., every change affects directly each rendered human.

5.3. Variety Case Study : Roman Society
To further illustrate the use of the GUI, we shortly present a
case study in the framework of the ERATO Project [ERA05].
In order to simulate Roman society, we had to differentiate
social classes. These differences were shown through clothing, where not only patterns, but colors as well were defining
the rank of individuals. For instance, decurion women (rich
elite) wore fine fabric with rich colors, while lower class citizens wore simple garments made of raw material usually
dark. HSB maps allowed us to easily specify these significant differences by setting saturation and brightness values
for rich garments and lower for modest ones (see Figure 9).
c The Eurographics Association 2005.

For validating our multithreading design architecture, we
have analysed our software using the Intel VTune Performance Environement [VTu05]. This toolkit allows to clearly
investigate bottleneck issues for multi-threaded applications.
The measurements were done on AMD64 4000+ with 2GB
of memory and a nVidia SLI 6800 Ultra graphics card. The
results depicts that our system is using an average of 75-80%
of all the CPU cycles. According to [Cas04] those results are
really interesting, especially considering that graphics applications are generally bound on I/O operations.
From Figure 11 we can see that the hardware shader approach of rendering the crowd completely outperforms the
software solution even if we use an animation and geometry
cache. In the tests we compared with one template consisting
of 8 geometrical LODs, going from 1026 down to 490 triangles, playing two animations that were mixed at quaternion
level using skeletons (33 bones) and consisting of 3 (512
pixels wide and high) textures. All humans were in view and
the average of minimum and maximum frames per second
were considered.
7. Conclusions and Future Work
By creating a virtual human character using our pipeline
with LODs, texture varieties and color keying zones, animations and animation rules, this is multiplied into a diversified
heterogeneous crowd, running fast on desktop computers.
One template is thus re-usable for many different scenarios
and settings with little or no changes to the data.
In the future we would like to extend the lighting model
for different parts of the character on a per texel value using
the fragment shader. Like this we can have special effects
for the skin, clothes and jewelry for example. Moreover we

98

P. de Heras Ciechomski et al. / Real-time Shader Rendering for Crowds in Virtual Heritage

will improve our tool for designing variety to handle highresolution characters and work toward a custom geometric
LOD creation tool.
We showed that shaders and multithreaded environment
give a massive performance boost of around 700 percent
consistently over the whole data set.

[GSM04] G OSSELIN D., S ANDER P., M ITCHELL J.:
Rendering a crowd. In ShaderX3 : Advanced Rendering
with DirectX and OpenGL (2004), Charles River Media,
pp. 505–517.
[Kru05] K RUSZEWSKI P.: A practical system for real-time
crowd simulation on current and next-generation gaming
platforms.

8. Acknowledgements

[nVi05]

nvidia, 2005. http://www.nvidia.com.

Special thanks to Mireille Clavien and Barbara Yersin for
their help in designing and editing the materials present in
this paper. The presented work is supported by the Swiss
Federal Office for Education and Science and the EC FIFTH
FRAMEWORK INCO-MED PROGRAMME, in frame of
the EU ERATO project, number ICA3-CT-2002-10031.

[Ogr05]

Ogre3d, 2005. http://www.ogre3d.org.

[Ope05]

OpenGL, 2005. http://www.opengl.org.

[Pho05]

Adobe photoshop, 2005. http://www.adobe.com.

References
[3DL05]

3dlabs, 2005. http://www.3dlabs.com.

[Bin03] B INSTOCK A.: Multithreading, hyper-threading,
multiprocessing: Now, what’s the difference? IDS 1, 1
(2003).
[Boo05]

Boost C++ Library, 2005. http://www.boost.org.

[Cas04] C ASEY S.: How to determine the effectiveness of
hyper-threading technology with an application. IDS 1, 1
(2004).
[CLM05] C OIC J.-M., L OSCOS C., M EYER A.: Three
LOD for the Realistic and Real-Time Rendering of
Crowds with Dynamic Lighting. Research Report RR2005-008, Laboratoire d’InfoRmatique en Images et Systèmes d’information, Université Claude Bernard, France,
2005.
[Del] Powerpc
970m.
306.ibm.com/chips/products/powerpc/.

http://www-

[dHCSMT05] DE H ERAS C IECHOMSKI P., S CHERTEN LEIB S., M AÏM J., T HALMANN D.: Reviving the roman odeon of aphrodisias: Dynamic animation and variety control of crowds in virtual heritage. In Proc. 11th
International Conference on Virtual Systems and Multimedia (VSMM 05) (2005).
[dHCUDC04] DE H ERAS C IECHOMSKI P., U LICNY B.,
D. T., C ETRE R.: A case study of a virtual audience in a
reconstruction of an ancient roman odeon in aphrodisias.
In Proc. 5th International Symposium on Virtual Reality,
Archaeology and Cultural Heritage (VAST 04) (2004).
[DHOO05] D OBBYN S., H AMILL J., O’C ONOR K.,
O’S ULLIVAN C.:
Geopostors: A real-time geometry/impostor crowd rendering system. In Proc. ACM SIGGRAPH 2005 Symposium on Interactive 3D Graphics and
Games (2005).
[ERA05] ERATO - identification, evaluation and revival of
the acoustical heritage of ancient theatres and odea, 2005.
project website, http://www.at.oersted.dtu.dk// erato.

[RFD04] RYDER G., F LACK P., DAY A. M.: Adaptive
crowd behaviour to aid real-time rendering of a cultural
heritage environment. In Proc. 5th International Symposium on Virtual Reality, Archaeology and Cultural Heritage (VAST 04) (2004).
[Smi78] S MITH A. R.: Color gamut transform pairs. In
SIGGRAPH ’78: Proceedings of the 5th annual conference on Computer graphics and interactive techniques
(New York, NY, USA, 1978), ACM Press, pp. 12–19.
[SWND03] S HREINER D., W OO M., N EIDER J., DAVIS
T.: OpenGL Programming Guide: The Official Guide to
Learning OpenGL, Version 1.4. Addison-Wesley, 2003.
[TCU∗ 04] T HALMANN D., C ETRE R., U LICNY B.,
DE H ERAS C IECHOMSKI P., C LAVIEN M.: Creating a
virtual audience for the heritage of ancient theaters and
odea. In Proc. 10th International Conference on Virtual
Systems and Multimedia (VSMM 04) (2004).
[TLC02] T ECCHIA F., L OSCOS C., C HRYSANTHOU Y.:
Image-based crowd rendering. IEEE Computer Graphics
and Applications 22, 2 (March-April 2002), 36–43.
[UdHCT04] U LICNY B., DE H ERAS C IECHOMSKI P.,
T HALMANN D.: Crowdbrush: interactive authoring of
real-time crowd scenes. In SCA ’04: Proceedings of
the 2004 ACM SIGGRAPH/Eurographics symposium on
Computer animation (New York, NY, USA, 2004), ACM
Press, pp. 243–252.
[VTu05]

Vtune, 2005. http://www.intel.com.

[Wan05] WANG D.: The cell microprocessor. ISSCC 1, 1
(2005).
[Wil83] W ILLIAMS L.: Pyramidal parametrics. In SIGGRAPH ’83: Proceedings of the 10th annual conference
on Computer graphics and interactive techniques (New
York, NY, USA, 1983), ACM Press, pp. 1–11.
[Wlo03] W LOKA M.: Batch, batch, batch: What does it
really mean? GDC 1, 1 (2003).
[WS02] WAND M., S TRASSER W.: Multi-resolution rendering of complex animated scenes. Computer Graphics
Forum 21, 3 (2002). (Proc. Eurographics’02).

c The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Visualizing Temporal Uncertainty in 3D Virtual
Reconstructions
T. Zuk1 , S. Carpendale1 and W. D. Glanzman2
1

Department of Computer Science
2 Department of Archaeology
University of Calgary, Canada

Abstract
Uncertainty in various forms is prevalent throughout Archaeology. With archaeological site data in particular, the
dating regularly has significant uncertainty. In this paper we present an application that enables integrating and
visualizing the temporal uncertainty for multiple 3D archaeological data sets with different dating. We introduce
a temporal time window for dealing with the uncertainty and review various visual cues appropriate for revealing
the uncertainty within the time window. The interactive animation of the time window allows a unique exploration
of the temporal uncertainty.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism - Virtual Reality; J.2 [Physical Sciences and Engineering]: Archaeology; I.3.6 [Computer
Graphics]: Methodology and Techniques - Interaction Techniques

1. Introduction
Uncertainty in various forms is prevalent throughout Archaeology. Archaeological site data can be recorded in numerous formats ranging from hand drawn sketches to ground
penetrating radar. All of the recorded data usually represents
a minuscule fraction of the information regarding the visual
appearance of a site over time and so missing data forms
a major component of the uncertainty. Of the data that are
available the dating regularly has significant uncertainty.
All archaeological data have a relative chronology value
(for example, an artifact’s placement within a stratigraphic
sequence, or the addition of a wall to an existing building),
and some data also have an absolute chronology value (for
example, coins bearing mint dates, inscriptions mentioning
an event during the reign of a certain ruler) that archaeologists can discern. In both conditions, dating must be thought
of as representing either a span of time during which an
event occurred, or a point in time before or after which an
event occurred. Furthermore, many archaeological sites and
their data sets are incomplete or disturbed, rendering their
chronological value obscure. All chronology pertaining to
archaeological data thus contains uncertainty.
This uncertainty should be integrated into any visualizac The Eurographics Association 2005.

tion to improve the cognitive task of spatio-temporal understanding. To aid in comprehension we present a time window
for the animated visualization of the temporal uncertainty.
We also analyze the applicability of various visual representations appropriate for revealing temporal uncertainty in interactive 3D scene reconstructions.
1.1. Visualization
Often archaeological data is visualized at a specific time in
the past. This can be categorized as a reconstruction, which
when using computer graphics is often called a virtual reconstruction. This has been performed on ancient sites such
as the Visir Tomb [PBM93] up to the recent past with the
Dresden Frauenkirche [Col93]. This methodology can even
be extended into the future for illustrating models of restoration or deterioration.
Usually within an archaeological site, however, data are
collected representing various periods of time. Site data is
3D spatial data acquired during an excavation but the dating
of each of the artifacts is not as precise as the spatial location. The 3D position of an object represents either the final
position of an artifact and thus its last probable use prior to
burial, or it represents its original, intended use and is thus

100

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

in situ, in its original placement on a site. A decision must
be made as to which location the viewer desires to visualize. Integrating the in situ object placement within a virtual
reconstruction (of approximate object burial date) can help
the archaeologist to visualize the use of an object, or hypothesize why the object came to rest in that position. Two
published examples of the visualization of last use locations
relative to in situ architectural reconstructions, are the location of bifaces, scrapers, and debitage (tools and fragments)
within a prehistoric pithouse [PFH95], and lamps and coins
inside the Great Temple of Petra [AVLJ01].
Reconstructions and their integration with archaeological
site data may allow more accurate hypotheses to be made.
Virtual reality can allow the archaeologist to understand the
past context of the 3D spatial layout of their data [vFL∗ 00].
When using a 3D model various lighting or sky/star models can be applied to test other theories as well. For example,
would a certain location within a building have adequate natural lighting for the inhabitant to perform a specific task? All
of these techniques can provide valuable new tools to aid in
interpreting the data.
Using the computational power of current consumerlevel computer graphics technology, interactive animation of
complex 3D scenes is now possible. The animation of time
provides a powerful visualization which allows complex 3D
spatio-temporal changes to be compared in a natural way.
Currently most archaeological visualizations represent spatially static scenes of a speculative nature that represent specific time periods. The following discussion will outline how
to extend this type of visualization by adding increased comprehension of the temporal changes and uncertainty using
interactive animation.
2. Time Windows and Interactive Animation
Any artifact or structure may have an estimated timeline
based on a creation and destruction date (the destruction
may be in the future). Using these dates the 3D scene for
a specific date, or an animation frame, can be constructed by
simply finding which data sets have a timeline that overlaps
the viewing date. However the overlap will be influenced by
the uncertainty in the creation and destruction dates. Uncertainty in these dates may be statistical such as from dating
technology, or more abstract such as when based on scientific judgement [RB00]. This judgement may consider things
such as the likelihood of contamination or just be an expert
estimate based on seriation (relative chronology based on associations).
2.1. Time Windows
The computer generation of an animation frame may use the
photorealistic rendering analogy of the shutter speed of the
camera taking the picture. This allows effects such as motion blur to be recreated for moving objects, or a moving

camera, by sampling the view repeatedly (while the shutter
is open) and then blending the pixels together. In our context we suggest that the frame (viewing time) also take into
account temporal uncertainty.
In expanding the camera shutter concept to a much larger
timescale we create a time window. This allows events on either side of a specific date to be viewed to take into account
uncertainty in the actual viewing time. It can provide a visualization to help in answering a question like: what would a
person have seen if they visited the site between 200 and 210
BCE? Arbitrarily expanding the time window also enables
the viewer to see how later and earlier construction relates in
an intuitive way. The time window could also be interpreted
reciprocally giving all artifacts temporal uncertainty equal to
half the time window.
The time window is illustrated in Figure 1. The time window’s width (range of time) can be controlled by the user.
This window of time allows data that comes within range
of the viewing date to be visualized in some way. The time
window allows two different types of uncertainty to come
into play: the uncertainty in the original dating, and the uncertainty over the time window.
Either the time window or timeline uncertainties can be
mapped to probability density functions or other schemes.
As an example, for the time window the centre can be
thought of as absolute certainty (equal to a probability of
one) and then certainty (probability) can drop off based on
a function (e.g. Gaussian) to zero at each end of the time
window. For the time window alone the uncertainty for an
object would be the maximum certainty function value that
the object timeline overlaps. These certainty functions over
the time window and timelines can be used independently
or combined. The uncertainty measures can then be used to
create visual representations that depict various levels of uncertainty other than the obvious inclusion or exclusion from
the scene.

Figure 1: Time Window. Segments A, B, C, and D represent
data sets and their timelines. The line down the centre of
the box represents a specific viewing time, and all data sets
that overlap this time are displayed normally (B & C). The
dotted-line box extends the standard viewing time to form a
time window. The data sets that only overlap the time window and not the viewing time may be rendered in a way to
indicate uncertainty (A). All data sets outside the time window would not be displayed (D).

c The Eurographics Association 2005.

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

2.2. Interactive Animation
Archaeological animations often are restricted to a specific
reconstruction date and provide a fly-through or a virtual reality experience [FS97]. In some cases a partially interactive
animation over time is created [VPW∗ 04], but these do not
include uncertainty. In these scenarios the rendered frame
represents a small window in time (usually infinitely small)
in contrast to our time window concept.
As time is experienced in a continuous and unstoppable
manner, it is natural to want to explore time interactively.
We provide a graphical user interface in the form of a slider
to allow the user to directly control the temporal position
of the time window. By manipulation of the time slider and
time window the user can create an interactive temporal animation either forward or backward in time. The user controllable animation along with uncertainty visualizations may
provide better temporal comprehension.
3. Visual Representations
Given an uncertainty metric there are numerous ways to render a 3D artifact within a scene to express the uncertainty.
We are concerning ourselves only with uncertainty in time
while ignoring the uncertainty in the other dimensions. Obviously the uncertainty in spatial position is relevant, and is
temporally dependent, as with the Arrigo VII funerary complex reconstruction [BBC∗ 04], but it is beyond the scope of
this paper. We are also limiting our discussion to visual integrations into a standard 3D virtual reality scene that can be
intuitively understood. Honouring these restrictions creates
a visual 3D scene rendering that is compatible with normal
virtual reality systems and only slightly reduces the options.
Non-photorealistic rendering (NPR) methods have been
shown to be able to depict uncertainty required to express speculative designs or constructions [SPR∗ 94, SS02].
Strothotte et al. [SPM∗ 99, SMI99] reviewed aspects of nonphotorealistic rendering and how they can be used in representing uncertainty in virtual reconstructions. They show
how sketch-like renditions and the use of variable transparency can express the speculative nature of archaeological reconstruction. The authors found that photorealistic detail distracted from the fundamental questions of the domain
experts. They conclude that more methods of visualization
and interaction are required for expressing the appropriate
level of uncertainty. Practical aspects of an implementation
using these techniques were presented by Freudenbert et al.
[FMRS01]. Roussou and Drettakis [RD03] have discussed
photorealisitic rendering, NPR, and interactivity, and found
they all have an important role in the perceived realism.
Reusing the camera shutter analogy and sampling the
scene over the time window (and including data timeline uncertainty) is the most straight forward visualization. While it
would be appropriate to integrate the certainty over the time
window, we simply used the maximum certainty in the time
c The Eurographics Association 2005.

101

window. If the maximum certainty of an artifact was 0.2 as
a probability then the opacity could be set to 0.2 to provide
the same effect as motion blur if the object was removed after 2/10ths of a frame. Where spatially incompatible artifacts
occupy the same space they will intersect each other.
3.1. Visual Cues
A visual cue can be defined as any visual encoding (color,
size, animation, etc.) and used to communicate meta-data.
Arbitrary visual cues beyond the motion-blur (transparency)
from the standard camera shutter model move us into styles
of non-photorealistic rendering. In the current context a visual cue is any visual encoding used to distinguish levels of
uncertainty. Some visual cues may be applied to a single artifact while others may cover the entire scene. For example
if fog is applied to only a single object it will be perceived as
color blending, similar to a color saturation cue rather than
environmental fog. Visual cues may also be overloaded in
that they have implicit meanings beyond their use as a representation of uncertainty. This is true for cues such as fog
and blur/depth-of-field [Mac92, KMH01], as a virtual reality rendering may already use these as depth cues [War04]
(visual encoding of the distance to objects in a scene).
In Pang et al.’s survey [PWL97] of uncertainty visualization there are numerous applicable methods including: sideby-side views, pseudo- color, contour lines, blinking, material properties, texture mapping, bump mapping, oscillation,
displacement, and blur. They categorize methods for visualizing uncertainty into the groups: add glyph, add geometry,
modify geometry, modify attributes, animation, sonification,
and psychovisual. We introduce a cue into Pang et al.’s animation category with the use of a rising/sinking animation
during continuous time changes (a form of displacement).
The rising/sinking animation provides a natural transition
animation similar to that of time-lapse photography of construction. A drawback of the rising/sinking cue is that it may
be misinterpreted in a static scene.
The two visual cues of transparency and the rising/sinking
animation are used to illustrate the time window technique
for presenting the uncertainty. Figure 2 contains photographs
with specific dates assigned matching the photograph’s contents. The photographs represent a series of sites which exist
at the current time. They are the Giza Pyramids, the Rammaseum, and the Kiosk of Qertassi near the Temple of Kalabsha. The figure shows three snapshots of the window containing the 3D scene view and time slider view. The uncertainty based on the relative position of a timeline in the time
window is visible in the top two images. The timeline of
each data set (photograph) is shown in a different color and
from top to bottom and corresponds to the photos from left
to right. Visual cues may be classified on various attributes
from perceptual to practical. Bertin’s framework called the
Properties of the Graphic System [BbWJB83] classified visual variables (which often may be used as cues) on the ba-

102

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

4. Implementation
Our application, ArkVis, was developed for visualizing 3D
archaeological data along with their temporal uncertainty.
ArkViz allows the user to import multiple 3D data sets and
assign various properties to them. The most important of
these properties are the dating, or creation and destruction
dates, of the physical artifacts or structures composing a data
set. Uncertainty may be assigned to each of these dates.
The data may be interactively viewed in a 3D perspective
scene. The user selects a date using the time slider and a
scene is automatically generated representing the scene (archaeological or site) at the given time. The user may also
drag the time slider to create a temporal animation. Once a
scene is constructed for a specific time window, ArkVis allows the user to navigate (walking or flying) through the site
at that specific time in history. They may also interactively
manipulate the time window to provide a larger or smaller
portal into the near future and near past. Various visual cues
for the temporal uncertainty of the data may be selected interactively.
The time window may be shifted along with the time
slider or may be specified by directly drawing it. As the concept of vagueness is often tied to uncertainty we also provide
the approximate input of values by allowing the time window to be "sketched" out. This process is shown in Figure 3.

Figure 2: Uncertainty cue animation. Viewing dates
(frames) from top to bottom of 1400, 220, and 30 BCE respectively. Time window constant at 300 years. Top image
shows rising/sinking cue, middle image transparency, and
bottom image no uncertainty.

sis of their characteristics such as the potential for immediate perceptual group selection, natural perceptual ordering (not learned), ability for quantitative comparisons, and
length (the number of discernible elements that can be represented in the set, i.e. cardinality). A summary of some visual cues appropriate for 3D rendering and relevant characteristics (including Bertin’s length and order) are presented
in Table 1. The table also indicates whether direct programming of the graphics processing unit (GPU) would be advantageous, and this will be discussed in more detail in Section 5.2. The practical length of a visual cue depends on the
visual size of the rendered artifact in the frame and so the
categories of small, medium, and large, are relative generalizations.

Figure 3: Approximate time window specification. Top image: no time window only artifact C visible. Middle image:
approximate time window specified with mouse input. Bottom image: new time window based on roughly guided input
in which artifacts B and D would be visible but could be rendered with visual cue of uncertainty. Timeline boundaries
with uncertainty are indicated by smaller sized extensions
with lower color saturation.
c The Eurographics Association 2005.

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

103

Table 1: Visual Cue Characteristics
visual cue
transparency
color change
wireframe
line style (NPR)
shading/hatching (NPR)
floorplan only
rising/sinking
animated warping of surfaces
blur
fog/haze
rain/snow

length
small
medium
2
large
large
2
large
medium
small
small
medium

order
Y
N
Y
N
Y
N
Y
N
Y
Y
Y

artifact/scene
artifact
artifact
artifact
artifact
artifact
artifact
artifact
artifact
artifact
scene
scene

GPU
Y
Y
N
Y
Y
N
N
Y
Y
N
Y

ArkVis was written in C++ using Microsoft’s Visual Studio. Trolltech’s window and widget library Qt was used. The
3D scene and visual cues are rendered using OpenGL and
Nvidia’s Cg language for GPU programming. Model loading
was based on the Lischke’s 3DS import library [Lis05], and
the sky rendering was based on Sempé’s sky demo [Sem05].

5. Results
Archaeological data recorded for the Mah.ram Bilqīs sanctuary complex in Mārib, Republic of Yemen [Gla98, Gla99,
Gla02] has been used to illustrate the system. The most recent spatial data is of the main oval wall of the temple, provided by a recent survey taking accurate measurements. This
data represents a structure deteriorated by looting and time.
The earlier data is a theoretical reconstruction of the site at
an early date. These two data sets are compared using different visual cues in Figure 4. Interactive animation provided
by the time slider and time window allow smooth transitions
between the two data sets. This along with the uncertainty
visualization may allow the user to more easily understand
the assumptions in the earlier theoretical data set.

5.1. Uncertainty Tasks
While simply visually revealing the uncertainty (at the
Boolean level) can clearly be achieved it is not clear
what representations are most appropriate for specific tasks.
While some of the cues have a length above a Boolean indicator they may not be appropriate or may lead to confusion.
For the task of simply eliciting possibilities most of the cues
in Table 1 would work.
Amar and Stasko’s general Rationale-based Task category
of expose uncertainty requires both the presentation of the
uncertainty and showing the possible effect of the uncertainty on outcomes [AS04]. Uncertainty cues such as transparency and wireframe directly allow the possible effects on
outcomes to be seen, as the user can ignore the data and consider that it did not exist at that time. Once uncertainty is
c The Eurographics Association 2005.

Figure 4: Juxtaposition of theoretical reconstructions and
survey data. Top image: incompatible data with scene haze
(no data set uncertainty cues). Middle image: wireframe and
transparency uncertainty cue. Bottom image: transparency
uncertainty cue.

revealed simply providing interactive toggling of a data set
also affords this.
Kirschenbaum and Arruda found that for some spatial
problems a graphical representation of uncertainty may improve the judgements of decision makers [KA94]. We hypothesize that this would also apply to spatial decisions that
must account for temporal uncertainty. Future work could
determine the cognitive tasks and set of applicable visual
cues that could be used to test this hypothesis. For example,
assuming Cohen et al.’s cycle of metarecognition [CFW96]

104

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

was applicable, then the time window could provide visual
queries to aid in the testing of incomplete, conflicting, and
unreliable information.
5.2. Interactive Rendering Considerations
When the time slider is used to create an animation, on each
sequential frame the time window moves and so the temporal uncertainty may change for all data sets. The data for a
virtual reconstruction may be very large even before adding
multiple temporal versions. Therefore any procedural rendering method can reduce resource requirements by simply
modifying the single representation of each data set during
the rendering process. As interactive animation is required
using the graphics processing unit to its full potential is desirable.
The uncertainty visualization method categories of modify geometry, modify attributes, and animation [PWL97]
are highly suited for interactive graphics. Using graphics
processing unit (GPU) programs to perform procedural rendering, one can work with a single representation of the
scene and directly modify the visual appearance based on the
uncertainty metric (e.g. transparency can by changed without modifying the model attributes). The uncertainty value
assigned to each data set can also be used to determine when
a different GPU program is used (e.g. to provide a sketchlike quality).
5.3. Visual Cue Discussion
We have simulated an ancient Egyptian archaeological site
to more clearly demonstrate some visual cues for temporal
uncertainty. The site is shown with its associated data timelines in Figure 5. This site contains different dating for the
columns, sphinxes, and the main statue. Various visual cues
are illustrated for the specific viewing date of 1575 BCE and
a time window of 100 years (both the statue and sphinxes are
uncertain with this temporal configuration) in Figures 6 and
7.

Figure 5: Simulated archaeological reconstruction. Rendered with scene haze. No data set uncertainty visualization.

Figure 6: Uncertainty cues. From top to bottom: no cues,
rising/sinking cue, wireframe, and transparency.

Cues implemented using standard OpenGL are usually efficient but have limitations. To achieve correct transparency
effects with OpenGL (or any Z-Buffer depth sorting) one
must ensure that transparent data sets are rendered last and
in back to front order. While this can easily be done at the
object (artifact) level it is not usually interactively feasible
at the polygon/pixel level. Therefore basic OpenGL transparency is not guaranteed to provide accurate results with
complicated objects and scenes. The wireframe cue also has
its drawbacks as it may be misleading. Wireframe rendering reveals much of the underlying polygonalization and so
is dependent on manner in which the object was created. It
may be better to determine the silhouette and crease edges
of the objects in the data sets and only display those as lines.
To do this we could utilize techniques similar to those of
technical illustration presented by Gooch et al. [GSG∗ 99].
It may also be possible for the modeller to design objects so
c The Eurographics Association 2005.

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

105

time window, data from non-overlapping periods in history
can be spatially integrated with user selectable visual cues
revealing the uncertainty. The animated time window is intended to provide a new look at the progression of time at an
archaeological or cultural heritage site.
Visualizations of archaeological and cultural heritage sites
serve two distinct user groups: the general public, and domain experts. They can be useful to the general public in
providing comprehensible visual explanations and to domain
experts by allowing them to see their data. While NPR renderings may better serve the cognitive tasks such as hypothesis building [SMI99], some tasks may benefit from other
types of rendering that may illustrate an another person?s
conceptualization [RD03]. For example, at a museum a photorealistic rendering style may best help people conceptualize that an ancient site was a living community. Interactive
animation that can allow the user to select the type of rendering style provides the most flexibility.

Figure 7: Animated shading uncertainty cue (GPU program). Uncertainty controls the presence and frequency of
shadows. Higher uncertainty has higher frequency and so
the sphinxes are in and out of shadow more often than the
statue.

that they provide a suitable look when rendered in wireframe
mode.
Each visual cue will have its own benefits and drawbacks.
Visual cues that can be created using GPU programs benefit
from increased flexibility (they are not bound by the fixed
OpenGL rendering pipeline) and potentially faster performance. Those that are more intuitive will be more accessible
to the general public (e.g. transparency, fog). More complex
cues may requiring learning, but then may allow domain experts to encode multiple types of uncertainty. Determination
of which cues are the most appropriate to use will depend on
task and hardware considerations.
6. Conclusions
We have described a method and an application, ArkVis, that
provides an easier way to cognitively merge multiple data
sets that represent different periods in time. In ArkVis after
importing and entering minimal information a scene can be
navigated arbitrarily in time and space. By controlling the
c The Eurographics Association 2005.

Similar to problems observed with photo-realistic drawings used in preliminary drafts of architecture [SPR∗ 94], the
clean data sets provided for theoretical reconstructions often give the false impression of accuracy and completeness.
They may give a viewer the impression that this is exactly
how it did look, even though a large portion may be artistic
interpretation. Therefore we feel it is important to give the
same regard to temporal uncertainty as spatial uncertainty.
We hope that the visual differences revealed by controlled
blending and contrasting of data from different times, as well
as different sources, can provide new insights, thereby providing an improved understanding of the past.
7. Acknowledgements
We thank the University of Calgary Information Visualization class for providing many suggestions for potential visual
cues. William Glanzman supplied access to site data and information for the Mah.ram Bilqīs sanctuary complex. This
work has been supported in part by the Natural Science and
Engineering Research Council of Canada (NSERC) and Veritas DGC Inc.
References
[AS04] A MAR R., S TASKO J.: A knowledge task-based
framework for design and evaluation of information visualizations. In Proc. of the IEEE Symposium on Information Visualization (2004), pp. 143–149.
[AVLJ01] ACEVEDO D., VOTE E., L AIDLAW D. H.,
J OUKOWSKY M. S.: Archaeological data visualization
in VR: analysis of lamp finds at the Great Temple of Petra, a case study. In Proceedings of the Conference on
Visualization 2001 (VIS-01) (Piscataway, NJ, Oct. 21–26
2001), Ertl T., Joy K.„ Varshney A., (Eds.), IEEE Computer Society, pp. 493–496.

106

T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

[BBC∗ 04] BARACCHINI C., B ROGI A., C ALLIERI M.,
C APITANI L., C IGNONI P., FASANO A., M ONTANI C.,
N ENCI C., N OVELLO R. P., P INGI P., P ONCHIO F.,
S COPIGNO R.: Digital reconstruction of the Arrigo VII
funerary complex. In VAST 2004: Virtual Reality, Archaeology and Cultural Heritage (2004).
[BbWJB83] B ERTIN J., BY W ILLIAM J. B ERG T.: Semiology of Graphics. The University of Wisconsin Press,
WI, 1983.
[CFW96] C OHEN M. S., F REEMAN J. T., W OLF
S.: Metarecognition in time-stressed decision making:
Recognizing, critiquing, and correcting. Human Factors
38, 2 (1996), 206–219.
[Col93] C OLLINS B.: From Ruins to Reality - The Dresden Frauenkirche. IEEE Computer Graphics and Applications (Nov. 1993), 13–15.
[FMRS01] F REUDENBERG B., M ASUCH M., R ÖBER N.,
S TROTHOTTE T.: The Computer-Visualistik-Raum: veritable and inexpensive presentation of a virtual reconstruction. In VAST ’01: Virtual Reality, Archeology, and Cultural Heritage (2001), ACM Press, pp. 97–102.
[FS97] F ORTE M., S ILIOTTI A. (Eds.): Virtual Archaeology: Re-Creating Ancient Worlds. Harry N. Abrams,
1997.
[Gla98] G LANZMAN W. D.: Digging deeper: the results of the first season of activities of the AFSM on the
Mah.ram Bilqīs, Mārib. In Proceedings of the Seminar for
Arabian Studies (1998), vol. 28, pp. 89–104.
[Gla99] G LANZMAN W. D.: Clarifying the record: the
Bayt Awwām revisited. In Proceedings of the Seminar
for Arabian Studies (1999), vol. 29, pp. 73–88.
[Gla02] G LANZMAN W. D.: Some notions of sacred space
at the Mah.ram Bilqīs in Mārib. In Proceedings of the
Seminar for Arabian Studies (2002), vol. 32, pp. 187–201.
[GSG∗ 99] G OOCH B., S LOAN P.-P. J., G OOCH A.,
S HIRLEY P., R IESENFELD R.: Interactive technical illustration. In Proceedings of the Conference on the
1999 Symposium on interactive 3D Graphics (New York,
1999), Spencer S. N., (Ed.), ACM Press, pp. 31–38.
[KA94] K IRSCHENBAUM S. S., A RRUDA J. E.: Effects
of graphic and verbal probability information on command decision making. Human Factors 36, 3 (1994),
406–418.
[KMH01] KOSARA R., M IKSCH S., H AUSER H.: Semantic depth of field. In IEEE Symposium on Information
Visualization 2001 (InfoVis 2001) (San Diego, CA, USA,
22–23 2001).
[Lis05] L ISCHKE M.: 3DS import library, http://
www.lischke-online.de/3DS.php, 2005.
[Mac92] M AC E ACHREN A. M.: Visualizing uncertain information. Cartographic Perspective 13 (1992), 10–19.

[PBM93] PALAMIDESE P., B ETRO M., M UCCIOLI G.:
The virtual restoration of the Visir tomb. In Proceedings
of the Visualization ’93 Conference (San Jose, CA, Oct.
1993), Nielson G. M., Bergeron D., (Eds.), IEEE Computer Society Press, pp. 420–423.
[PFH95] P ETERSON P., F RACCHIA F. D., H AYDEN B.:
Integrating spatial data display with virtual reconstruction. IEEE Computer Graphics and Applications 15, 4
(July 1995), 40–46.
[PWL97] PANG A. T., W ITTENBRINK C. M., L ODHA
S. K.: Approaches to uncertainty visualization. The Visual Computer 13, 8 (1997), 370–390. ISSN 0178-2789.
[RB00] R ENFREW C., BAHN P.: Archaeology: Theories
Methods and Practice, 2nd ed. Thames & Hudson, 2000.
[RD03] ROUSSOU M., D RETTAKIS G.: Photorealism and
non-photorealism in virtual heritage representation. In
VAST 2003: Virtual Reality, Archaeology and Cultural
Heritage (2003), pp. 51–60.
[Sem05] S EMPÉ L. R.: OpenGL SkyDomes, http://
www.spheregames.com/articles.asp, 2005.
[SMI99] S TROTHOTTE T., M ASUCH M., I SENBERG T.:
Visualizing Knowledge about Virtual Reconstructions of
Ancient Architecture. In Proceedings Computer Graphics International (1999), The Computer Graphics Society,
IEEE Computer Society, pp. 36–43.
[SPM∗ 99] S TROTHOTTE T., P UHLE M., M ASUCH M.,
F REUDENBERG B., K REIKER S., L UDOWICI B.: Visualizing Uncertainty in Virtual Reconstructions. In Proceedings of Electronic Imaging & the Visual Arts, EVA Europe
’99 (Berlin, 1999), VASARI, GFaI, p. 16.
[SPR∗ 94]

S TROTHOTTE T., P REIM B., R AAB A., S CHU J., F ORSEY D. R.: How to render frames and influence people. Computer Graphics Forum 13, 3 (1994),
455–466.
MANN

Non[SS02] S TROTHOTTE T., S CHLECHTWEG S.:
Photorealistic Computer Graphics: Modeling, Rendering,
and Animation. Morgan Kaufmann, San Francisco, 2002.
[vFL∗ 00] VAN DAM A., F ORSBERG A. S., L AIDLAW
D. H., L AV IOLA , J R . J. J., S IMPSON R. M.: Immersive VR for scientific visualization: A progress report.
IEEE Computer Graphics and Applications 20, 6 (Nov./
Dec. 2000), 26–52.
[VPW∗ 04] V ERGAUWEN M., P LETINCKX D., W ILLEMS
G., V ERBIEST F., G OOL L. V., H ELSEN T.: As time flies
by: Mixed image and model-based rendering of an historical landscape from helicopter images. In VAST 2004: Virtual Reality, Archaeology and Cultural Heritage (2004),
pp. 241–250.
[War04] WARE C.: Information Visualization: Perception
for Design, 2nd ed. Morgan Kaufmann Publishers, 2004.

c The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Investigating the Structural Validity of Virtual
Reconstructions of Prehistoric Maltese Temples
A. Chalmers† and K. Debattista
University of Bristol, United Kingdom

Abstract
The prehistoric temples found on the Maltese islands, dated from 3600 -2500 BC, are unique examples of truly megalithic complexes. Although the temples can still be viewed today, they are unroofed. One of the major questions
that still has to be answered is: Were the temples roofed, and if so with what? The key evidence for the presence of
roofs is the hypogeum temple at Hal Saflieni, found in 1902, which appears to be an imitation of the above ground
temples and the discovery, at Mgarr, of a contemporary miniature model in limestone with a roof. Since then,
Ceschi in 1939 and more recently Piovanelli in 1988 have proposed that the temples were roofed with Globerigerina limestone slabs. Although convincingly illustrated, neither of these “reconstructions" has been tested for real
stability and strength. In this paper we describe a detailed investigation of the reconstructions of Ceschi and Piovanelli. We use computer graphics and structural engineering techniques, based on the actual measured strength
of Globerigerina limestone, to show whether in fact these reconstructions are indeed valid.
Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
Graphics and Realism - Color, Shading, Shadowing, and Textures

1. Introduction

Figure 1: Tarxien temple as it appears today.
The prehistoric temples of Malta are some of the ear† Alan.Chalmers@bristol.ac.uk
c The Eurographics Association 2005.

liest examples of free standing, megalithic monuments in
the world. Built throughout the period 3600-2500 BC, the
temples are scattered over the Maltese islands. They all
have common features, including a perimeter wall, often of megalithic construction, surrounding an inner structure of "apses" with an entrance portal and interconnection passages [Cla98]. They were built with a combination
of Coralline limestone and Globigerina limestone which is
much more easily dressable. Earlier temples were simple
lobed ones, but later they comprised trefoil and more apses
up to a six apse one. All the temples are currently unroofed
and one of the important questions still asked by archaeologists is: Were they indeed roofed, and if so, by what?
In addition to the above ground temples, there were two
hypogea constructed in parallel. These underground chambers were used for burial of the dead. The hypogeum at Hal
Saflieni is carved directly out of the Globigerina limestone
and mimics many of the features of the above ground temples. As discussed in section 3, this is often used as evidence
for the form of the temple roofing [Cla98].
In this paper we analyse, by means of civil engineering

108

A. Chalmers & K. Debattista / Investigating the Structural Validity of Virtual Reconstructions of Prehistoric Maltese Temples

techniques, two of the popular roof reconstructions theories, by Ceschi [Ces39] and Piovanelli [Pio88], to determine
whether they are indeed feasible.

Figure 2: Miniature temple found at Mggar.
Figure 4: Ceschi’s
apses [Ces39].

reconstruction

of

the

Tarxien

2. Valid Virtual Archeology
Archaeological sites have been studied and recorded for hundreds of years, from medieval drawings of the sites, to the
systematic illustration of the 18th century, to photographs of
the early 20th century and finally computer graphics from
the 1980s onwards [CD02, MR94, Nov98, RS89].
Computer graphics is now regularly providing powerful
tools for modelling multi-dimensional aspects of data gathered by archaeologists [BFS00]. In recent years, techniques
have been developed which can be used to reconstruct and
visualize features of sites which may otherwise be difficult to
appreciate. While these new perspectives may enhance our
understanding of the environments in which our ancestors
lived, if we are to avoid misleading impressions of a site,
then the computer generated images should not only look
“real", but must simulate accurately all the physical evidence
for the site being modelled [DC01, Mar01].
High-fidelity reconstructions of archaeological cites has
typically focussed on the use of laser scanners or photogrammetry for creating accurate models, for example [Add01,
BM02, GG03], or the authentic illumination of the models,
for example [DC01, CR03, SCM04].

Figure 3: The hypogeum at Hal Saflieni [Bon91].

3. Roofing the Temples
The major debate about the roofing of the temples is whether
they were roofed by stone beams laid over the walls of the
c The Eurographics Association 2005.

A. Chalmers & K. Debattista / Investigating the Structural Validity of Virtual Reconstructions of Prehistoric Maltese Temples

109

4. Structural Analysis
The stability of the proposed reconstructions depends on the
arrangement of the constituent blocks of stone. Each block
needs to remain in position under the combined action of its
own weight and of the forces which are being applied to it
by adjoining blocks [Gor78].
The shape, size and type of the building material will affect the way in which the materials and structure will deflect.
If the force applied to a constituent block is too great for the
strength of the material then it will break. In addition, the
forces present in the structure must be in equilibrium. That
is, every force must be balanced and reacted by another equal
and opposite force at every point throughout the structure, If
this is not the case then the structure will fall.

Figure 5: PiovanelliŠs reconstruction of the Tarxien
apses [Pio88].

apses, the so-called Italian view, or by timber beams overlaid with bushes, the British view [TVC87]. Evidence used
to counter the British view is the current lack of suitable
wood on Malta and the lack of any supporting evidence in
the form of trade goods with neighbours who could have
provided the wood, for example Sicily. Of course in prehistoric Malta there could indeed have been a sufficient supply
of suitable wood [Eva59]. In this paper we only consider the
structural feasibility of the Italian view.
Ceschi in [Ces39] compared the Maltese temples with
similar structures in Sardinia, Pantelleria and the Balearics.
One of the key pieces of evidence he used to support the
theory of stone slab roofs is the small limestone model of
a temple excavated at Mggar, figure 2. As can be seen, this
miniature temple is roofed with horizontal slab roofing at
right angles to the long axis.
Furthermore, as figure 3 shows, the underground hypogeum at Hal Saflieni exhibits very similar architectural
features to the above ground temples, and its roof is carved
in the shape of overlapping stone slabs. Figure 4 shows the
reconstruction of the roofing at Tarxien of two apses that
Ceschi proposes.
Although Piovanelli [Pio88] had his doubts about the
massive nature of Ceschi’s reconstructions, he also supported the stone roofing theory. Using Hal Saflieni as his
example, he suggested that concentric rings of stone were
placed on the inward sloping walls of the apses. Such a
construct progressively reduces the space to be covered and
closely matches the form of the hypogeum. Piovanelli’s proposed reconstruction is shown in figure 5.
c The Eurographics Association 2005.

Figure 6: Determining the thrust lines.

4.1. Stability
The stability of a structure can be evaluated by translating Newton’s laws of equilibrium to the components of the
structure. For example, if block B is positioned on top of
block A then block B will balance on top of block A provided the centre of gravity of block B acts down between the
areas of contact between the two blocks. If block B is moved
outside this bound of block A then B will unbalance and fall
over. If it is desired that B’s centre of gravity is outside the
bounds of block A then either another block must be placed

110

A. Chalmers & K. Debattista / Investigating the Structural Validity of Virtual Reconstructions of Prehistoric Maltese Temples

on top of B to counter balance it, or a block can be placed
under B.
In the former case, a thrust line approach may be used
to determine the stability of the structure. A thrust line is a
position from which a given mass acts down. This is equal to
the centre of gravity of a single block. For a stack of blocks,
the centroid and each block’s mass comprise the thrust line
acting on the blocks below. If this thrust line moves outside
the bounds of the structure then it will fall. For example,
as shown in figure 6, the stability of the structure can be
tested by considering the highest block first. If block D’s
thrust line is outside the bounds of block C then the structure
is unstable. If it is within the bounds of C then the stability
of C and D on B can be tested. A new thrust line for C is
computed which is the combination of the thrust lines for C
and D. If this combined thrust line is within the bounds of
block B then the combination of C and D on B is stable. If
not, then the combination of C and D on B is unstable.
The centroid of a three dimensional composite mass of
two blocks with centroids C1= (x1, y1, z1) and C2 = (x2,
y2, z2) and masses M1 and M2 can be calculated

x0 = x1M1 + x2M2/M1 + M2
y0 = y1M1 + y2M2/M1 + M2
z0 = z1M1 + z2M2/M1 + M2
In the case where a block is supported by two (or more)
other blocks placed underneath it, then the weight of the supported block will, of course, act down through both supporting blocks. The weight acting on each support block is proportional to its distance from the thrust line of the supported
block and is calculated, as shown in figure 7 as:

Weight acting on B = WAb/a + b
Weight acting on C = WAa/a + b

4.2. Spanning
Having established the stability of a spanning block, it is
equally important to ensure that the block is indeed strong
enough not to crack under its own weight or the weight of
any blocks above it. The spanning ability of a beam depends
on the tensile strength of the material.

Sel f weight per metre length =
width(w) ∗ height(h) ∗ density(d)

also,
Section Modulus Z
Moment M

=
=

(w ∗ h2 )/6
Z ∗ tensilestrength(s)

The spanning distance (l) can now be calculated as:

l 2 = (8 ∗ s ∗ Z)/Sel f weight
As described in section 4.3, the tensile strength of Globigerina limestone was found to be 4.6N/mm2 and density
1725kg/m3 . So, for example, for a regular block of Globigerina of height and width 0.5m.
Sel f weight per metre = 0.5 ∗ 0.5 ∗ 1725 = 4.31kN/m
Z = (500 ∗ 5002)/6 = 20833333mm3
So l = 13.34m. This means that the maximum distance a
block of Globigerina limestone with dimensions 0.5 ∗ 0.5m
can span is 13.34m. Any further than this and the block will
break under its own weight.
This maximum distance is, of course, shortened if the
block is carrying any other blocks. This reduced length can
be calculated by determining the additional weight acting on
the block through the thrust line.

4.3. analysing Globigerina limestone

Figure 7: Calculating the weight acting down on a block.

Samples were collected of Globigerina during a visit to
Malta. From these three small beams of 256 × 35 × 35mm
were cut from one sample, and fourth similar size beam from
a second sample. These beams were subjected to a tensile
test by the Civil Engineering Department of the University
of Bristol to determine their flexural strength, or Modulus of
Rupture. An increasing load was applied to cubes of Globigerina through steel plates until the cubes failed. The results obtained are [Por98]:
c The Eurographics Association 2005.

A. Chalmers & K. Debattista / Investigating the Structural Validity of Virtual Reconstructions of Prehistoric Maltese Temples

Table 1: Results of Globigerina analysis
Cube
Number
1A
1B/1
1B/2
2A

Cube
mm
35
35
35
35

Area
mm2
1.225
1.225
1.225
1.225

Density
kg/m3
1.725
1.725
1.725
1.725

Load
kN/mm2
15.3
17.4
15.6
20.9

Strength
N/mm2
12.5
14.2
12.7
17.1

111

that of Pavionelli, although the front section is stable, when
the entire apse is considered the structure would also fail.
We will never know how the temples were roofed. A system, such as we have developed, can be used as one of many
tools by the archaeologists to enable them to explore at least
structurally feasible possibilities of reconstructions.
To provide a more general system for analysing archaeological site reconstructions, a complete database of the tensile and compressive strengths of possible building materials
needs to be collected. We intend to continue collecting this
information in conjunction with archaeologists and civil engineers.

Figure 8: Structural analysis of CeschiŠs proposed reconstruction.

5. Results
The structural analysis techniques were implemented as a
plug-in to the modelling package Maya from Alias. This enabled the reconstructions to be simply modelled using Maya
and then tested for stability and spanning ability. Any part
of a reconstruction which is not structurally sound is highlighted by colouring the unstable blocks white.

Figure 9: Thrusts lines for the PiovanelliŠs reconstruction.

Figure 8 shows the model of the reconstruction proposed
by Ceschi after it has been analysed by our system. As can
be seen, the structure is unstable and thus the reconstruction
infeasible.
The calculated thrusts lines for Piovanelli’s reconstruction
are shown in figure 9. All the thrusts lines are within the
bounds of the structure and thus this part of the reconstruction is structurally sound. Figure 10, shows the analysis of
the reconstruction of the entire apse. Here one of the blocks
now becomes unstable.
6. Conclusion
The roofing of the prehistoric Maltese temples remains
one of the key unanswered questions about these unique
sites. Computer graphics together with structural engineering techniques and a detailed analysis of the building materials can be used to validate at least the structural feasibility
of any reconstruction. As our results have shown, the reconstruction proposed by Ceschi in 1939 is not stable, while for
c The Eurographics Association 2005.

Figure 10: Analysis of the complete apse reconstruction
showing the instability of the structure.

7. Acknowledgements
We would like to thank Daniel Clark and Rowland Morgan
who helped collect the samples in Malta, Sarah Porter for the
preliminary implementation of the system and Joseph Cordina for the picture in Figure 1. The work reported in this

112

A. Chalmers & K. Debattista / Investigating the Structural Validity of Virtual Reconstructions of Prehistoric Maltese Temples

paper has formed part of the Rendering on Demand (RoD)
project within the 3C Research programme whose funding
and support is gratefully acknowledged.
References
[Add01] A DDISON A. C.: Virtual heritage: technology in
the service of culture. In VAST ’01: Proceedings of the
2001 conference on Virtual reality, archeology, and cultural heritage (New York, NY, USA, 2001), ACM Press,
pp. 343–354.
[BFS00] BARCELO J., F ORTE M., S ANDERS D.: Virtual
Reality in Archaeology. ArchaeoPress, 2000.
[BM02] B OEHLER W., M ARBS A.: 3d scanning and photogrammetry for heritage recording: A comparison. In
12th Int. Conf. on Geoinformatics - Geospatial Information Research: Bridging the Pacific and Atlantic University of Gävle (2002).
[Bon91] B ONANNO A.: An archaeological paradise. MJ
Publications, 1991.

[Nov98] N OVITSKI B.: Reconstructing lost architecture.
Computer Graphics World (1998), 24–30.
[Pio88]

P IOVANELLI G.: Missione a Malta. 1988.

[Por98] P ORTER S.: Civil engineering constraints for prehistoric site reconstructions. Final year project Thesis,
University of Bristol, 1998.
[RS89] R EILLY P., S HENNAN S.: Applying solid modelling and animated three-dimensional graphics in archaeological problems. Tech. rep., IBM Uk Scientific
Centre, 1989.
[SCM04] S UNDSTEDT V., C HALMERS A., M ARTINEZ
P.: High fidelity reconstruction of the ancient egyptian
temple of kalabsha. In AFRIGRAPH 2004 (November
2004), ACM SIGGRAPH.
[TVC87] TAMPONE G., VANNUCCI S., C ASSAR J.:
Nuovo ipostesi sull’architettura del tempio megalitco di
ggantija a gozo. Bollittino Inegheir (1987), 3–20.

[CD02] C HALMERS A., D EVLIN K.: Recreating the Past.
ACM SIGGRAPH, July 2002.
[Ces39] C ESCHI C.: Architettura dei Templi Megalitici di
Malta. Fratelli Palombi, 1939.
[Cla98] C LARK D.: Insular monument building: A cause
of social stress? The case of prehistoric Malta. PhD thesis, University of Bristol, 1998.
[CR03] C HALMERS A., ROUSSOS I.: High fidelity lighting of knossos. In VAST ’03: Proceedings of the 2003 conference on Virtual reality, archeology, and cultural heritage (New York, NY, USA, 2003), ACM Press, pp. 47–
56.
[DC01] D EVLIN K., C HALMERS A.: Realistic visualisation of the pompeii frescoes. In AFRIGRAPH 2001
(November 2001), Chalmers A., Lalioti V., (Eds.), ACM
SIGGRAPH, pp. 43–47.
[Eva59]

E VANS J.: Malta. Praeger, 1959.

[GG03] G. G ODIN F. B LAIS L. C. J. B. J. D. J. T. M.
R. S. E.-H.: Laser range imaging in archaeology: issues
and results. In cvprw vol. 01, p. 11 (2003).
[Gor78] G ORDON J.: Structures or why things don’t fall
down. Penguin, 1978.
[Mar01] M ARTINEZ P.: Digital realities and archaeology:
a difficult relationship or a fruitful marriage? In VAST
’01: Proceedings of the 2001 conference on Virtual reality, archeology, and cultural heritage (New York, NY,
USA, 2001), ACM Press, pp. 9–16.
[MR94] M ILLER P., R ICHARDS J.: The good, the bad,
and the downright misleading: Archaeological adoption
of computer visualization, computer applications in archaeology. British Archaeological Reports (Int. Series,
600) (1994), 19–22.
c The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Exploring Digitized Artworks by Pointing Posture
Recognition
C. Malerczyk, P. Dähne and M. Schnaider
ZGDV e.V. Computer Graphics Center, Darmstadt, Germany

Abstract
This paper describes a Mixed Reality-supported interaction system to explore digitized artworks like twodimensional paintings and three-dimensional sculptures. Using an easy and intuitive pointing gesture recognition
system, the user is able to interact directly with the artworks, which leads to a deeper involvement with and understanding of the art pieces. The usage of a video-based gesture tracking system ensures seamless integration of
Mixed Reality technologies into a traditional museum’s environment. Furthermore, it addresses even technically
unversed users since no additional physical devices are needed and even no training phase is necessary for the
interaction.
Categories and Subject Descriptors (according to ACM CCS): I.3.6 [Computer Graphics]: Interaction techniques
H.5.2 [Information Interfaces and Presentation]: Interaction styles

1. Introduction
Cultural organizations such as museums and galleries face
the situation of competition for the favor of visitors or the
public in general with numerous suppliers of different educational or entertaining offers. As a consequence, they are
looking for novel ways of attractively present their cultural
assets, helping them to increase demand for cultural related
edutainment. Interactivity here is one of the key factors to attract visitors. Hence information technology in general and
in recent years Mixed Reality technology in particular received attention as these promise a wide range of interaction potentials. But anyhow, just applying technology is not
enough to reach full success. Bob Raiselis, an exhibit developer, summarized in his article ’What makes a good Interactive Exhibit’ [Rai05] some key characteristics that should be
considered when trying to build interactive exhibits, like
• Be inviting, i.e. the exhibit raises the interest of visitors
and invite them to spend some time with it;
• Be understandable, i.e. the navigation scheme of the exhibit should be instantly understandable and the visitor
should be able to get the exhibit to ’work’;
• Be explorative, i.e. invite visitors to explore the exhibit
and let them discover things by themselves;
• Be accessible to people of varying ages and development.
c The Eurographics Association 2005.

In this paper we propose a number of interactive scenarios
matching the accustomed association of artworks in a museum such as the presentation of paintings on a canvas or the
presentation of three-dimensional sculptures with innovative
human-computer-interaction methods. The combination of
intuitive interaction techniques and the presentation of multimedia content on a large screen resolution system is used
to generate a novel experience during an exhibition visit. Interacting with virtual exhibits should instantly increase the
level of interest of the user and thus the impact of quality of
education through hands-on experiences [MS03].
We describe the idea of a Mixed Reality-supported interaction system for museums and large galleries and its prototypical installation. The visitor is able to create his/her own
exhibit and can choose between different digitized paintings
and three-dimensional sculptures for exploration. Once selected, he/she can interact with the selected artwork in an
easy and intuitive way just by pointing at the interaction canvas. The display of digitized paintings and sculptures on an
interactive screen is usable for museums that are too limited in space to present all their art pieces in a traditional
way. Furthermore, the direct interaction with art pieces typically leads to a deeper involvement with and understanding
of the art pieces, whereas the manipulation of original paintings and sculptures is obviously prohibited. Exploration of

114

C. Malerczyk, P. Dähne and M. Schnaider / Exploring Digitized Artworks by Pointing Posture Recognition

the paintings is e.g. achieved by giving the user the possibility of looking at details of the paintings, which he/she normally only can see using tools like a magnifying glass or by
helping the to obtain additional information about the painting, which normally have to be read in the museums guide.
Due to the fact that all visitors of the museum should be able
to use the system, the input device has to be as easy and
intuitive as possible. We address this issue using an innovative video-based hand pointing recognition system as the
input device. The usage of a video-based tracking system
addresses even technically unversed users since no cumbersome physical devices need to be used and no training phase
is necessary to perform interaction.
Rendering software and image processing algorithms are
used for data management, which enables displaying and
manipulating high-resolution 2D images of digitized paintings and 3D scanned sculptures in real time.
2. Technichal Setup
The equipment for the interactive museum exhibit consists
of one single standard PC, which is used for both rendering
of the scenario application and pointing gesture recognition
and tracking. A standard video beamer or a large plasma display is connected to the PC, displaying high-resolution images, virtual additions to the image or control objects like
sliding menu frames on the canvas in front of the exhibit visitor. For tracking purposes two Firewire (IEEE1394) cameras
are connected to the computer feeding the system with greyscaled images of the interaction volume in real time. It is
possible to use special lenses with infrared diodes to flood
the scene with additional infrared light to ensure safer light
conditions and to enhance the robustness of the video-based
tracking system.

as a technical device like e.g. a laser pointer, it is important
to permanently provide the user of the system with a visual
feedback for comprehensible perception of his/her interaction instead of calculating the pointing direction as precise
as possible.
The position of the user is somewhat pre-defined with respect to the camera set-up. Inducing the user to take the correct position can easily be achieved by adding markers like
footsteps or other indicators on the floor in front of the interaction canvas. The actual position of the cameras with respect to the user and the displaying canvas however depends
on various parameters like
• Focal length of the camera lenses;
• Dimension of the rendering canvas; and
• Designated speed and accuracy of the tracking system.
The above-mentioned distances of the cameras are proposed values for a canvas of approximately 2.0m width and
1.5m height and a user position located at about three meters
in front of the canvas.
The project hardware used for the demonstrator consists of
off-the-shelf components. The system is running on a standard PC. The current configuration features a Pentiumő 4
processor with 3.2 GHz, 1GB of memory and a NVIDIA
GeForce 6800LE graphics board. The software runs under
MS Windows XP.
As output device any standard video beamer with a resolution of 1024 * 768 pixel or higher and a projection canvas of approximately 2.0m width and 1.5m height can be
used. The current set-up of the implemented prototype uses
a back projecting system by OTLO VR Systeme GmbH, Rostock, Germany (http://www.otlo.de/) with a Liesegang
ddv 1800 beamer. In addition, the system is tested with a
large scale monitor (NEC 61” Plasma Display Monitor PX61M1A, http://www.nec.com/) with a resolution of 1360
* 768 pixels. As no hardware interaction devices need to be
connected to a computer, the complete technical set-up can
be concealed from the user. The display canvas, the user is
interacting with, is the only piece of technical equipment visible to the user.
3. Tracking Software

Figure 1: Camera and (optional) infrared light beamer.
Equipped with 4mm lenses the cameras are mounted at
the ceiling at approximately 2.5 meters from ground with a
distance of 2-3 meters from left to right.
Since the human pointing posture is naturally not as precise

The purpose of the tracking module is to recognize and to
track a static pointing gesture of the user to enable intuitive
interaction with the scenario application. The definition of
an abstract gesture description allows the tracking system
to recognize individual pointing gestures without any learning procedure. The video-based module uses two cameras,
which observe the user in front of the display canvas identifies if the user is pointing at the canvas and extracts the pointing direction. Due to a background communication with the
rendering module the user gets a direct visual feedback on
the canvas in real time.
The tracking module is separated into two different operation modes:
c The Eurographics Association 2005.

C. Malerczyk, P. Dähne and M. Schnaider / Exploring Digitized Artworks by Pointing Posture Recognition

115

Figure 2: Sketch of the virtual interaction area: pointing position (dot) and different button regions (boxes) defined.

• Tracking of the pointing direction and its target point at
the canvas and
• Observation of predefined regions that can be used like
virtual buttons (pointing for a period of ∼ 21 second at a
button object leads to a ’button selected’-event handled
by the scenario application)
Both modes (pointing direction and region selection) are
usable in parallel.
Hand gesture recognition in computer vision is an extensive area of research that encompasses anything from static
pose estimation of the human hand to dynamic movements
such as the recognition of sign languages [Koh05]. The demands on the tracking software used for this application
arise from the scenario itself. In a public place such as a museum, a wide range of different visitors are expected to use
the system. Therefore, it is necessary to have a tracking system at hand that is able to handle the interaction of different
users, no matter if they are left- or right-handed, if they use
just the index finger for pointing or even the opened hand.
Furthermore, it is obvious that the tracking system needs to
be usable without a visitor specific training phase. A museum visitor should instantly be able to interact with the exhibit without reading operating instructions first.
A combination of different basic computer-vision and image processing algorithms ensures a fast and robust identification of an eventually existing pointing gesture [SHB98].
The approach is based on the recognition of the human fingertip within a calibrated stereo system. Therefore, position
and orientation of the cameras are determined with respect
to the world coordinate system by swaying a small torch
light for a few seconds in the designated interaction volume [SM02]. This calibration procedure has to be performed
only once after setting up the cameras.
During runtime of the system, difference images are used to
detect moving objects, which then are analyzed and the probability of a pointing posture and its direction in 3D space is
c The Eurographics Association 2005.

Figure 3: Screenshot of pointing tracking software showing
two (superimposed) camera images.

calculated (see Figure 4). Intersecting the pointing ray with a
virtual and normalized representation of the display canvas
triggers the respective visual feedback or selection events.
Smoothing of the tracking results using smoothing splines to
reduce jittering effects [SE00] leads to an immersing experience during the interaction without the need of any technical
device. The tracking system is nearly self-calibrating. Only
a few parameters like the dimensions of the regions of interest in the images and a segmentation threshold have to be
set or adapted during and after the installation of the system.
Furthermore a simple graphical interface ensures the easiest
handling of the tracking application.
Due to the separation of the tracking module and the scenario application, it is easy for the support staff of the museum to change or replace the content on the scenario side of
the virtual exhibit without any need of changing parameters
in the tracking software.
4. Scenario Applications
The goal of the scenario application is to create an intuitively
usable experience for any museum visitor, who is curious
enough to explore digitized paintings on a technical exhibit
canvas with a new interaction paradigm like the pointing
recognition system.

116

C. Malerczyk, P. Dähne and M. Schnaider / Exploring Digitized Artworks by Pointing Posture Recognition

the low-level interfaces often provided by traditional VR
toolkits.
• There are a great number of books and tutorials available.
5. Paintings Exploration Scenarios

Figure 4: Original camera image (top) and edge-ofdifference image (bottom), both superimposed.

One of the most obvious applications for the presentation of
and interaction with artworks in a museum is the exploration
of two-dimensional digitized masterpieces. In museums it is
often strictly forbidden to approach the original canvases.
Therefore, a lot of visitors are in the dilemma that nonetheless they would like to look at details of the paintings. This
problem could be solved by using the interaction system,
where the visitor is able to explore a digitized copy of the
masterpiece having e.g. a virtual magnifying glass or a virtual pocket lamp at his/her fingertip.
Furthermore, additional information about the original art
pieces often has to be retrieved from books like printed museum guides or by using audio guides. We propose to use the
interaction system to provide background information about
the painting itself, the artist or even other paintings of that
day directly by hands-on experience.
There is a large number of possible and useful applications
how to interact with digitized paintings. The following sections describe three different applications we implemeted for
demonstration and testing purposes.
5.1. Rousseau Scenario

For the creation of new content for the exhibition it is important to have standardized and easy to use authoring tools
and rendering components at hand. We use Avalon [Ava05]
[BF98] [BDR04] for the rendering part of the interactive museum exhibit. Avalon is an open environment for VR applications developed at the Computer Graphics Center (ZGDV)
in Darmstadt, Germany. Avalon uses VRML/X3D as the
programming language for the virtual worlds the user interacts with. Like most traditional toolkits, Avalon uses a
scene-graph to organize the data, as well as spatial and logical relations. In addition to the scene description, VR applications like the interactive museum exhibit need to deal
with dynamic behavior of objects, and the user interaction
via non-standard input devices. The use of VRML/X3D as
an application programming language leads to a number of
advantages over a proprietary language [BDR04]:
• It is integral to an efficient development cycle to have access to simple yet powerful scene development tools. With
VRML97/X3D, the application developer can use a wide
range of systems for modeling, optimizing and converting.
• The interface is well defined by a company-independent
ISO standard.
• Due to platform independence, development and testing
can even be done on regular standard desktop computers.
• VRML and JavaScript are much easier to learn than

As a first proof-of-concept of the interactive museum exhibit, we developed a simple application for the exploration
of three different paintings of the French post-impressionist
painter Henri Rousseau (1844-1910). The visitor of the museum exhibit is invited to take position in front of the canvas
indicated by footstep markers on the floor indicating the designated interaction position. The application directly starts

Figure 5: Image selection menu with permanent visual feedback.
with the full screen exploration of one of the three images.
c The Eurographics Association 2005.

C. Malerczyk, P. Dähne and M. Schnaider / Exploring Digitized Artworks by Pointing Posture Recognition

At the right hand side border of the image a virtual button is
displayed indicating an image selection menu. Pointing for
at least 21 second at this button will activate the menu, which
smoothly slides into the canvas and displays the selectable
images (see Figure 5). During the interaction with an activated menu a small red point (understandable as a laser
pointer metaphor) provides direct visual feedback during the
selection phase. After the selection of a new image, the menu
smoothly disappears and allows the exploration of the selected painting. At the left hand side border of the image
another virtual button provides the exploration tools menu.
Here, the visitor of the exhibition is currently able to switch
between two different interaction modes, using the pointing
posture as a virtual pocket lamp or as a virtual magnifying
glass.
The spot light-based exploration, activated by selecting the
according button of the tool selection menu, helps the visitor
to focus on interesting parts and to blind out currently uninteresting parts of the image. If pointing does not take place
the canvas is left black. Only the virtual button objects at the
left and right hand edges of the canvas are visible. Pointing
at the canvas leads to the effect of having a virtual pocket
lamp at the fingertips of the user.
The magnifying glass-based exploration, activated by selecting the according button of the tool selection menu, allows
the user to focus on interesting parts of the currently displayed image and to zoom in on details at which the user
points. With the virtual magnifying glass at hand the user is
able to let the lens slide over the image.

Figure 6: Exploring Hieronymus Bosch’s ”The Haywain”
triptych with a virtual magnifying glass.

117

painter Hieronymus Bosch (c. 1450-1516), which is originally located at the Prado Museum in Madrid, Spain. Paintings of Hieronymus Bosch perfectly fit for an exploration
using a virtual magnifying glass since Bosch is well known
for his complex painted panels featuring fantastic and very
detailed portrayals of demons, fools and other creatures from
Eden to hell. The application directly starts with the full
screen exploration of the painting. While no menu bars or
other objects disturb the visual impression of the digitized
painting, the visitor is able to focus solely on the painting
and its details. Due to the panoramic aspect ratio of the original triptych we use a large 16:9 plasma display monitor with
a diagonal screen size of 61" for the presentation. An additional post processing step in the pointing gesture tracking
module allows an extremely stable position of the magnifying glass, if the user is bringing an interesting detail into
focus.

Figure 7: Guardi (1712-1793), The Marcus place with the
clock tower, visible image (top) and x-ray image (bottom),
Courtesy of the Picture Gallery of the Academy of Fine Arts
Vienna.

5.2. Hieronymus Bosch Scenario

5.3. Guardi Scenario

The second scenario application exclusively addresses the
exploration of a digitized painting using a virtual magnifying glass. For this scenario we have chosen the well known
triptych ”The Haywain” (see Figure 6) by the Netherlandish

Another scenario application allows the exploration of a
single painting of Francesco Guardi with a virtual x-ray
beam. The painting of the Italian Rococo Era painter shows
the Marcus place in Venice, Italy (see Figure 7, top). The

c The Eurographics Association 2005.

118

C. Malerczyk, P. Dähne and M. Schnaider / Exploring Digitized Artworks by Pointing Posture Recognition

original masterpiece is located at the picture gallery of the
Academy of Fine Arts Vienna in Austria. X-ray photographs
of the painting brought to light that Guardi used an already painted canvas [Sch03]. The x-ray photograph shows
a painting of a manger scene of an unknown artist (see Figure 7, bottom), where several details of the original painting
like the face of the Madonna, Jesus hold by his mother, two
of the three Magi or even the head of the donkey are clearly
visible.
Instead of showing the x-ray image on a separated canvas beside the original painting and using an information board or
the museum guidebook for explanations, we use the pointing gesture tracking for an interactive exploration of both
layers of the canvas. Without interaction, only the original
painting of the Marcus place is visible. Pointing at the canvas enables a virtual x-ray view on the manger scene behind
(see Figure 8). In addition to the visual feedback during the
exploration, several regions of interest are predefined. If the
visitor finds an interesting detail in the x-ray layer (indicated
by pointing for at least 21 second at the virtual region), a
voice gives further information on this detail. The voice can
either be a pre-recorded voice of a speaker, but it can also
be synthesized during runtime of the system using a speech
synthesis module generating speech from a given information text. The later allows easy creation or variation of the
speech annotations incorporated in the scenario.

are not allowed to be approached or even to be touched by
the visitors.
Unfortunately, the presentation of three-dimensional content
is not as easy as the presentation of digitzed paintings. That
applies to the creation of suitable content as well as to the
interaction with the virtual world itself. The major problem
of the creation of 3D content is the fact that the original masterpiece has to be scanned to achieve a high resolution model
of the object, which needs a much higher technical effort to
spend than to take just one digital picture of a painting.
Furthermore, the interaction with a 3D virtual world is not as
intuitive as just pointing at a specific position on the screen.
While the museum visitor is able to look upon a real 3d
sculpture from all sides, he/she has to rotate the virtual copy
of the object in 3D space. There are two major possibilties
how to interpret the pointing posture as an input device for
the rotation of a three-dimensional object:
• The position on the displaying canvas the user is pointing
at indicates directly the direction of the rotation of the object. In practice, it is neccessary to define an empty area in
the center of the screen where no interaction takes place
to avoid permanent rotation, whenever the user is pointing
at the canvas.
• The virtual world is enhanced by additional virtual buttons for the rotation of the 3D-object. Whenever the user
is pointing at one of these button, the object rotates left,
right, up or down with a predefined rotation speed.
As a first application dealing with the presentation of threedimensional objects we have chosen the exploration of a bust
of the Greek mythological creature Medusa. The user is able
to look upon the bust from every angle by rotating it using
four virtual buttons arranged at the right hand side and the
bottom of the screen (see Figure 9). A small red cursor ensures permanent visual feedback during the interaction.

Figure 8: Image exploration with virtual x-ray functionality.

6. 3D Object Exploration Scenario
As a consequence of using an open X3D/VRML environment for immersive applications for the rendering purposes
the next logical step is to build applications beyond the limitations of interacting with two-dimensional paintings and to
enhance the system by the possibilities of exploring threedimensional objects like statues, sculptures or other artefacts. The argumentation why to use digitized copies of the
real artwork for exploration is the same as mentioned for
two-dimensional paintings: The original 3D masterpieces

Figure 9: Interactive rotation of the Medusa bust, Courtesy
of the Picture Gallery of the Academy of Fine Arts Vienna.

c The Eurographics Association 2005.

C. Malerczyk, P. Dähne and M. Schnaider / Exploring Digitized Artworks by Pointing Posture Recognition

7. Conclusion
The paper describes an approach for the creation of interactive exhibits in the context of cultural heritage. It addresses
the specific needs in the context of interactive cultural heritage applications and offers extended interaction and utilization of digitized art works. The interactive museum exhibit has been tested and evaluated at different public places
e.g. showing the Guardi scenario described above at the picture gallery of the Academy of Fine Arts in Vienna, Austria,
which allows drawing some first conclusions with respect
to acceptance and handiness of Mixed-Reality technology in
museums and other cultural heritage sites.
The approach offers a sufficient set of interactivity at a very
generic level and hence meets the requirement to be as intuitive and understandable as possible. It enables exploration
of various forms of digitized art works, such as resulting
from the digital preservation actions currently fostered by
the European Commission in the field of cultural heritage.
However, it is yet to be discovered whether or not interacting with virtual exhibits will directly lead to an increase in
the level of interest of the user and thus the impact on the
quality of education through hands-on experiences. This is
considered to be a next step, which will include the incorporation of the approaches into real exhibitions at cultural
institutions.
8. Acknowledgements
Parts of the work presented here were accomplished with
support of the European Commission through the art-e-fact
project; contract number IST-2001-37924 [Art05], and the
SIMILAR Network of Excellence; contract number IST2002-507609 [Sim05].
References
Project homepage of the art-e[Art05] A RTEFACT:
fact project. Retrieved July 2005 from http://www.
art-e-fact.org.
[Ava05] AVALON: An open x3d/vrml-environment for virtual and augmented reality applications. Project homepage, Retrieved July 2005 from http://www.zgdv.de/
avalon.
[BDR04] B EHR J., DÄHNE P., ROTH M.: Utilizing x3d
for immersive environments. In Web3D 2004 Proceedings
(2004).
[BF98] B EHR J., F ROEHLICH A.: Avalon, an open vrml
vr/ar system for dynamic applications.
[Koh05] KOHLER M.: Vision based hand gesture recognition systems. University of Dortmund, Website, Retrieved
July 2005 from http://ls7-www.cs.uni-dortmund.
de/research/gesture/.
[MS03] M ALERCZYK C., S CHNAIDER M.: Video based
interaction for arts and cultural heritage applications. In
c The Eurographics Association 2005.

119

1st International Workshop on Information and Communication Technologies (ICTs), Arts and Cultural Heritage
(May 2003).
What makes a good inter[Rai05] R AISELIS B.:
active exhibition.
Retrieved February 2005 from
http://www.montshire.net/stacks/exhibits/
goodexhibits.html.
[Sch03] S CHREINER M. R.: X-rays in art and archaeology: History, present state and perspectives. Denver X-ray
Conference, Denver, Colorado, USA.
[SE00] S UN S., E GERSTEDT M.: Control theoretic
smoothing splines. IEEE Transactions on automatic control 45, 12 (2000).
[SHB98] S ONKA M., H LAVAC V., B OYLE R.: Image Processing, Analysis, and Machine Vision. PWS Publishing,
1998.
[Sim05] S IMILAR: Project homepage of the network
of excellence. Retrieved July 2005 from http://www.
similar.cc.
[SM02] S CHWALD B., M ALERCZYK C.: Controlling virtual worlds using interaction spheres. In Proceedings
of 5th Symposium on Virtual Reality (SVR) 2002 (2002),
C.A. Vidal B. C. S., (Ed.), pp. 3–14.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Interface Evaluation for Cultural Heritage Applications: the
case of FERRUM exhibition
A. Alzua-Sorzabal1 , M.T. Linaza2 , M. Abad1 , L. Arretxea1 & A. Susperregui2
1 Faculty

of Humanities, University of Deusto, Spain
2 Asociación VICOMTech, Spain

Abstract
Intuitive access to information in real environments is a challenge for Information Society Technologies. Having
this in mind, a new device for Mixed Reality applications called Virtual Showcase has been developed. It has been
conceived to be used in museums and exhibitions, due to its compatibility with traditional museum showcases,
offering additional information to the public by means of a simple, direct and intuitive interface. This paper
presents the real implementation and usability evaluation of a Virtual Showcase passive-stereo prototype exhibited
in a museum for three months. Some results concerning the evaluation process have also been included. Although
the technology seems to be promising, implementation aspects such as size should be improved.
Categories and Subject Descriptors (according to ACM CCS): I.3.6 [Computer Graphics]: Methodology and techniques

1. Introduction
In the current rapidly changing world, Cultural Heritage is
playing an increasingly important role in providing people
with a sense of who they are as well as where they are coming from. Heritage has been widely recognized as an essential capital for articulating identity and meaning for local
communities, cities and humankind as a whole. At the same
time, heritage and culture in a globalized world are exposed
to a wide range of people, consumptions and communication
demands.
Therefore, new communication and interpretation approaches are required to be able to understand and manage
these changes in such ways that they will enhance the cultural value of Heritage. These innovative approaches should
go beyond the static object-oriented presentations and views
of the past and take into account the wide variety of interests. There is no doubt that emerging technologies offer new
opportunities and environments to create, exchange, discuss
and diffuse cultural contents.
Interpretation can be defined as a means of communicating ideas and feelings which help people enrich their understanding and appreciation of their world and their role in it.
Interpretation has also been described as an educational acc The Eurographics Association 2005.
°

tivity which reveals meaning and relationships. Thus, interpreters should achieve presentations that are interesting and
catch the audience.
Interpreters are supposed to apply a wide range of available techniques and multimedia content in an attempt to
get messages across. In fact, some previous research activities have shown that the best approach for an interpretative presentation is the use of the most effective communication techniques available. However, some authors have
questioned the effectiveness of this effort and believe that
the power, persuasiveness and significance of the message
lie in the story itself rather than in the technology used to
communicate it.
The current paper aims at better understanding the attitudes and behaviour of the visitors towards the application of
new technologies for Cultural Heritage interpretation. In the
domain of users’ technology acceptance, the main objective
of the current approach is to study whether different factors
such as past experiences or technology familiarity can influence the behaviour towards cultural and heritage content
when new technological approaches are implemented.
The paper presents an overall introduction to the applied
research of the Virtual Showcase for archaeological exhi-

122

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

bitions and the methodology used for the evaluation. Section 2 presents the main objectives of the project, both from
the technological point of view and from the socioeconomic
one. Section 3 describes the evaluation methodology, including the main objectives and a brief description of the most
significant methods that have been applied within the exhibition. In Section 4, the real scenario of the FERRUM exhibition is presented, providing also information about the
technical features of the Virtual Showcase prototype and
the modelling of the reconstructed artworks. Section 5 summaries the main results obtained by the implementation of
the evaluation methodology.
2. Objectives of the project
The main objective of this research project was the implementation of a prototype based on Mixed Reality technologies for the dissemination of Cultural Heritage in a userfriendly way for the visitor. This objective was achieved by
the implementation of an innovative prototype and the assessment of the interpretative plan designed for the exhibition.
2.1. Technological and scientific objectives
The main technological objective of this project is the design
and implementation of a prototype based on Mixed Reality
technologies, including the following issues:
• Data acquisition for 3D models of some archaeological
pieces, taking into account the constraints related to their
own characteristics. This issue is crucial, because of the
unique value of the historical pieces that are in different
conditions of conservation and accessibility. It should be
mentioned that some of the artworks could not be photographed with flash;
• Implementation of the Mixed Reality technologies for the
virtual reconstruction of damaged or non-existent parts of
the artworks; and
• Design and development of the Virtual Showcase prototype, that includes the display or Virtual Showcase, the
tracking system and the visualization glasses.
Thus, the main aim includes the development and implementation in a museum of a first prototype based on the application of Virtual Showcase technologies to the dissemination of Cultural Heritage.
2.2. Socioeconomic objectives
One of the most important objectives of FERRUM was the
evaluation of the behaviour of museum visitors concerning
the integration of Information and Communication Technologies (ICTs) in Cultural Heritage (CH) institutions. The
evaluation aims at generating a deeper knowledge about the
attitudes and behaviour of the visitors towards the application of new technologies for Cultural Heritage interpretation. This is crucial information for cultural organizations,

because they will be provided with a methodology for the
evaluation of the acceptance of new ICTs among visitors.
3. Evaluation methodology
Evaluation studies about visitors have already been applied
in two different types of centres (museums and exhibitions)for the dissemination of Cultural Heritage in order to
assess the transmission of a very concrete type of Cultural
Heritage, such as tangible heritage.
Evaluation studies about visitors are a major task for Social Sciences, so researchers have made substantial effort improving existing methods and techniques. The application
of these methods should be based on a scientific process
in order to gain knowledge. This scientific approach, as it
was mentioned by the American Association of Museums
in 1991, should allow gaining systematic knowledge from
and about the current and potential visitors of cultural institutions. Planning and development of new activities for the
public may be based on this knowledge.
Although countries such as United States, Canada,
France, United Kingdom or Germany have a long tradition
concerning evaluation programmes at a national level, this
can not be applied to Spain. Therefore, evaluation studies
about visitors are rarely carried out, although different approaches have been adopted especially related to science
museums. Some examples could be the Museum of Zoology in Barcelona, the National Museum of Natural Science
in Madrid or the National Archaeological Museum also in
Madrid.
On the other side, the trend of the evaluation studies about
visitors to assess the learning process, which means the acquisition of concrete knowledge, is commonly understood.
However, the interpretative theory works mainly on the assessment of different aspects of communication, not only related to the textual contents, but also their meaning for the
visitor, his/her relationship with the object or the cultural institution, and the significance of the institution in the city.
The evaluation study about FERRUM’s visitors was based
on the evaluation of the interpretative plan made on the basis
of predefined informal learning, emotional and behavioural
objectives. It fits within a definition that describes evaluation as a "systematic approach to judge what is better, more
desirable or effective taking into account some defined aims
and criteria" [StMo70]. Therefore, objectivity is required because if evaluation techniques are not objective, the whole
interpretation plan can be considered as worthless.
The evaluation of an interpretative project such as FERRUM is not only searching for quantitative and sociodemographical data, but for qualitative information above
all. The evaluation of the impact of the FERRUM exhibition on the visitor is what really matters, taking into account
that this includes the achievement of some learning and emotional predefined aims.
c The Eurographics Association 2005.
°

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

Therefore, both the quality of the communication from the
museum to the public and the interest of the visitor about the
proposed exhibition from the cultural institution is assessed.
It must be pointed out that the future management of cultural
institutions is determined by the results of such evaluations.
3.1. General objectives
The main objectives of the methodology that has been developed and implemented in FERRUM exhibition are the
following ones:
• Assess the acceptance of the new Information and Communication Technologies as an added value for the dissemination of Cultural Heritage among the general public;
• Analyse the trends of the public concerning the new technologies as general tools and as tools for the dissemination of Cultural Heritage; and
• Determine the way these new technologies could enhance
the learning experience of the visitor and the relationship
with the cultural institution.
3.2. Evaluation methods
The methods applied in FERRUM have been traditionally used by other disciplines of the Social Sciences. This
methodology has allowed quantifying some aspects related
to the quality of the visit. Moreover, the choice of the tools
has determined the type of information that was going to
be gained. This research should allow defining social profiles that could be directly applied to the evaluation analysis
about visitors.
Generally speaking, most of the authors divide the evaluation methods into two main types [Marsh88]:
• Qualitative methods. These are the most desirable ones
in order to assess the communicative process to visitors.
Among these methods, focus groups, in-depth interviews
or direct observation can be included.
• Quantitative methods. These are based on statistical data
about the use of the services.
Taking into account what has been previously mentioned,
a list of the methods that have been applied in FERRUM is
presented.
• Phone queries before and after the exhibition. They included a brief questionnaire that was done in the area
round San Sebastian to about 1200 people.
• In-depth interview. It is a qualitative method based on indepth interviews to one or more people, so that different
conclusions could be obtained after an oriented interview.
The main advantages of this method are the face-to-face
interview, a bigger control of the order of the questionnaire or the higher amount of answers due to human dealing with it.
c The Eurographics Association 2005.
°

123

• Questionnaires. The design of the questionnaires was a
mixture between the model of the phone and the in-depth
questions. For FERRUM, the self-administrative survey
was selected.
• Focus group. It is a qualitative method, for groups of five
to ten people that lasts no longer than one hour and a half.
The members of the group must have some kind of connection (museum curators, technicians).

4. Real experience in a museum environment: the
FERRUM exhibition
4.1. Event definition
The first worldwide application of a Virtual Showcase prototype in a real museum was developed in the context of the
"FERRUM. Burdina Gipuzkoan. El hierro en Guipúzcoa exhibition", in San Telmo Museum (Donostia-San Sebastián)
between March and June 2003. This exhibition must not
be considered as an ordinary exhibition but as a real scale
laboratory, whose design and methodology were carefully
planned.
FERRUM is an applied research project which tries to investigate the way in which visitors take part in cultural environments which make use of new interfaces. Presentation
systems were defined according to the characteristics of the
artworks and the interest to ease the dissemination of the information of the objects.
Museums are facing increasing competition from theme
parks and other entertainment institutions. Some museums,
however, have learned that their visitors do not only want
to be educated, but also be entertained. Consequently they
are investigating new edutainment approaches. Technologies
such as multimedia, Virtual and Augmented Reality in combination with appropriate interaction and storytelling techniques may become the new presentation forms of future
museums.
FERRUM was designed for the general public that goes
into San Telmo Museum alone or in small groups. There
has not been a categorization of target public such as elderly
people, schools or other leisure groups, in the design of the
exhibition.
At the exhibition entrance, the visitor found a Virtual
Showcase of great dimensions, placed in a dark space, where
virtual objects were projected in a clearer and more impressive way (Fig. 1). The reproduced historical and archaeological pieces have been selected depending on their suitability
according to a clear aim: the vision of the real object must
not be overcome by the virtual image.
Archaeological treasures related to iron were placed inside closed showcases to guarantee their conservation conditions. Other objects related to more common issues were
presented without visible protection.

124

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

Figure 3: 3D model of the reconstructed archaeological
pieces displayed inside the Virtual Showcase.
Figure 1: Situation of the Virtual Showcase in the real exhibition.

Figure 4: 3D model of the oldest archeological iron piece of
Gipuzkoa.

4.3. Modelling the 3D reconstructions

Figure 2: Prototype of the Virtual Showcase in FERRUM.

4.2. The Virtual Showcase prototype
The Virtual Showcase has the same form factor as a conventional showcase, making it compatible with traditional
museum displays. Inside the Showcase, real artefacts of cultural or scientific interest can be placed [BEB01]. These can
be enhanced or augmented by projecting computer generated 3D stereoscopic graphics and animations (Fig. 2). The
device can also be used exclusively as a Virtual Reality device, which was the case of the current application.
Several users could look at the Virtual Showcase from
different sides and interact with it. Users were located and
tracked by means of a tracking system. It allowed the graphic
system to generate the adequate view of the virtual object for
each user, depending on its position, what would occur if the
objects were real. The result is that virtual and real objects
are merged correctly, no matter which point of view is chosen.
Another interesting aspect of the system is its support of
several simultaneously tracked users looking at the Virtual
Showcase from different sides [MSC04]. This feature allows
collaborative exploration of artefacts shown in the Virtual
Showcase.

The historical and archaeological artefacts reconstructed in
the Virtual Showcase have been selected having in mind that
the virtual object must provide additional information that
helps to understand the real artwork.
All the artworks of the exhibition have been modelled using Maya. One of the challenges when obtaining 3D models from the art pieces has been the interdisciplinary work
of archaeologists, curators, historians and Information Technologies experts. Thus, when 3D reconstructed models have
been developed, continuous meetings between the technological experts and the archaeologists that discovered the
pieces have taken place in order to achieve scientific validation of the models.
Fig. 3 and Fig. 4 show some of the virtual models that
were projected inside the Virtual Showcase. The sickle is the
oldest iron piece that has been found in Gipuzkoa. Due to its
bad curative conditions, it must remain in the archaeological
deposit and cannot be shown in a public exhibition or even
be scanned to obtain a detailed model. Therefore, photogrametric techniques have been used in its reconstruction.
5. Evaluation results
Before analysing each of the methods developed for the evaluation of FERRUM, and according to the data provided by
San Telmo Museum, about 7500 people have visited FERRUM exhibition during three months. However, it must be
pointed out that these figures are the global amount of visitors to the Museum, although it can be easily assumed that
c The Eurographics Association 2005.
°

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

most of the visitors during this period have also visited FERRUM exhibition.

5.1. Questionnaires
Questionnaires are considered by social scientists as the key
technique for Social Science research, due to their great tradition within these disciplines. Nowadays, although being
only applied to a sample of the population, this method has
reached high accuracy and reliability.
The main difficulty remains in the design of the questionnaires, as it is the basic tool for gathering information. For
the evaluation of FERRUM, this design was based on the
aims of the research. All the answers to the questionnaires
have been stored in SPSS files for further statistical analysis.
5.1.1. Brief questionnaires
Brief questionnaires about FERRUM were placed at the exit
of San Telmo Museum. An important amount of valid surveys (over 60) was collected. It must be pointed out that even
though it seems to be a very low percentage of the whole
amount of visitors, this is a representative amount for a selfadministered survey, that is, filling the questionnaires was
voluntary. The questions of the questionnaires were mainly
focused on the Virtual Showcase.
The average profile of the visitor was a young adult, between 25 and 55 years old, living in the city of Donostia-San
Sebastián, who has a direct relationship with San Telmo Museum. Regarding the way of visiting the exhibition, the percentage was quite balanced between "alone" and "in pairs",
followed by "with family and friends". Organized visits represented a very low percentage. More than half of the visitors
came specifically to visit FERRUM exhibition.
Within the first part of the questionnaire, visitors were
asked about the different subjects that were covered by the
exhibition. With regard to the selected subject (the iron in
Gipuzkoa), about 75 per cent of the visitors agreed that it
was an interesting theme.
Regarding the use of the Virtual Showcase, two thirds of
the visitors recognized the objects represented virtually inside. However, the 58,6 per cent stated that the virtual reproduction did not add any additional value when interpreting
the use of the objects. Moreover, a correlation was found between those visitors who were usual users to Information and
Communication Technologies and the positive acceptance of
the Virtual Showcase.
When asking for a numeric assessment, the exhibition got
an average score of 5,89 along a 1 to 10 value scale. While
43,9 per cent of the visitor evaluated it between 7 and 8, near
a third of the visitors failed the exhibition with values under
5.
c The Eurographics Association 2005.
°

125

5.1.2. Long questionnaires for specialized groups
Longer questionnaires were distributed among people that
took part in the focus groups or people related to Cultural
Heritage. 48 answers were collected, mostly coming from a
group of last-year students from the Humanities Faculty of
Deusto University (43 per cent) and from employees of VICOMTech (27 per cent), who was responsible for the technical implementation of the prototype.
Following the same structure as in the previous case, the
first group of questions was devoted to the assessment of
the different elements of the exhibition. Although the subject of the exhibition has been considered as interesting, the
percentage was lower than in the previous case (only 57 per
cent).
With regard to the Virtual Showcase, most of the visitors
(around 80 per cent) recognized the objects represented virtually. Nearly the same percentage of visitors (77 per cent)
were satisfied with the exhibition and most of them would
recommend it to their friends.
When asking for a numeric assessment, the exhibition got
an average of 5,56 along a 1 to 10 value scale. While 24 per
cent evaluated it between 7 and 8, near a fifth of the visitors failed the exhibition with values under 5. It must be
pointed out that the assessment was quite balanced, with a
lot of punctuations between 5 and 7.
5.2. Focus groups and in-depth interviews
Focus groups are commonly used by the marketing and advertising industry. Its popularity is growing very fast among
other research teams, because they allow observing reality
from the point of view of the user. They are considered to
be one of the most efficient methods for qualitative evaluation [Krueger91]. Therefore, they have been used in FERRUM, where the evaluation objectives aim at knowing the
behaviour of cultural users and the learning process in cultural organizations.
A meeting of a focus group includes a variable amount of
people (seven to ten) with a moderator that acts as a guide
during the session. During the session, participants talk and
give their opinion about those subjects selected by the guide,
who observes and gathers each of the contributions of the
members. As Krueger says, "it is a conversation carefully
planned and designed in order to gain information about a
predefined area".
On the other hand, in-depth interviews are another qualitative method, mainly concerning a personal interview with
representative people in order to achieve some conclusions.
For FERRUM exhibition, focus groups included experts
from Cultural Heritage and educational areas, cultural enterprises and ICTs communities. One of the main issues when
selecting the groups has been the ability of getting a very focused and specialized view of the exhibition. Concerning the

126

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

in-depth interviews, the objectives and methodology have
been the same as for the focus groups. The main difference
was the number of people involved in each group, but the
schema of the questions remained untouched.
All the material generated within the focus groups was
written down and recorded in order to make the transcription of data easier. Therefore, two people from the evaluation team joined each focus group and in-depth interview:
one acted as the guide and moderator of the group, and a
second one observed and listened to the conversation of the
group. The sessions did not last longer than one hour in any
case.
The obtained results were qualitative evaluations. Even
not being expected, it was found out an overall agreement
within and among the different focus groups, no matter the
different backgrounds and profiles.
Focusing on the Virtual Showcase, one of the main technical challenges was the huge difference between a controlled
environment such as the lab and the real environment. Some
of the assumptions about the usability and user-friendliness
of the Virtual Showcase have been refuted during the evaluation process. This has been reinforced by the fact that the
feedback from people that took part on a guided visit was
much more positive than "free" visitors.
On the other hand, the graphical reconstruction of the archaeological pieces was largely criticized. Some of the participants of the groups correlated this issue with the lack of
relationship between the real pieces and their virtual representation inside the Virtual Showcase. This relationship
was not obvious in most of the cases. The quality of the 3D
graphics was also strongly criticized, including the textures
and the visualization of the reconstructions.
Another conclusion concerning the physical setup of the
Virtual Showcase (size, required infrastructure) was the difficulty to integrate the prototype within the exhibition, becoming the main attraction of the whole exhibition. This led
to great expectations that were not completely fulfilled in
most of the cases.
However, it should be mentioned that the evaluation also
showed the positive perception of the potential of this kind
of new interface. Some of the cultural managers agreed that
Augmented and Virtual Technologies may work in cultural
organizations, as they may increase the surprise on visitors
or really provide things that could not be provided by conventional means.
On the other hand, reasons for the disappointment with
the Virtual Showcase included the difficulty in the vision,
the importance of the real object against the virtual reconstruction, the refreshment rate of the graphics, the nonfriendliness of the glasses or even motion sickness.

5.3. Observation
Observation is one of the most popular methods in traditional visitors studies. Two different and complementary approaches have been performed within FERRUM: direct observation through camera recordings in order to observe the
behaviour of the visitors, their emotions and reactions; and
direct observation in the museum to arbitrarily chosen visitors, writing down the itineraries, the standing times and the
duration of the visit.

Figure 5: Users around the Virtual Showcase.
The behaviour of a selection of 150 persons was analysed.
Even though the central location of the Virtual Showcase,
only 20 per cent of the visitors moved directly towards the
Virtual Showcase. Concerning the interaction of the visitors
with the prototype, a third of the visitors did not even approach the prototype. Among the people that have interacted
with the prototype, nearly 45 per cent have tried the glasses
on and about 25 per cent have put them on more than once.
On the other hand, among visitors that stopped at the Virtual Showcase and put on the glasses, the stopping time was
no longer than half a minute. This means that visitors were
not able to watch all the virtual objects, because each object
was projected during six seconds. Only a third of the visitors
stayed longer than three minutes.
5.4. Learning modules
During the FERRUM exhibition, groups of different ages
were selected to take part in learning modules. The didactic materials were adjusted to the different profiles of the
c The Eurographics Association 2005.
°

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

127

visitors. Although the survey has been carried out in small
groups, so that data should not be extrapolated, the modules
have provided the researchers with interesting results about
the communication approaches among different age profiles.
The general public needs some stimuli during the learning process. For instance, young public demands stimuli that
mix learning, discovering and entertaining. Therefore, the
learning modules try to encourage the appropriation of different contents depending on the age of the visitors. The
learning units of FERRUM were designed to achieve three
main objectives:
• Knowledge (cognitive), such as the importance of iron in
Gipuzkoa, the elaboration process of iron, its uses and the
evolution throughout the history, the introduction of concepts of new technologies and uses, specially related to
the Virtual Showcase, and a new vision in the relation between Cultural Heritage and new technologies.
• Attitudes and values (emotional), establishing a property
bond with the iron heritage in Gipuzkoa and its testimonies, letting children express their feelings freely and
considering the visit to be a positive experience.
• Competencies (behavioural), including the promotion of
active participation in visits to cultural institutions and
emphasizing the potential of new technologies for this
purpose, the development of artistic sensitivity and personal capabilities of appreciation and observation, the increase of awareness of the value of our historical memory
and heritage among young population, and the promotion
of good practices associate to Cultural Heritage conservation.
The methodology in the development of the learning modules was based on the classification of the activities into
phases, which were distributed temporarily (two sessions of
one hour each) and spatially (at school and in the museum).
In all of the cases, there has been a previous work with the
teachers in order to verify that contents were appropriated
for each educational segment.
As an example, the results of the evaluation process with
47 children between 9 and 10 years old (26 boys and 21
girls) are presented. The first part of the learning module
was presented in the classroom and the second one in the
museum.
The work at school included the theoretical explanation
about both content and technology, and some activities related to that content. The presentation was mainly focused
on the great importance of the iron industry in the past at the
region of Gipuzkoa, mainly regarding the mines of Arditurri
in the nearby area. Technology was also briefly introduced
due to the presence of the Virtual Showcase in the exhibition.
The groups were very active asking different questions about
the use and performance of a Virtual Reality system. After
those theoretical sessions, children were asked to match several iron objects presented both in situ and in images.
c The Eurographics Association 2005.
°

Once in the museum, a guided tour was conducted, explaining to children the most important features of the exhibition and the Virtual Showcase. Afterwards, they had
free time to visit the exhibition by their own. To finish up,
they made some drawings with the features they liked most.
These drawings were shown in big panels at the entry of the
exhibition.
Furthermore, teachers were provided with some questionnaires to be filled by the children, so that they could evaluate the exhibition as well as the use of the Virtual Showcase. Among the questions in the questionnaires, children
were asked about their personal use of new technologies. Although scanners, digital cameras or simulators got a low percentage, they were quite familiar to computers, videogames
and mobiles devices.
Focusing on the evaluation about the Virtual Showcase,
about two thirds of the children liked it. The acceptance of
the Virtual Showcase was reinforced when assessed in a 1
to 7 ranking, with an obtained average of 5.7. These results
showed clearly that the acceptance of the Virtual Showcase
increased after a guided visit.
Concerning the drawings, the results were very significant
(Fig. 6). The most often drawn issue was the war (61,7 per
cent), with swords and spurs as the most representative elements. The Virtual Showcase and the 3D models presented
by the system such as the sickle, the arrow or the pick, were
represented by 42,6 per cent. One important issue that should
be highlighted is that about one fifth of the drawings included the sickle, which was only virtually presented.
Furthermore, after having finished the learning modules
as well as the guided visit, all of the children agreed that
they have learnt something.

128

A. Alzua-Sorzabal* et al / Interface Evaluation for Cultural Heritage Applications: the case of FERRUM exhibition

needed to understand whether technology may help to better
reach human emotions. Research in interpretation has uncovered that people learn better when basic human emotions
are aroused. Future improvements of the prototype should
consider the ability to capture and symbolise a variety of
emotional commitments and values in the audience in order
to have a wider persuasive impact.
New technologies should furnish opportunities to enhance
interpretative programmes to empower the public by increasing their awareness and understanding, and even to contribute to an attitude-behaviour or behaviour-attitude change.
If this is to happen, Cultural Heritage operators as well as interpreters may need to move forwards innovative means and
move and motivate the self-righteous into action.
7. Acknowledgements
Figure 6: Drawings from some of the children.

The exhibition was part of the project "Applications of Virtual and Augmented Reality Technologies to the diffusion of
Cultural Heritage", partly financed by the PROFIT program
of the Spanish Industry Ministry.

6. Conclusions
The implementation of the first passive prototype of a Virtual
Showcase in a real museum environment has been successfully achieved. It has made it possible to evaluate the impact
of new technologies in the dissemination of Cultural Heritage. Therefore, this research project constitutes a first step
towards the integration of these technologies in traditionally
not too technologically driven environments.
The evaluation results have shown that the interface of the
current prototype should be more intuitive in order to enjoy the whole range of possibilities without any training or
further information. This theory has been demonstrated with
the change in the degree of acceptance of the interface obtained during guided visits, in which the assessment of the
Virtual Showcase has been much more positive.
Another conclusion that was partly foreseen is that the
physical dimensions of the current prototype of the Virtual
Showcase make it difficult to be integrated in the exhibition
space. The Virtual Showcase takes a leading role regards to
the rest of the exhibition, generating huge expectations that
may not be fulfilled.
Finally, it has also been remarked the great potential of
this technological approach. The introduction of real artefacts inside the Virtual Showcase for their augmentation will
provide users with new sensations that could not be imagined otherwise. The edutaintment potential of the prototype
is enormous and should not be refused, taking into account
that the Virtual Showcase should not replace the real objects.
Nevertheless, the presented results have not been enough
to test whether the prototype changes positively the attitude
and behaviour of the visitor toward new means of Culture
and Heritage communication. In addition, more research is

References
[ABBF01] A ZUMA R., BALLOT Y., B EHRINGER R.,
F EINER S., J ULIER S., M AC I NTYRE B.: Recent Advances in Augmented Reality. In IEEE Computer Graphics and Applications 21, 6 2001, 34–47.
[BEB01] B IMBER O., E NCARNAÇAO L.M., B RANCO P.:
The Extended Virtual Table: An Optical Extension for
Table-like Projection Systems. In Presence: Teleoperators and Virtual Environments 10, 6 2001.
[BFSE01] B IMBER O., F RÖHLICH B., S CHMALSTIEG
D., E NCARNAÇÃO L.M.: The Virtual Showcase. In IEEE
Computer Graphics and Applications 21, 6 2001.
[Brooks99] B ROOKS F.P.: What’s Real About Virtual Reality?. In IEEE Computer Graphics and Applications 16,
6 1999, 16–27.
[MSC04] M ACIA I., S USPERREGUI A., C ARRASCO E.,
L INAZA M.T., M IHALIC L., S TORK A.: Application of
Virtual Showcase technologies in real scenarios: the case
of San Telmo Museum. CGI2004- Computer Graphics
International Conference, Crete, Greece, (2004).
[Marsh88] M ARSH J.: Heritage Interpretation Evaluation:
Needs and Methods. Proc. of the First World Congress on
Heritage Presentation and Interpretation, Banff, Alberta,
Canada, (1988).
[StMo70] S TEELE S.M., M OSS G.M.: The criterian
Problem in Program Evaluation. Dept. of Agriculture and
Extension Education, University of Wisconsin 1970.
[Krueger91] K RUEGER R.A.: El grupo de discusion. Guia
practica para la investigacion aplicadad. Piramide 1991.

c The Eurographics Association 2005.
°

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Visitors’ Evaluations of ICTs Used in Cultural Heritage
Ruth Owen a, Dimitrios Buhalis a, and Daniël Pletinckx b
a

b

School of Management, University of Surrey, UK

ENAME Center for Public Archaeology and Heritage Presentation, Belgium

Abstract
Technology that serves to enhance the visitors’ experience is gradually becoming more commonplace at Cultural
Heritage (CH) sites. However ICT is not usually the CH professional’s area of expertise and they have to make
choices from a bewildering array of technology, often without fully understanding their visitors’ ICT needs. This
research aims to alleviate the situation by gathering visitors’ evaluations of technologies that are frequently used at
CH sites along with advanced applications, to identify which technologies visitors use and what they need. The
research took place in five CH attractions in the UK and incorporates the results of one hundred and sixty four
interviews with visitors. Both CH professionals and technology developers can use this research to gain insights into
the use of ICT applications at sites and to identify emerging needs in the marketplace. The findings of this research
indicate that ICTs in use at the CH sites involved were underutilised. Despite this, respondents strongly supported the
advanced applications which included: Augmented Reality; an Interactive Museum Installation; a Mobile Media
Guide and an Avatar Application. This is because they could see how they would benefit. This paper concludes that
the use of ICT was supported by visitors to some degree. However in order to encourage use, the benefits must be
clearly communicated to visitors.

1. Introduction
In software development it is of primary importance
to ensure that the end product meets users’ needs,
within the confines of the project’s objectives.
Therefore it is essential to engage users in design and
to gain user feedback at every stage of the project
[Cap90]. This feedback also assists those who have
commissioned the project. In CH, professionals
report feeling overwhelmed by the array of
technology available. Their concerns are centred on
choosing the application that best meets their visitors’
needs [Che05].
Despite the importance of
communication and collaboration in systems
development, CH and ICT professionals often work
in isolation. This research gathers visitors’ views and
aims to provide the CH and ICT professional with an
insight into what visitors think about ICTs used at
CH sites. The purpose is to support CH and ICT
professionals to understand the priorities and
preferences of their users and to determine where to
allocate resources in the future.
A review of the literature found that there has been
significant work in developing software for CH and
c The Eurographics Association 2005.


to a lesser extent research that obtained user’s views
of technology applications in CH. Two conferences
in particular: VAST and Computer Applications of
Archaeology [CAA] are targeted towards this
research area. However no evidence has been found
of any studies which gather visitors’ requirements for
ICTs at CH sites in order to identify what could
enhance visitor experience. This research seeks to
address the gap in the literature and to add to the
general body of knowledge in this area.
The primary objective of this study is to research
visitors’ views on the use of ICTs in CH to identify
their needs. A further aim is to initiate a dialogue
between technologists and CH professionals for
further
collaboration.
This
study
assesses
technologies used before, during and after a visit.
Four advanced CH ICT applications are evaluated by
visitors.
These include: an augmented reality
application; an interactive museum installation; a
mobile multimedia guide and an avatar application.
The applications were identified as representing
major trends in technology development from papers
presented at VAST 2003 and 2004.

130

R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

2.0 Literature review
There is growing recognition in the literature of the
role that ICT can play at CH sites [Ric96; Kee98; and
Ben04] and in particular how ICT can enhance the
visitor’s experience [San03; Sig05 and Ad05]. The
following paragraphs summarise ways in which ICTs
can add value to a site visit.
i) Entertaining: ICTs offer new ways to make
interpretation entertaining. Interactive games are
designed to be fun to use and educational at the same
time. This is known as Edutainment or a combination
of education and entertainment [Buh03].
ii) Helps me to understand: There are lots of ways in
which technology can help visitor to understand
[Ben04]. For example Augmented Reality can help
visitors visualise what an exhibit used to look like.
iii) Educational: Most CH visitors seek to learn
during their visit [MdC02]. Interactivity allows
content to be layered. The user selects the depth of
information they wish to receive. Therefore different
types of visitors can use the same application. For
example they could be novices or experts etc.
iv) Usability: If the application is complicated to
operate, the visitor’s concentration is focused on how
to use the system, rather than the presentation.
Conversely an application that is easy to use enables
the user to quickly find the content they require.
2.1 Technology trends
A review of papers presented at VAST 2003 and
2004 identified several specific trends in terms of
ICT applications for CH. Four applications were
found to be particularly useful for visitor
interpretation. These are as follows:
i) Augmented Reality (AR): is the term used for a
computer generated reconstruction which overlays a
photograph or film [LL00]. This enables comparisons
to be made between before and after scenarios.
ii) Interactive Museum Installation: Interactivity
refers to a system that is computer-based, where the
user's input is then processed by the system, which in
turn affects the output [Bee05]. This application can
only be used within the site.
iii) Mobile Multimedia Guide: An enhancement of
the traditional audio guide so that multimedia can be
presented. Wireless devices have a context specific
capability where the user’s position can be tracked
and appropriate information sent accordingly
[VDB*04].

iv) Avatars are computer generated images which
represent the user in the virtual world [Bee05]. In a
CH context, avatars are used to guide visitors around
the virtual site and to disseminate information.
For the purpose of this research the ways in which
technology can enhance a visit to the site was tested
by visitors evaluating advanced applications on the
basis of concepts identified from the literature.
3.0 Methodology
The research design required all respondents to be
visitors of CH sites. Conducting research within the
actual premises of a CH site ensured that this prerequisite was met. A pilot study was carried out at
one site in order to test the questionnaire design. The
actual study was undertaken at five well-known CH
sites which attract different types of visitors.
3.1 Site selection
The research focused on three kinds of CH sites:
museums, monuments and archaeological sites. Each
site was chosen as an outstanding example of their
field. Sites were also selected on the basis that they
used different technologies, in order to obtain views
on a wide range of applications.
3.2 Sample selection
The interest of this study focuses on visitors to CH
sites in Europe. However it would not be feasible to
conduct research at every site in Europe, because the
population of interest is too broad and respondents
widely dispersed. This research focuses on CH sites
in England. Since CH visitors tend to originate from
many countries, the inferences produced here should
be representative of CH visitors throughout Europe.
It was not practical to conduct probability sampling
due to the exploratory nature of the research.
Therefore non probability sampling was used. This
means that the findings cannot be generalized to the
population. However this study’s contribution is to
provide exploratory data on a topic which has little
coverage in the literature. Audience segments were
identified to ensure a wide range of visitors were
interviewed. These are as follows:
i) Age: The study sought to examine whether children
who are growing up with exposure to ICTs from a
young age have different views to adults [Dea96], or
perhaps it would be the “silver surfers” those who
have retired and have taken up computing as a hobby,
who are most likely to use ICTs at CH sites.

c The Eurographics Association 2005.


R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

ii) Gender was tested to see if there were any
differences in the opinions and use of technology
between men and women. It is a common perception
that men adopt technologies quicker than women.
This research aims to examine whether this is the
case at CH sites.
iii) Domestic / International visitors could have
different needs in terms of: culture; language;
previous experience and expectations of the site etc.
Therefore identifying domestic and international
visitors and examining their responses may bring to
light differences in perspectives.
iv) Adopter category follows Rogers’ Diffusion of
Innovations theory [Rog62] which classifies people
into five categories in terms of how quickly they
adopt ideas. It begins with people who are the fastest
to adopt ideas, the “Innovators”, to those who are the
last to adopt ideas, the “Laggards” (Figure 1). For
this research respondents were classified according to
how quickly they adopt technology to see whether the
diffusion of innovations curve was followed in terms
of their opinions and use of technology at CH sites.

131

3.4 Survey procedure
The interviewer chose locations where respondents
would not be in any hurry such as a café or gift shop.
In the café setting interviewees were selected as they
were sitting at their table. In the museum and gift
shop locations the researcher approached the person
as they arrived at the interview point.
The
interviewer introduced herself and explained the
purpose of the questionnaire and gave an indication
of how long the research would take.
Once
permission had been granted, the interviewer read out
the questions and the possible answers.
The
interviewer recorded the respondent’s answers and
any other comments that they chose to make.
The questionnaire was divided into three scenarios:
technology used before, during and after the visit.
The advanced applications were evaluated in the
during the visit section of the questionnaire as this is
the situation where they are most likely to be used.
The number of technologies evaluated was limited to
four to minimise interviewee fatigue.
3.5 Modifications to the questionnaire

Innovators
2.5%
Early Adopters
13.5%

Early Majority
34%

Late Majority
34%

Laggards
16%

Figure 1: Rogers’ Diffusion of Innovations Curve
A quota was established to ensure that each
segment was large enough to enable the findings to
be examined in some depth [Tro04]. It was not
necessary to ensure that the respondent categories
were proportional to each other. Rather that each
category reached a minimum, set at 10% of the total
number of respondents.
3.3 Survey method
The data collection method chosen was questionnaires. The main advantage of questionnaires is that
response rates are usually high and cost is low.
Quantitative closed questions were used on subjects
where information could be captured quickly.
Qualitative open ended questions were used to give
interviewees the opportunity to make further
comments and expand on their views. When
evaluating the advanced ICT applications,
respondents were given visual aids which had a
picture of each application printed on them. The
questionnaires were personally administered so that
the interviewer could explain the technologies shown
on the visual aids and answer any questions the
respondents had.
This also ensured that the
questionnaires were completed in full.

c The Eurographics Association 2005.


During the pilot study it emerged that the interviews
were taking too long to complete. Therefore for the
actual study, although structure of the questionnaire
remained the same, the questions were streamlined to
allow more people to be interviewed. For example
some of the closed questions had the number of
optional responses reduced so that they would be
quicker to read. Also the original evaluation method
for the advanced applications used the open ended
questioning technique. This was changed for the
actual study to rating the following five criteria:
Respondents were asked to rate how entertaining
they thought the application would be. Interviewees
were then asked whether the application helped them
to understand to find out whether the visitor thought
they understood the message better having viewed
the application. Visitors were then asked to what
extent they thought each application would be
educational, followed by if the application looked
complex or easy to use. The final criterion was to
find out whether the respondents felt that the costs of
developing the technology was a good use of public /
private spending. Or in other words, money well
spent, or whether it would be better spent elsewhere.
A five point Likert scale was used to rate
respondents’ reactions where 1 = strongly disagree, to
5 = strongly agree. Any comments that respondents
made were written down during the course of the
interview.

R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

132
4. Findings

The findings are organised in terms of evaluating
ICT visitors’ need: before, during and after the visit.
Quantitative findings are presented along with an
analysis of visitors’ comments which helps to provide
an insight into this subject. Table 1 and Figure 2
describe the demographics of the respondents.
Type
Male
Domestic
Day visit
Been before Yes

Freq.
50%
88%
68%
57%

Type
Female
International
Overnight stay
Been before No

Freq.
50%
12%
32%
43%

Table 1: Description of Respondents.
Age

12.2

17.68

Under 18
10.98

Adopter category

18-25
26-35

17.68

Some CH sites have added advanced booking to
their websites. The research found there was strong
resistance to advanced booking as 55% of
respondents said they “definitely would not book
over the Internet” and 66% “definitely would not
book over the telephone”. The main reason was that
people felt they would be restricted to visit at a
specified time. Some respondents indicated they
might use advanced booking if there was an
important benefit such as to avoid queuing or
discounts. The question was asked at sites that
charged an admission fee because most visitors to
sites that do not charge, simply walk in. Also by
asking the question to respondents who paid an
entrance fee, enables the results to be comparable
with other European sites. However caution should
be applied to the results because the research was
carried out in low season when visitors are
substantially fewer. If the research was conducted in
high season and the respondents had to queue, the
results may have been different.

40

36-45
15.85

46-55 30
56+
%

25.61

20
10
0
Laggard

Late M

Early M

Early A

Innovator

Figure 2: Respondent profile.
4.1 Before the visit
The Internet’s capabilities of generating awareness
about the site and information provision were
examined. The results found that 70% of the
interviewees did not hear about the site from a
specific source. They already knew about the site.
This can partly be explained by the fact that 88% of
respondents were domestic visitors and many were
repeat visitors. 5% heard about the site from the
Internet. This demonstrates that there is some
potential for generating site awareness through the
Internet.
In terms of what visitors used to gather information,
the most frequently used source was tied between the
Internet and leaflets / brochures (11%). 62% of
respondents did not need to look for information
because they had been before etc. Visitors were
using the Internet to search for dynamic information
which is subject to change such as admission and
exhibit information and leaflets / brochures for more
static information about the site.

Respondents were asked about the extent to which
technology influenced their decision to visit. 89%
said that it had had “very little influence” on their
decision. Interviewees explained that they didn’t
visit a site simply to use the technology. They came
to see what the site is known for, such as the
Dinosaur exhibition at the Natural History Museum
and so on. Although in terms of visitor segments,
some weak relationships were found where the
younger the respondent, the more technology
influenced their decision to visit (r-.17) (p=.026).
Also the faster the respondent adopts new ideas the
more technology influences their decision to visit (r.17) (p=.029).
4.2 During the visit
Respondents were asked what technology they had
used at the site. Table 2 depicts the percentage of
respondents who used each technology taking into
account availability at the site. Touch screens were
most frequently used and searchable catalogues the
least used. However the results clearly show that
every application could be used by more visitors.

%
Base*

Touch
screen
58

Audio
guide
40

Computer
game
15

Searchable
catalogue
7

99

139

99

139

* Number of respondents

Table 2: Technology used at sites.

c The Eurographics Association 2005.


R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

The use of technology at the site was compared with
the audience segments specified in section 2.2. There
were no significant differences found. The use of
technology was compared between those who had
“been before” and those who were on their first visit.
A much higher percentage of those who had visited
the site previously (16%) used a computer game in
comparison to only 4% of respondents who used a
computer game on their first visit. Therefore on
repeat visits, people were more likely to use
computer games. This demonstrates that on an initial
visit people want to see the actual artefacts. Whereas
on repeat visits visitors start using other resources
such as using computer games. This suggests
additional uses for technology: to entice the visitor
back to the site and to enhance the repeat visit
experience by exploring the technology on offer.
However it has to be said that touch screens were the
only device where a significant difference was found.
Nevertheless technology could feasibly provide
added value for repeat visitors.
Respondents were asked what they used to help
them navigate around the site. They could specify
more than one category. Table 3 depicts the results.

%
Base

Signs
74

Audio
guide
30

Staff
21

No
assistance
8

Touch
screen
6

164

136

164

164

99

Table 3: Navigational aids at sites
Audio guides were given out to every visitor at one
site. This may cloud the picture of audio guide use
because at other sites they were designed to be used
in a specific area, rather than the whole site. Touch
screens were used by just 6% of respondents.
However it is recognised that many touch screens are
not designed for site navigation.

133
generated image of an abbey that once stood at the
site (shown on the right hand side) can be viewed.

Figure 3: Augmented Reality application. [ENA04]
The evaluation criteria with the highest mean score
for the augmented reality application was
“understand” (4.27) followed by “educational”
(4.26).
The lowest mean score was (3.75)
“entertaining”. The mode value was 4 or “agree” in
every category. In terms of who was using the
application, there was only one relationship found in
terms of: the younger the respondent the more likely
they would say it helped them “understand” (r-.16).
(p=.038). There were no significant differences found
for this application which demonstrates that it appeals
to a wide range of audience segments.
From the respondents’ comments, the most
frequently cited benefit of this application was that it
helped them to picture the subject. The second most
frequent comment was that it gave them further
information, such as 3D measurement and
perspective. One respondent felt that content was
more important than the device presenting the
information. If the content is disappointing, the user
is unsatisfied with the overall experience. This view
was found to be supported during the visitors’
evaluations of the interactive museum installation.
Therefore ICT developers need to work closely with
CH experts in interpretation design.
4.3.2 Interactive Museum Installation

4.3 Advanced technologies
Interviewees were asked for their views on four
advanced technologies. It was not feasible to install
each of the four advanced technologies at the sites
where the study was undertaken. Instead visitors
were presented with an image of each technology,
and were given a description of how it worked.
Respondents were asked to rate their first impressions
based on the descriptions of the technology.

Figure 4 shows an application that was developed
for a museum associated with the Olympic Games in
Greece. The idea is to fit pieces of the pottery
together in order to identify which of the vessels
shown at the top of the picture it is.

4.3.1 Augmented Reality Application
Figure 3 shows an augmented reality application
developed by ENAME, in Belgium. The viewing
platform on the left hand side of the picture contains
a computer screen through which a computer
Figure 4: Interactive Museum Installation [GCP04]
c The Eurographics Association 2005.


134

R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

The highest mean score for the interactive museum
installation was “easy to use” (3.9) followed by
“entertaining” (3.7). The lowest mean score was
“helps me to understand the subject” and “good use
of public / private spending” (3.36 jointly). The
mode was 4 for each evaluation criteria. Therefore
the most frequent response was “agree”. Significant
differences were found between domestic /
international with respondents rating “educational” as
(p=.001) and “easy to use” as (p=.008).
The most frequent comment from respondents was
that this application is more suited to a child than an
adult. It was regarded as being “too easy” for adults
to use. However ease of use is necessary if children
are going to use the application. Several adults said
they would like to see more content added. For
example explaining the reasons why the shape and
pattern were chosen, to explain the meaning
associated with them.
This demonstrates that
interpretation is important to the visitor. However,
the mean rank scores from Kruskal Wallis tests
showed the international visitors gave a higher score
than domestic visitors for “educational”, 113.39
compared with 78.45 respectively.
Perhaps
respondents were indicating their support for an
application that could be understood in any language.
However in order to cater for the need for more
interpretation without alienating visitors who speak
different languages, interpretation / instruction should
be available in multiple languages.

(3.82). Three of the evaluation criteria had a
maximum score of 5. No other advanced application
tested in this research achieved a mode value of 5 for
any of the evaluation criteria demonstrating that
interviewees found this device to be the most useful
out of all four applications.
Although the results show that this device had the
strongest support from visitors, some felt that it
looked very complex to use. However one respondent
had a different perspective. She felt that this device
would be easier to use than audio guides because the
visitor does not need to press any buttons which is
often very confusing for the visitor. Some visitors
were concerned that they would be so busy trying to
operate the device, that they would miss seeing what
is actually at the site. Despite these comments, there
was a very positive reaction to this device overall.
4.3.4 Avatar Application
Figure 6 depicts an avatar application that is based
on a town in Germany. The avatar takes the user on a
virtual tour of the site. The user can move the avatar
around the scene and can ask questions about the
buildings etc. to the avatar who supplies the answers.

4.3.3 Mobile Multimedia Guide
Figure 5 shows a mobile multimedia guide
application that has been tested in Pompeii, Italy and
Olympia, Greece. The idea is that as the user walks
around the site, multimedia content is sent to the
device according to where they are located. The
glasses can be used to see computer generated
reconstructions of what once stood at the site.
Therefore visitors can compare the site’s appearance
today, with what it once looked like.

Figure 5: Mobile multimedia guide [VDB*04]
The highest mean score for the mobile multimedia
guide was (4.38) for “educational” followed closely
by “helps me understand” (4.37). The lowest score
was for “good use of public / private spending”

Figure 6: Virtual Guide [RFD04]
The highest mean score for the avatar application
was (4.11) for “educational”, followed by
“entertaining” and “helps me to understand” which
both scored (3.98) respectively. The lowest mean
score was for “good use of public / private spending”.
The mode was 4 (agree) for every category.
In terms of who would use this device, many adults
felt this application would appeal more to children
than themselves, although the statistical tests found
that the children’s scores were similar to the adult’s
scores. There was a relationship found for adopter
category where the closer respondents were to being
innovators, the higher they rated the application as
Domestic
educational where (r=-.018) (p=.021).
/ international also differed in terms of “helps me to
understand” with domestic visitors giving higher
c The Eurographics Association 2005.


R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

mean rank scores than international visitors 85.77 to
57.55 respectively. This was possibly due to the
application’s reliance on language.
There was some resistance among adults for this
application because of the fact that it looked like a
computer game. It emerged that one of the reasons
for visiting a site was to give children a change from
playing computer games.
However some
respondents said that they would like this application
to be made available over the Internet for use at
home. This would also avoid the problem of
bottlenecks of visitors waiting to use the application.

135
age, adopter category, gender and domestic /
international categories found between their
satisfaction with the use of technology at the site and
their evaluations of the site overall. However there
was a positive relationship found for “variety of
technology at the site” and “satisfied with the site
overall” with (r=.030) (p=.028). Therefore the more
the respondent was satisfied with the variety of
technology, the more they were satisfied with the site
overall. This demonstrates that variety was more
important to interviewees than use. This contrasts
with some respondents’ views cited earlier, that
interpretation was more important than the device
which presents the information.

4.4 After the visit
5. Conclusions
Respondents were asked if they would look for
further information after leaving the site. Of the 52%
of respondents who said they would look for
information, 51% would use the Internet and 64%
would use books. Many respondents said they would
use both books and the Internet. This shows that
although traditional methods are still the most
frequently used source for finding further information
after the visit, the Internet is catching up.
Visitors were asked would they recommend the site
on the basis of technology. More than half of
respondents (55%) said they were “likely” to “very
likely” to recommend the site on this basis.
Therefore after having seen and considered the use of
technology at the site, more people would
recommend on this basis (55%) than were influenced
themselves as 89% said that technology had “very
little influence” on their decision to visit.
Respondents were asked to rate their satisfaction
with the technology at the site and satisfaction with
the site overall using a 5 point Likert scale where 1 =
extremely unsatisfied to 5 = extremely satisfied.
Table 4 shows the results.

Satisfied
use tech

Satisfied
variety
tech

Satisfied
site
overall

Extremely unsatisfied

0

0

0

Fairly unsatisfied

2

4

1

18

27

3

57

57

34

Extremely satisfied

23

13

62

Base

136

136

158

%

Neither satisfied nor
unsatisfied
Fairly satisfied

Table 4: Site satisfaction ratings (%)
Most respondents said they were either fairly or
extremely satisfied with the site. There were no
relationships or significant differences in terms of
c The Eurographics Association 2005.


This research examined CH visitors’ views of the
benefits that technology could bring in terms of
enhancing interpretation and whether visitors used
technology currently available.
It was found that visits are frequently spur of the
moment decisions or repeat visits. Therefore many
visitors do not have the need to look for information,
or book in advance. Technology plays a negligible
role in the decision to visit. The main driver is to see
the site and actual artefacts at first hand.
In terms of technology currently operational at the
study sites, all were found to be underutilised. Yet
this did not affect the ratings for the advanced
applications where all of the evaluation criteria for
every technology had a mean of greater than 3 out of
a possible score of 5. Therefore once the technology
was in front of the respondents, they could see for
themselves how they would benefit.
The Internet was identified as a primary source to
gather further information after visitors had left the
site. Also despite the use of technology being
incidental to the visit decision, more than half of
interviewees said that technology would influence
their recommendations to others. However a quarter
of respondents indicated that they would be very
unlikely to recommend a site on the basis of
technology alone. According to the satisfaction
ratings, respondents preferred more of a variety of
technology, rather than how it was used.
In terms of who was using technology there were
few marked trends although there seemed to be slight
indications of innovators and children adopting the
technologies quicker. However this varied according
to the device.
The main issue arising from this research was to
increase the use of applications both for existing
technologies at the site and new technologies to be

136

R. Owen, D. Buhalis & D. Pletinckx / Visitors’ Evaluations of ICTs Used in Cultural Heritage

introduced in the future. It is an easy mistake for
those developing technology to assume that the
visitor will use the device because it happens to be
there. Visitors are trading off their time with using
the technology. Therefore the application’s benefits
and uses must be made immediately apparent to the
visitor in order to encourage use.
This research examined the extent to which visitors
used and valued the use of ICTs at sites. Due to the
limited studies in this area, this research concentrates
on a preliminary analysis of ICT needs. Further
research could involve an investigation of the
advanced applications in use.
An additional
limitation of this research is that it required
respondents to have a good understanding of English
otherwise they would have difficulties answering the
questions in the study. Future research could involve
interviewing respondents who spoke different
languages.
Acknowledgements
This study forms part of the EC, FP6 Network of
Excellence IST-2002-507382 EPOCH. The authors
would like to thank the developers of the
technologies examined in this study for their
contributions and the European Union for their
substantial financial support, without which this
project would not have been possible.

[AP98]
[Bee05]
[Bee04]

[Buh03]

[Cap90]

[Che05]

[ENA04]
[GCP04]

[Kee98]

[LL00]

[MdC02]

[Ric96]
[Rog62]
[RFD04]

[San03]

References
[Add05]

[Dea96]

ADDIS, M.: New technologies and
cultural consumption – edutainment is
born! European Journal of Marketing,
39(7/8) pp 729-736, 2005,
AMBROSE, T. and PAINE, C. Museum
Basics. Routledge, 1998.
BEEKMAN, G.: Computer Confluence,
Prentice Hall, 2005.
BENNETT, M.: The role of technology,
in Heritage Visitor Attractions, eds.
Leask, A., and Yeoman, I., Cassell,
2004
BUHALIS, D.: eTourism Information
Technology for strategic tourism
management.
Pearson
Education
Limited, 2003.
CAPRON, H.L.: Tools for an
Information Age, Benjamin Cummings
Publications, 1990.
CHEDI, Study of the New ICT needs of
those responsible for Cultural Heritage
Sites, monuments, museums and their
visitors. Interim paper

[Sig05]

[Tro04]

[VDB*04]

DEAN, D.: Museum Exhibition Theory
and Practice, Routledge, 1996
www.ename974.org. Retrieved from
the World Wide Web 28/10/04
GAITATZES, A., CHRISTOPOULOS,
D., AND PAPIAOANNOU, G., The
Ancient Olympic Games: Being Part of
the Experience. VAST 2004 Conference
proceedings. pp 19-28 ISBN 3-905673
KEENE, S.: Digital collections
Museums and the Information Age,
Butterworth-Heinemann, Oxford, 1998
LAUDON K.C., AND LAUDON, J.P.:
Management Information Systems,
Prentice Hall, 2000
MCKERCHER, B., and DU CROS:
Cultural Tourism The Partnership
between Tourism and Cultural Heritage
Management, The Haworth Hospitality
Press, 2002.
RICHARDS, G.: Cultural Tourism in
Europe, CAB International, 1996
ROGERS,
E.M.,
Diffusions
of
Innovations, Free Press, 1962
RYDER, G., FLACK, P. AND DAY,
A.M.,
2004,
Adaptive
Crowd
Behaviour, to Aid Real-Time Rendering
of a Cultural Heritage Environment.
VAST 2004 Conference proceedings pp
19-36 ISBN 3-905673
SANDIFER, C.: Technological novelty
and
open-endedness:
Two
characteristics of Interactive Exhibits
that contribute to the holding of
VISITOR ATTENTION in a science
museum, Journal of Research in
Science Teaching, 40(2) pp 121-137,
2003
SIGALA, M.: In search of post-modern
online authenticity in Sigala, M. &
Leslie, D. International Cultural
Tourism management, implications and
cases. Elsevier, 2005
TRONCHEIM, W.,
www.socialresearchmethods.net.kb
Retrieved from the World Wide Web
28/10/04
VLAHAKIS, V., DEMIRIS, A.,
BOUNOS, E and IOANNIDIS, N.,
2004, A Novel Approach to Context –
Sensitive Guided e-Tours in Cultural
Sites: Light Augmented Reality. VAST
2004 Conference proceedings. pp 57-66
ISBN 3-905673

c The Eurographics Association 2005.


The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

A Texture Based Approach to Reconstruction of
Archaeological Finds
M.Ş. Sağıroğlu and A. Erçil
Sabancı University, İstanbul, Turkey

Abstract
Reconstruction of archaeological finds from fragments, is a tedious task requiring many hours of work from the
archaeologists and restoration personnel. In this paper we present a framework for the full reconstruction of
the original objects using texture and surface design information on the sherd. The texture of a band outside
the border of pieces is predicted by inpainting and texture synthesis methods. The confidence of this process is
also defined. Feature values are derived from these original and predicted images of pieces. A combination of
the feature and confidence values is used to generate an affinity measure of corresponding pieces. The
optimization of total affinity gives the best assembly of the piece. Experimental results are presented on real
and artificial data.
Categories and Subject Descriptors (ACM CCS): I.4.9 [Image processing and Computer vision] : Applications

1. Introduction
In archeological sites, we may encounter a large number of
irregular fragments resulting from one or several broken
objects. The reconstruction of the original objects is a
tedious and laborious task. In this paper, we consider the
complex problem of automatically assembling 2D/3D
objects, from their fragments commonly called sherds,
using input from multiple cameras or 3-D scanning system
with synchronized texture facility. The artifacts are freeform, multiscale individually and with respect to one
another, they are geometrically and photometrically highly
complex and highly variable, and huge in number.
Previous works on the assembly problem have focused
mainly on geometrical properties of the pieces. The puzzle
pieces are represented by their boundary curves. As the
fractions of boundaries are adjacent and thus similar, a
pairwise affinity measure is computed by partial curve
matching. Some approaches especially related to standard
toy-store jigsaw puzzle solver use feature based matching
methods. The problem of jigsaw puzzle solving is a
reduced and restricted version of the general assembly
problem. Its computerized solution was first introduced by
Freeman [FG64], who successfully solved a 9-piece jigsaw
puzzle. Other works [GMBO2, WSKL88, CFF98, KDB*9]
also use feature based matching approaches. These
methods are relatively fast so that they manage to
 The Eurographics Association 2005.

assembly even if the number of puzzle pieces becomes
large. The main drawback of this approach is that they
cannot provide detailed matching of boundaries and
overlapping regions. Research involving classical jigsaw
puzzles has so far ignored texture or color information to
the assembly problem. There are a few approaches, which
use only the color values of pixels on the boundary
contour [CFF98].
More general partial curve matching algorithms that
solve the global 2D and 3D assembly problems based on
geometrical properties were presented in [KK01, RB82,
Wol90]. The problem of 3D curves is addressed by
[UT99]. The accuracy of the matching technique depends
on perfect extraction of the trace of a curve and the
computation of curvature and torsion. It is potentially a
non-robust process and has only been tested on artificial
data. Another research [SL02] matches 2D and 3D break
curves by combining a coarse-scale representation of
curves and refine iteratively via a fine-scale elastic
matching. The works that achieved global assembly of
pieces based on curve matching have not attempted to
combine the geometrical methods with textural
information.
There is great scientific interest in the archaeological
community in reconstructing objects from fragments. An
automatic tool that assists archeologists in reconstructing
monuments or smaller fragments would be highly

138

M.Ş.Sağıroğlu & A. Erçil / A Texture Based Approach to Reconstruction of Archaeological Finds

beneficial. Such a tool would lead to avoiding unnecessary
manual experimentation with fragile and often heavy
fragments, and reduce the assembly time. Currently, the
Digital Michelangelo team is tackling the problem of
assembling the Forma Urbis Romae[Lev00]. It is a marble
map of ancient Rome that has more than a thousand
fragments. Their investigation is based on broken surface
border curves, possibly texture patterns, and additional
features of the fragments. The University of Athens has
developed “The Virtual Archaeologist” [PKT01] system,
relying on the broken surface morphology to determine
correct matches between fragments. This method detects
candidate fractured faces, matches fragments one by one
and assembles fragments into complete or partially
complete entities. The Shape Lab at Brown University
presents an approach to automatic estimation of
mathematical models of axially symmetric pots made on a
wheel [WC03]. This technique is based on matching break
curves, estimated axis and profile curves, a number of
features of groups of break-curves. Finally, the assembly
problem is solved by maximum likelihood performancebased search. At the Technical University of Vienna, a
fully automated approach to pottery reconstruction based
on the fragments profile, is given.[KS03]
Neglecting continuity of color and texture for adjacent
fragments is a waste of valuable information for many
cases. The pictorial information on a fragment consists of
various components, and different specifications of surface
image of pieces are dominant according to implementation
field. In the classical jigsaw puzzles, the essentials of
assembly depend on the alignments of object edges (e.g.
picture of a house), the similarity of colors (e.g. cloud
drawing) and continuity of textural properties (e.g. grass of
a garden) for the adjacent pieces. In the archeological
field, the pictorial features may include highly directional
marble veining, the pattern of surface incisions, paintings
on the outer and inner surfaces, carvings and horizontal
circles due to finger smoothing while the pot is spinning
on the wheel.
In archeology, erosion, impact damages or undesired
events cause fragments to vanish or deteriorate, such as in
the case of Forma Urbis Romae. This reality increases the
necessity of pictorial information to solve the
reconstruction of all types of puzzles, because the
geometrical approaches relying on exact matching of break
curves are not applicable to the assembly of the pieces, if
the border of fragments have disappeared. The texture
prediction method can manage to estimate possible
adjacent fragments, even if there is a gap caused by
erosion between two neighboring pieces.
In this paper, we design a texture prediction algorithm,
which predicts the pixel values in a band outside the
border of the pieces with a confidence measure. Features
obtained from the predicted texture outside a piece are
correlated with original pictorial specifications of possible
neighboring pairs. An affinity measure of corresponding
pieces that utilizes all kind of image information, such as

continuity of edges, textural patterns, and color similarities
is defined and the assembly problem is stated as the
optimization of this affinity measure.
The rest of this paper is organized as follows: Section
2 outlines the method used in solving the assembly
problem, Section 3 presents image inpainting and texture
synthesis methods that are used in predicting the expanded
part of the pieces. The cost function/affinity measure used
in the assembly process is explained in Section 4.
Experimental results are given in Section 5.
2. Automated puzzle assembly method
Our proposed approach is based on defining a
performance measure that represents the appropriateness
of the assembly based on textural features and geometrical
shape, and to find best transformations of pieces that
maximize matching of textures of fragments while the
geometrical constraints are being satisfied. Initially, we
acquire and preprocess the images of pieces. After
collection of visual data, the first step is the prediction of
the pixel values in a band around the border of the piece;
this step is applied to all pieces separately. The prediction
algorithm automatically fills in this extension region using
information in the central part. The main idea in extending
the picture/texture on the fragment outwards is that the
correlation between the features of the predicted region
and its true neighboring piece is significantly higher than
alternative pairings. We use the mixture of inpainting and
texture synthesis methods for prediction. Image inpainting
is the process of filling in missing data in a designated part
of an image or a video from the surrounding area, and
texture synthesis is to create a new image with the same
seed texture but of different shape to a sample region.
While extending the fragment image, we introduce
confidence of extension as a new parameter in the
prediction phase of the assembly problem. This parameter
represents the reliability of extended values and will be
used by later processes. The confidence depends on the
structure of the texture such as continuity of edges,
roughness of texture and distance to the border of original
fragment. We then derive feature values in both the
original fragment and the extended region. The proposed
approach does not bound the number of features or does
not restrict the type of image features. Any textural feature
believed to improve the success of assembly can be easily
inserted into the process. The next step is to determine a
similarity or a cost function between two textural regions.
The final goal of the proposed approach is to establish an
affinity measure of corresponding pieces by the
combination of the feature and confidence values. The
matching of pieces and achievement of the assembly is
established by optimizing this affinity measure. Initially,
each fragment has a random position in space. To improve
the assembly, we have to be able to sense whether a
particular arrangement of pieces improves the puzzle or
not; this is done using a total affinity measure defined as
the sum of affinity measures of all points in the space. The
 The Eurographics Association 2005.

M.Ş.Sağıroğlu & A. Erçil / A Texture Based Approach to Reconstruction of Archaeological Finds

space may be 2D such as for the broken marble problem or
3D such as for the pot assembly problem. In this paper, we
present results on 2D examples. The extension of the
proposed method to 3D is computationally costlier, but is
theoretically possible.
3. Inpainting and texture synthesis for expanding the
pieces
As mentioned in section 2, the first step in the
assembly process is the expansion of each piece in a band
around the border of the piece by predicting the pictorial
information on the surface outwards. Inpainting and
texture synthesis are two techniques that will be used to
carry out this task.
Image inpainting refers to the process of filling-in the
missing areas or changing an image in an un-noticeable
way by an observer. It is usually applied to the task of
restoring photographs, films or paintings, and removal of
occlusions, such as subtitles, stamps and text. In [BSCB00,
OBMC01], a series of partial differential equations is used
to mathematically model this process. These techniques
determine how the linear structures (called isophotes)
propagate into the region to be inpainted. Other inpainting
approaches are the Total variational (TV) and Curvaturedrive diffusion models (CCD)[CS00]. TV uses an Eulerlagrange equation to minimize total variation and employs
anisotropic diffusion. Such a method handles noise well,
but does not complete broken edges. CCD is based on the
TV algorithm and geometric information of isophotes. The
drawback of these methods is the blurring of inpainted
image introduced by the diffusion process in the larger
filling regions.
Texture synthesis is an active research topic in
computer vision, which has broad applications such as
foreground removal, lossy image compression, and texture
generation. The problem of texture synthesis is to fill
large image regions with a sample texture. This method,
which replicates consistent textures, can be used in
extension of images, but it has problems to fill in real
image patterns. Linear structures such as a drawing of a
line or crossing regions of different textures usually
include high frequency components, which prevent to
generate natural images by this approach.
To overcome the drawbacks of inpainting and texture
synthesis algorithms, the method presented in [BVSO03]
first decomposes the image into the sum of two
components with different basic characteristics and then
reconstruct each one of these components separately with
inpainting and texture synthesis. Another approach by
Harrison [Har01] and Criminisi [CPT03] use exemplarbased synthesis for object removal process. In this paper,
we use the approach used by Criminisi to predict the pixel
values in a band around the border of the piece, however,
the implementation is slightly different.
The source region, Φi, is the acquired image of the ith
piece. A target band, Ωi, outwards from the ith piece is
defined. This target band represents the extension region
 The Eurographics Association 2005.

139

of the ith piece. The border between Φi and Ωi is indicated
by δΩi. This border evolves outward as the inpainting
algorithm progresses. The inpainting algorithm consists of
three main steps. These steps are iterated until the whole
target region or band has been filled. The first step is to
compute the priority, P, which determines the order in
which they are filled. Priority value is computed for the
patches Ψp centered at the point p for p∈δΩi.
Conceptually, the priority depends on continuation of
strong edges, D, and confidence of neighbor pixels, C:

P ( p ) = C ( p ).D ( p )

C ( p) =

∑ C (q)

q∈Ψp ∩φi

Ψp

(1)
⊥

, D( p) = ∇I p .n p

(2)

where |Ψp| is the area of Ψp, np is unit vector
orthogonal to the front δΩ at the point p and ⊥ indicates
the orthogonal operator. This confidence value reflects the
reliability of a region or a pixel, and it effects the filling
order during inpainting process. Initially, we set C=1
(%100 reliability) to pixels in the original piece, and
assign C=0 to the pixels in the target region to be filled.
The Data term D(p) is a function of the strength of
isophotes hitting the front δΩ. This term increases the
priority if an isophote flows into that patch which is
important for the assembly process since it causes the
linear structures to be synthesized or filled first. Therefore,
the linear structures orthogonal to border of pieces are
completed earlier and these points or patches get higher
confidence values.
When all priorities have been computed, the highest
priority, p’, is determined. The second step of the
prediction process is propagating the texture and structure
information into the target band. The color information is
propagated via diffusion in classical inpainting techniques.
In our work, as in [CPT03], propagation of the image
texture occurs by direct sampling of source region. The
most similar patch for sampling is given as:

Ψq′ = arg min d (Ψp′ , Ψq )

(3)

ψ q∈φi

where d(Ψp′,Ψq) is the distance between the already
filled pixels of patches at the points p’ and q. The patch at
the point q’ is the most similar one and the values of each
pixel to be filled in the p’ patch {neighbor p’ | neighbor
p’∈(Ψp′∩Ωi)} are copied directly from the patch in the q’
point.
The last step for iterations is to update the confidence
values. After the patch Ψp′ has been filled with new values,
the confidence values affected by the filling of the new
patch are updated. This region is limited by the neighbors
of the point p’.
(4)
C ( p ) = C ( p′) ∀p ∈ψ p′ ∩ Ω i
As the filling proceeds, the confidence values decrease as
the pixels in the predicted region get farther from the
original boundary. This indicates that the color values of
pixels far from border are less reliable than closer ones.

M.Ş.Sağıroğlu & A. Erçil / A Texture Based Approach to Reconstruction of Archaeological Finds

140

np

 nk

∑ ∑ w D
j

i≠ j

(a)
(b)
Figure 1: (a) An archeological sherd to be expanded
(b) The expanded piece
4. Combining puzzle pieces
While matching or calculating similarity of possible
two neighboring pieces, pixel-by-pixel comparison of two
pieces is not meaningful. Thus, image features, (fik), are
extracted from the source and target regions for each piece
after predicting the target band. Selection of the features
depends on the structure of the image. Currently, only first
and second moments (mean and variance) are used in the
experiments. In the case of using suitable texture features,
serious improvements can be obtained. The features are
calculated in a window whose size depends on the
resolution of the pictures on the pieces. The next step is
the computation of confidence values for the features.
When a feature value is extracted by using the pixels in a
window, the confidence of this feature for a point depends
on the confidences of all pixels in the window. Mean of all
confidence of pixels in the window is assigned as
confidence of the feature, Ci'.
Let Dk(fik(Ti(x,y,θ)),fjk(Tj(x,y,θ))) be the distance
function between the kth feature values of the i and j
pieces. Ti(x,y,θ) denotes the transform of the ith piece at
the point (x,y,θ). In our experiments, Euclidian distance is
used for all features. If distances specific to texture and
features of pieces are selected, the performance of
assembly might improve.
For the simplicity of
expressions, the Ti(x,y,θ) parameter for each variable will
not be shown.
We set a threshold, Thk, for the kth feature distance, so
that the more similar the pieces are, the larger negative
value the similarity measure, Sk, will take or visa versa.
(5)
S k = D k ( f k , f jk ) − Th k
i

nk

∑S
k

k

nk

[

]

(6)

= ∑ w D ( f i k , f jk ) − 1
k

k

k

where nk is the number of features.

nk

∑S

k

gives the total

k

similarity between the ith and the jth pieces at the point
(x,y,θ). We can transform

nk

∑S
k

k

into (6) by dividing all Sk

into Thk and normalizing the total constants to 1, so that
both of them give related responses for the same inputs. wk
are the weight values for the kth feature and are inversely
proportional to Thk.



k

k

k


( f i k , f jk ) − 1C ′j


(7)

where np is the number of pieces in the puzzle.
Expression (7) denotes that total similarity between the ith
and the jth pieces are weighted according to the jth
confidence values, since low confidence points are
unreliable, even if two pieces are similar. After weighting
the similarities, summation for all j pieces where i is
different than j shows how much the ith piece fits the other
pieces at Ti(x,y,θ).

m1 ( x, y ) =

 nk

np

np

i

j =i +1

∑ ∑ ∑ w D


k

k

k


( f i k , f jk ) − 1C ′j Ci′


(8)

np

∑ C′
i

i

This is the first part of the Cost or Affinity function
and is derived from the weighted mean of (7). It is the
summation of similarities for possible pairs. This value
goes towards negative if there exists a good matching
between the pictures on the candidate pieces.
np n p
1 x = 1 (9)
m2 ( x, y ) = ∑ ∑ wc L(Ci ) L(C j ), L( x) = 

i j =i +1
0 x ≠ 1

The second part of the function is for embedding the
geometrical constraints to Cost or Affinity. In reality, two
pieces cannot overlap at any point. The confidence values
are used to formulize overlapping operation. The L
function will be 1 only for pixels in the original part of the
image, otherwise it will be 0. Using a sufficiently large wc,
the Cost increases when the original parts of the ith and jth
images overlap.
(10)
Fcos t = (m1 + m2 )

∑

Total cost is the summation of similarity and
geometrical constraints terms for all points in space. The
only parameter of this performance measure that
represents the goodness of the assembly of pieces based on
textural features and geometrical shape is the
transformation of pieces, Ti.
The fitness between the pieces is increasing while the
Cost function is being optimized. Two types of
optimization methods are used in the experiments. The
first one depends on the best replacement strategy.
Initially, the transformations of pieces are randomly
assigned. The algorithm progresses by finding best
movement in each step. When the function is stuck into a
local minimum, two randomly selected pieces are
exchanged. All local minima are buffered to find the best
assembly. The algorithm is stopped if the function reaches
the best value in the local minima buffer more than n
times.
The second method depends on pairing of pieces.
Initially, the algorithm searches for the best pair that gives
the minimum cost. Then, these paired pieces are merged to
produce a unique piece. The algorithm is stopped when all
the pieces in the puzzle are combined and become one
 The Eurographics Association 2005.

M.Ş.Sağıroğlu & A. Erçil / A Texture Based Approach to Reconstruction of Archaeological Finds

piece. In this method, the algorithm backtracks when the
pairing cannot improve the cost. To implement this
method, the confidence and feature values of the new
piece should be defined after merging process.
(11)
C′ = 1 −
(1 − C ′)

∏

new

i

i∈M



∑  ∏ (1 − C ′ ).C ′. f
j

i

k
i





(12)
 j∈M ,i ≠ j


 ∏ (1 − C ′j ).Ci′
∑
i∈M i∈M ,i ≠ j

M is the set of pieces that will be merged. (11) gives
the new confidence value for overlapping points of pieces.
It express that new confidence value is equal to 1 if one of
pieces has a confidence of 1, otherwise it is the
geometrical mean of possible confidence values at that
point. (12) gives the new kth feature values by calculating
the weighted mean of pieces in the set M.

f k new =

i∈M

(a)
(b)
(c)
Figure 2: (a) A puzzle consisting of 4 pieces, (b)
confidence values of the predicted regions (c) expanded
versions of the pieces. (Fcost = 0)
5. Experimental results
The behavior of the defined affinity measure is
observed under different scenarios. The first one is
whether the edges continue on the neighboring pieces or
not. In the inpainting phase, the edges obtain higher
confidence values as was explained earlier. The higher
confidence values force the cost function to locate the
pieces properly. The second important criterion is
similarity of corresponding textures on the neighboring
pieces. The distance measure in the cost function attracts
similar textures together if the expanded regions of pieces
are accurately inpainted.
In the paper, we present results from two different
datasets. The first dataset consists of 21 pieces of a
ceramica tile. We will give the details of the experiment
with 4 pieces so that the details of the images can be
distinguished. The second dataset (13 pieces) from
Stanford university website is part of the Forma Urbis
Romae dataset[L00] which is a marble map of ancient
Rome that has more than a thousand fragments.
In Figure 2, the original images, confidence images
and expanded images of 4 pieces are placed, respectively.
The cost in the solution space is equal to zero for this
placement, because the expanded or original regions of the
4 pieces are not overlapped anywhere. In Figures 3 and 4,
different assembly stages and the corresponding cost
values are shown. Two neighboring pieces are placed
 The Eurographics Association 2005.

141

closer with a shift in Figure 3a, and their corrected
placement is represented in Figure 3b. The main difference
between the cost values of (a) and (b) is because the edges
don’t continue in (a) although the neighboring textures are
mostly similar. In Figure 3c, the third piece is placed to
their right position, but the original (real) regions of the
fourth piece and the third piece are overlapped; in other
words, the fourth piece violates the geometrical
constraints. For this situation, the second part of the cost
function (m2) becomes dominant and the cost increases
seriously. In Figure 3d, we see the forth piece is placed in
the most appropriate location. Fig 4 shows the completed
reconstruction with the associated cost.Figure 5 shows the
steps of assembling the 13 pieces from the Forma Urbis
Romae dataset.
A second experiment is performed to test the
consistency of the cost function. Puzzles including a few
pieces (2,3 or 4) were artificially prepared. Exhaustive
search was carried out calculating the cost function for all
possible transformations of pieces. The reason of this
experiment was to check whether there exists any
placement giving less cost value than the correct assembly
or not. As a result of the experiment, it was observed that
all other placements of pieces cost more than the true
placement.
The optimization program developed is also tested
against erosion and missing pieces. Even if the edges of
the pieces are eroded or one of the puzzle pieces
disappears, the program was able to find the right
assembly for the puzzles under test.

a ) Fcost = +269

b ) Fcost = -40

c ) Fcost = +2987
d ) Fcost = -1966
Figure 3: (a), (b), (c) Total cost for different layouts (d)
Total cost for the completed puzzle
6. Summary and conclusions
We presented a method for the automated puzzle assembly
problem using surface texture and picture. The approach is
based on expanding the boundary of each piece using
inpainting and texture synthesis and minimizing a cost
function based on matching feature values obtained from

M.Ş.Sağıroğlu & A. Erçil / A Texture Based Approach to Reconstruction of Archaeological Finds

142

these predicted regions. Initial experiments show that this
approach is very promising for the automated puzzle
assembly problem. Future work will concentrate on
optimizing the search for best transformation and
generalizing the presented algorithm to solving 3D
puzzles.

Figure 4:Total cost for the completed puzzle Fcost=-20,076

a ) Fcost = 0

b ) Fcost = +9356

c ) Fcost = -15554
d ) Fcost = -19318
Figure 5: (a), (b), (c) Total cost for different layouts (d)
Total cost for the completed puzzle
Acknowledgements: This research was partially
supported by NSF- IIS-0205477, TUBITAK EEAG

104E155
projects.

and

FP6-2004-ACC-SSA-2

SPICE

References
[BSCB00] BERTALMIO M., SAPIRO G., CASELLES
V., BALLESTER C.: Image inpainting. In
proc.SIGGRAPH, (July 2000), New Orleans, LU ,
pp.417-424.
[BVSO03] BERTALMIO M., VESE L., SAPIRO
G.,OSHER S.: Simultaneous structure and texture image
inpainting. In Proc. Conf. Comp. Vision Pattern Rec.,
(2003) Madison, WI.
[CFF98] CHUNG M.G., FLECK M.M., FORSYTH D.D.:
Jigsaw Puzzle Solver Using Shape and Color. In Proc. of
ICSP, 1998.
[CPT03] CRIMINISI A., PEREZ P., TOYAMA K.: Object
removal by exemplar-based inpainting. CVPR, (2003),
pp. 721-728.

[CS00] Chan T., Shen J. Non-texture inpainting by
curvature-driven diffusions(CCD). (Sept. 2000), UCLA
CAM TR 00-35.
[FG64] FREEMAN H., GARDER L.: A pictorial Jigsaw
puzzles: The computer solution of a problem in pattern
recognition. IEEE Trans. Electron. Comput. 13 (1964) ,
pp. 118-127.
[GMB02] GOLDBERG D., MALON C., BERN M.: A
global approach to automatic solution of jigsaw puzzles.
In Proc. of Conf. on Computational Geometry, 2002, pp.
82-87.
[Har01] HARRISON P.: A non-hierarchical procedure for
re-synthesis of complex texture. In Proc. Int. Conf.
Central Europe Comp. Graphics , Visua. And Comp.
Vision, (Feb. 2001), Plzen, Czech Republic.
[KDB*01]
KOSIBA
A.D.,
DEVAUX
P.M.,
BALASUBRAMANIAN S., GANDHI T.L., KASTURI
R.: An automatic jigsaw puzzle solver. In Proc. Int.
Conf. Pattern Recognition,2001.
[KK01] KONG W., KIMIA B. B.: On solving 2D and 3D
puzzles using curve matching. In Proc. of CVPR, (Dec.
2001), Hawaii, USA.
[KS03] KAMPEL M., SABLATNIG R.: Profile based
Pottery Reconstruction. in Martin D. (ed). IEEE/CVPR
Workshop on Applications of Computer Vision in
Archaeology, CD-ROM, Madison, USA, 2003.
[Lev00] LEVOY M.: The digital michelangelo project:3D
scanning of large statues. Computer Graphics. In Proc.
SIGGRAPH 2000, New York, (July 2000), pp. 131-144.
[OBMC01] OLIVERIA M., BOWEN B., McKENNA R.,
CHANG Y.: Fast digital image Inpainting. Proc. of
Internetional Conference on Visualization, Imaging and
Image Processing, (Sep. 2001) , Marbella, Spain, pp.
261-266.
[PKT01] PAPAIOANNOU G., KARABASSI E.-A.,
THEOHARIS T.: Virtual archaeologist: assembling the
past. IEEE Computer Graphics and Applications. 21(2)
(March/April 2001), pp. 53- 59.
[RB82] RADACK G.M., BADLER N.I.: Jigsaw puzzle
matching using a boundary-centered polar encoding.
CGIP, 19 (1982), pp. 1-17.
[SL02] STOL J., LEİTAO H.: A multiscale method for the
reassembly of two-dimensional fragmented objects.
PAMI, (2002).
[UT99] UCOLUK G., TOROSLU I. H.: Automatic
reconstruction of broken 3-D surface objects. Comp. and
Graph., 23(1999), pp. 573-582.
[Wol90] WOLFSON H.: On curve matching. PAMI, 12
(1990), pp. 483-489.
[WC03] WILLIS A., COOPER D.: Accurately estimating
sherd 3D surface geometry with application to pot
reconstruction. In CVPR Workshop : ACVA, (June
2003).
[WSKL88] WOLFSON H., SCHONBERG E., KALVIN
A. LAMBDAN Y.: Solving jigsaw puzzles using
computer vision. Annals of Operations Research,
12(1988), pp. 51-64.

 The Eurographics Association 2005.

The 6th International Symposium on Virtual Reality, Archaeology and Cultural Heritage
VAST (2005)
M. Mudge, N. Ryan, R. Scopigno (Editors)

Digital Restoration of Medieval Tapestries
Sonja Schär1 , Hanspeter Bieri1 , Xiaoyi Jiang2
1

Institute of Computer Science and Applied Mathematics, University of Bern, Switzerland
2 Department of Mathematics and Computer Science, University of Münster, Germany

Abstract
Medieval Burgundian tapestries belong to the most valuable treasures of historical museums, in particular of the
Bern Historical Museum. Many of them are well preserved, but much of their color is highly faded. Thus their
today’s appearance is very different from the original one. This paper deals with the digital restoration of the
appearance of such tapestries. Two methods are developed and examined, one using the back side of the tapestry,
the other one using color clustering. Our main criteria are a convincing approximation of the expected appearance
and - due to the large size of many of the tapestries - a high degree of automation.
Categories and Subject Descriptors (according to ACM CCS): I.3.3 [Image processing and computer vision]:
Restoration of tapestries, registration, color clustering

1. Introduction
The present paper deals with a rather special but important part of Cultural Heritage, namely the famous Burgundian tapestries dating from the epoch 1440-1515. These wall
tapestries are valuable because of their historical, artistical
and manufactural significance. Many of them are very big,
e.g. 10m in width and 5m in height. Our work has been performed in collaboration with the Bern Historical Museum
which owns a large collection of about a dozen of such
tapestries.
The Burgundian tapestries in the Bern Historical Museum
are well preserved, but their colors are often highly faded.
This reduces their artistical quality, in particular their plastical appearance, and makes it difficult to judge them in an objective way. In the past, these tapestries have been restored
”classically” several times: a number of patches have been
applied, and - worse - some of the weaving has been redone. These restorations are certainly not as doubtful as in
many other cases, but they too have modified the originals
in an irreversible way. Therefore, there was enough motivation to make digital ”copies” of some of these tapestries and
to try to recover their original appearance by means of these
copies. Although nobody will ever know exactly how the
original tapestries looked like, there exists enough knowledge to judge reconstructions reasonably well. Beside obtaining a good appearance of the digital reconstructions, a
c The Eurographics Association 2005.


high degree of automation is attempted because of the large
size of many of the tapestries.
In this paper we develop and discuss two restoration methods, one using the back side of the tapestry, the other applying color clustering. Both use standard techniques from image processing but have to adapt them to the given special
application. With both methods we get promising results,
but the frequent classical repairs as well as ”noise” due to
the weaving process set limits to the degree of automation
we can achieve. Our paper is organized as follows: after a
short discussion of related work and an introduction to Burgundian tapestries, we present our two approaches in detail
and indicate their advantages and limits. Then we try a short
direct comparison of the two methods and end our paper by
giving some conclusions and an outlook to possible further
work.

2. Related work
Digital preservation and reconstruction is a wide field and includes manuscripts, newspapers, books, photographs, films,
paintings and sculptures. But to our knowledge there exists no work dealing specifically with the reconstruction or
preservation of tapestries.

144

S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

2.1. Photographs
To reconstruct faded or discolored photos one can use techniques included in most image processing software packages: correction of tone, color, contrast and saturation, copying of texture, or noise and blur filters. Small errors in a homogeneous environment can often be removed by a simple
interpolation of the adjacent pixels. But if larger parts of the
motif are missing this technique does not succeed. Sapiro
and Bertalmio developed a solution to this problem, i.e. digital inpainting attempts to replicate the techniques used by
”classical” restorators [BSCB00]. Further work analyzes inpainting with automatic error recognition, global inpainting
and acceleration algorithms. A summary of the state-of-theart can be found in [Pra04].

obeys the same laws in the whole picture. On the other hand
tapestries are woven with different materials and dyed with
different colors which change differently in the course of
time. A global correction of the tone or color, as with the
reconstruction of photographs, is not possible. Each color
must be corrected individually.
Inpainting: The idea of inpainting is used in our first approach to remove disturbing features on the back side of
the tapestry. Since images of the front and back side exist,
a comparison of these images is used with our inpainting algorithm.

Unlike with photographs, the frames before and after a certain frame of a movie can be used to reconstruct this frame.
Reconstruction of films is a large subject which cannot be
discussed here. Useful information can be found e.g. in
[Kok98]. The possibility to compare successive frames is
important for us because, like the frames in a film, the front
and back side of a tapestry can be compared.

Modelling the fabric: The technique described in
[AMT03, AMTF03] for modelling woven textiles could
probably be adapted to the reconstruction of tapestries. The
weaving pattern would have to be adapted in such a way
that only weft threads are visible, and the color pattern of
the front and back side would have to cover the whole
tapestry, for in Burgundian tapestries no repeating patterns
arise. However we do not model the fabric of the tapestry.
The reason is that small irregularities, errors in the fabric
and peculiarities of the weaving techniques are very important for the natural appearance of the tapestries which should
not be lost during the reconstruction.

2.3. Textiles

Color clustering: The classification of pixel values into
clusters is very useful to identify regions of a certain color.

2.2. Film

Adabala et al. [AMT03, AMTF03] developed a technique to
render textiles with complex weaving patterns. The microand millistructure of textiles and single threads, respectively,
and the complex refraction of light are considered. To represent a weaving pattern, three images are used. The first is
a greyscale image showing the portion of warp threads at a
certain place. The two others are color images which indicate the color pattern of the front and back side of the fabric.
Thereby the number of different colored threads is unlimited. Further information for the modelling of textiles can be
found e.g. in [GRS96, XCL∗ 01].
2.4. Color transfer between images
Besides the techniques for digital restoration mentioned
above, also color transfer between images is a relevant topic
with respect to our problem. Reinhard et al. [RAGS01] perform a statistical analysis to determine the color characteristics to be transformed from one image to the other. Chang
et al. [CSUN05] determine color characteristics by a classification of pixel values according to color categories which
are defined by considering properties of human color perception.
2.5. Importance for the reconstruction of tapestries
A photograph or a film consists of homogeneous material.
This means that all points in the picture have the same physical and chemical characteristics, thus the change of colors

In summary it can be said that many ideas can be taken
from related fields e.g. inpainting or the comparison of
frames. But they must be adapted to the specific situation
with tapestries.

3. Burgundian tapestries
3.1. Fundamentals
Burgundian tapestries enjoyed a high appreciation, and the
production of large size tapestries was even more important than that of oil paintings. Often their drafts were drawn
by the best artists [Jez01]. Some of these drafts are preserved and can be used to study the original coloring of the
tapestries. Another useful possibility is to study the coloring
of paintings made by the same artists.
Often the size of a tapestry is enormous; 5 meters in height
and 10 meters in width is not unusual. The motifs are very
complex and detailed, and different techniques were used to
intensify the plastic impression. Thus the figures are mostly
enclosed by a dark brown outline. Other lines result because
the gaps which arise when leading back the weft thread are
set intentionally. In order to make larger parts woven in the
same color more vivid, the thread was inserted partially in diagonal or curved form. The high degree of detail makes the
restoration more difficult too. None of the important small
parts should be lost, and the plastic effects should be preserved.
c The Eurographics Association 2005.


S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

145

3.2. Manufacturing the tapestries
The tapestries are hand-woven. The needle leads the weft
thread through the warp only as far as necessary, according
to the intended picture. The tapestries are dyed with natural
colors e.g. archil, indigo or a yellow color called ”luteolin”.
These colors are described in [HdG04]. The specific way of
manufacturing also affects the restoration. To obtain a good
result, all factors constituting the characteristics of a tapestry
must be considered. For example, the kind of weaving affects its structure. The structure is important for the appearance of the tapestry and may not be lost during restoration.
Also important is the ”noise” on the back side, caused by
the weaving technique e.g. loose threads or threads carried
along the back side.
3.3. Aging and preservation
The aging process has damaged the fabric: The tapestries are
frayed at their borders, they contain holes, and in some parts
the weft thread is lost and the warp thread is visible. Furthermore the colors have faded in the course of aging. On
the front side the originally intensive colorfulness is often
lost. Parts dyed with archil have faded from a dark magenta
to a pale beige. Because the silver has oxidized, the silver
gimps have become black, and because the lightfastness of
”luteolin” is much smaller than the lightfastness of indigo,
originally green parts are bluish now. On the back side the
color is normally much better preserved, although fading has
occurred here too. Parts dyed with archil can only be identified here. By classical restoration defective parts have been
repaired, patches of dyed linen fabric have been stitched on,
or parts have been even woven again. During these restorations crosses and ribbons of fabric were attached to the back
side to reinforce the tapestry. They have all been taken away
today, but their positions on the tapestry are still visible because the color is less faded where they were stitched on
[RBSS01]. The fact that the back side is normally less faded
forms the basis of our ”back side approach”. Of course the
patches and traces of the attached ribbons lead to problems.
Most features of Burgundian tapestries and all problems
listed above can be found in the part ”widow” of the famous Trajan tapestry. It contains most colors of the tapestry,
and a large part has been dyed with archil. On its back side
many traces from classical restorations can be found. Figure
1 shows the front side and Figure 2 the back side of the part
”widow” of the Trajan tapestry.
4. Digital restoration
We present two methods to restore the ”original” colors of a
Burgundian tapestry. Both are applied to a high-quality digital photograph of the tapestry in its today’s state. Our first
approach makes use of the photographed back side of the
tapestry, our second approach works mainly with color clustering.
c The Eurographics Association 2005.


Figure 1: The front side of the part ”widow”

Figure 2: The back side of the part ”widow”

4.1. Preprocessing
With both methods, the pixels on the front side must be compared to the corresponding pixels on the back side. To do
this, the image of the back side must be ”laid” as accurately
as possible onto the image of the front side. Thus a registration of the images must be made. To match the two images,
matching points are defined by hand in both of them and then
the transformation matrix is calculated using a least-square
estimate.

146

S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

4.2. Back side approach
As the back side of a tapestry hanging at the wall is less
exposed to light than the front side, its colors are normally
much less faded. Therefore replacing each front side pixel
by the corresponding back side pixel should give a good approximation to the tapestry’s original appearance. The idea
is straightforward, but its implementation is complicated by
a number of difficulties: unfortunately, there are normally
many disturbing features on the back side, especially patches
sewn on and loose threads hanging down (Figure 3). Our approach intends to restrict the restoration to the removal of
such features. To do this, the colors of the matching pixels
of the front side are applied, or the restoration of the color is
determined by means of a simple inpainting algorithm.

Figure 3: Disturbing features on the back side of the tapestry
The restoration of the image of the back side is performed
in three steps. The first step consists in the registration between the front side and the back side image. In the second
step, disturbing features on the back side are marked. Finally, the marked pixels are replaced by the matching pixels
of the front side, or the marked parts are filled by inpainting.
Figure 4 illustrates this back side approach.

a loose thread on the back side, the difference between the
color of the pixel on the front side and that of the matching
pixel on the back side is normally much bigger than it would
be if the color were simply more faded on the front side than
on the back side. Therefore the difference between the colors of two matching pixels can be used to mark the disturbing features on the back side. If the distance is bigger than
a chosen value, the pixel will be marked. The calculation of
the distance can be done in the RGB or HSI color space.
Marked areas smaller than n connected pixels can be deleted
automatically, and unmarked spots of less than n connected
pixels can be marked. The appropriate value for n depends
on the resolution and size of the image.
Inpainting: Each spot of marked connected pixels on the
back side is filled pixel by pixel, beginning at the border of
the spot. For each pixel pb1 on the back side the new color
is calculated in four steps:
• The color of the matching pixel p f 1 on the front side is
read.
• This color is compared with the color of the pixels in the
neighbourhood of the pixel p f 1 . Only pixels not marked
or already recolored are used for this comparison.
• The pixel whose color has the smallest distance d to the
color of p f 1 becomes the pixel p f 2 . The distance between
the two colors is saved.
• The color of the corresponding pixel pb2 on the back side
is read. The distance d is subtracted from it. The result is
the new color of pixel pb1 .
Figure 5 illustrates this inpainting.

Figure 5: Corresponding pixels during the inpainting
process

4.3. Color clustering approach

Figure 4: The back side approach
Marking the disturbing features: Their pixels can either marked by hand or automatically: If there is a patch or

A human observer can easily describe the colors appearing
in a certain part of the tapestry. For example he perceives
the colors black, blue, beige and yellow. An experienced observer can also estimate the original appearance of these colors. He may judge e.g. that a beige region was originally
purple. Color clustering allows us to find all pixels corresponding to this beige region. They do not all have exactly
c The Eurographics Association 2005.


147

S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

the same beige, of course, therefore an average beige is determined and then converted to a suitable purple. Now, if we
would just recolor all beige pixels with this purple, the structure of the tapestry would get lost. Therefore we calculate the
distance between the color of the corresponding pixel on the
front side and the color of the cluster and subtract this distance from the new color. Unfortunately, a faded color cannot be replaced by the same new color in every part of the
tapestry. An example can be found in the part ”widow” of
the Trajan tapestry: the horse and the skirt of the widow have
the same color. But on the back side can be seen that the skirt
originally was purple, while the horse was beige. These parts
cannot be separated by a clustering of the front side colors. If
all beige colors were converted to the same color, the result
would be false. Either the horse would be purple or the skirt
would stay beige. Because the color of one cluster cannot
be converted to the same color everywhere in the image, in
general, a segmentation of the image is necessary. Thus the
clustering method consists of four steps. In the first step, the
image of the front side is segmented. Then the colors of each
segment are clustered separately. In the third step, the colors
of each cluster are transformed into the new colors. Finally
all segments are assembled to the restored image. Figure 6
illustrates the clustering approach. In the following, the four
steps will be explained in detail.

edges or whole outlines. It is performed by the following
steps:
•
•
•
•
•

Conversion to a greyscale image
Edge detection
Thresholding to convert the image to a binary image
Skeletonizing
Identification of the segments

For edge detection we use several convolution and morphological filters. The skeletonization is done using the ”two
pass algorithm” of Zhang and Suen [Lyo99], and segments
are identified by a simple flodfill algorithm. Because too
many segments can cause problems, we try to determine
only the important ones. That is, lines caused by the structure of the tapestry should not be marked as edges, and all
segments should have a certain size. For this reason a preprocessing with a lowpass filter is done before edge detection. A second method to reduce the number of segments and
to get larger segments consists in marking all spots smaller
than n pixels as edges, after edge detection and before skeletonizing. Figure 7 shows an example.

(a)

(b)

(c)

(d)

Figure 7: Edges before and after marking small spots (a,b),
and the resulting skeletons (c,d)

Figure 6: The clustering approach

Segmentation: The simplest way to divide an image into
segments is to define the outline of the segments by marking points with the mouse. The advantage is that the segments result in accordance with the requirements. The disadvantage is the lack of automation. Our automatic segmentation tries to find segments by finding edges in the image.
We choose this procedure because of its relative simplicity
and because all relevant segments are separated by clear-cut
c The Eurographics Association 2005.


Color clustering: The clustering of the colors can be
done either with predefined cluster centers or automatically.
If we use predefined cluster centers, we define their colors
for all clusters we need. Furthermore we define a new color
for each cluster. For each pixel the cluster with the smallest distance between the color of the pixel and the color of
the cluster center is found and the pixel’s color is changed
to the new color. Clustering with predefined cluster centers
can only be used in case of a small number of segments because one has to define interactively the cluster centers for
each segment. Automatic clustering is done using a color
histogram. The number of clusters and the colors of the cluster centers are not known in advance. Automatic clustering
is done by the following steps:
• A histogram of the colors occurring in the image is created.
• For each cell in the histogram a pointer to the biggest
neighbour is saved.
• Now the histogram contains chains of cells, each one
pointing to a local maximum. Each of these chains represents a cluster [SKPB00].
Figure 8 shows the automatic clustering in case of a

148

S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

greyscale image. Colored images need a three dimensional
histogram. The number of clusters is influenced by the quantization of the colors. Clustering can be done in the RGB or
HSI color space and with different levels of quantization.
There are three possibilities to determine the new colors of
the clusters:

A better registration should enhance the results of the back
side approach. Another possibility might be to process the
pixels in correlation with their neighbourhood. If all these
problems could be solved, the problem of correctness would
still remain. The color on the back side is faded too. Thus the
result is only an approximation to the original appearance.

1. Only the colors on the front side are clustered. The new
colors are defined by the user.
2. The colors on the front and on the back side are clustered.
The matching of a clusters on the front side to a cluster
on the back side is done by the user.
3. Again, the colors on the front and on the back side
are clustered. Those clusters are matched that share the
largest number of pixels.

Our conclusion is that a faded tapestry can successfully
be restored by means of our first approach if on its back side
the number of patches and loose threads is small, the patches
themselves are small, and the colors are not much faded. A
high degree of automation and a fast restoration may then be
expected.

Transforming the colors: Before transforming the color
of a pixel, the distance between this color and the color of the
cluster center is saved. After performing the transformation
this distance is added again. Doing this, the structure of the
tapestry is preserved. Without considering this distance the
tapestry looks flat because all pixels of a cluster get the same
color.

5.2. Color clustering approach

5. Results
We have implemented both approaches and performed a
number of tests using the part ”widow” of the Trajan
tapestry. The size of the corresponding digital images of the
front and back side is 5780 (height) x 4521 (width) pixels
(Figures 1 and 2). The tests used different parts of this image and different resolutions.
5.1. Back side approach
The main advantage of this approach is its simplicity. As the
new color is taken from the back side, no further knowledge
about the colors is needed. Furthermore no segmentation is
necessary. A single color area is automatically refreshed in
different colors if appropriate. Problems arise because some
patches and threads on the back side are not marked because
the difference between the color of the patch or thread, respectively, and the corresponding color on the front side is
too small. Especially threads are often not marked because
they typically hang over parts of the same color. Another
problem arises because the registration of the two images
is not perfect. Wherever false pixels match, the color difference is big, and thus the pixels are marked. These marks
are wrong. Moreover parts dyed with archil are marked too,
because the color is very much faded and thus the color distance is big. These marks are wrong too. The burls on the
front and on the back side of a woven fabric do not lie at
the same position, in general. Therefore the front side and
the reflected back side of a tapestry never look the same.
This leads to wrong marks and to a bad reconstruction of
the structure of the tapestry after inpainting. The result of inpainting is good in case of images with a perfect registration
and no difference in structure.

The best results for the task of segmentation can be achieved
by edge detection with a morphological filter after a preprocessing with a lowpass filter. In addition, after edge detection edges smaller than a certain number of pixels should
be removed and spots smaller than a certain number of pixels should be marked as edges. The optimal number of pixels
depends on the resolution and on the size of the image.
The main advantage of the color clustering approach is its
flexibility. The new colors of the tapestry may be defined arbitrarily. Furthermore the degree of automation is relatively
high. If the reconstruction is done by automatic segmentation and by automatic clustering, using the matching of the
front side and back side colors, it is possible to reconstruct
a significant part of a tapestry in a reasonable time. Unfortunately, it is not yet possible to have the advantage of flexibility and that of automation at the same time because the
assignment of arbitrary colors to the pixels of the front side
clusters cannot yet be done automatically. Another advantage is the good preservation of the tapestry’s structure due
to considering the distance between the color of a pixel and
the color of the cluster center it belongs to.
One of the disadvantages of the method is the dependency
of the result on the quality of segmentation. As the new color
is automatically extracted from the back side image, the occurrence of too many segments in a part where the back side
contains patches is problematic. The reason is that the color
of the patch is taken as the new color. The quality of the
clustering influences also the result. Colors located almost
in the middle between two cluster centers are problematic
because they are assigned only to one cluster and normally
have a relatively big distance to the center of this cluster. After converting these colors to the new color and adding the
color distances, undesired colors can result. This happens
mostly if a color is modified very much. After adding the distance, it may result that the value of a color component has
to be clipped. Thus the automatic matching of the clusters of
the front and back side is better if the clusters contain many
pixels. The clustering using predefined cluster centers is not
very useful. The main problem is to determine the colors of
c The Eurographics Association 2005.


S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

149

Figure 8: Clustering in case of a grayscale image

the cluster centers. A color may seem to be a good representative for a group of colors to the human eye, but proved
not suitable to calculate the clusters. Clustering using a histogram is much more practicable. The clusters correspond
to the color groups that are identifiable by eye. An important parameter is the value of quantization. If it is too small,
fairly different colors are combined into one cluster. If it is
too big, too many clusters arise, causing problems when the
clusters of the front side are combined automatically with
the clusters of the back side. Suitable is a value between
15 and 30 for each color component. If there are only few
segments, it is possible to refresh the colors by user-defined
colors. When defining the new colors, it is necessary to make
sure that the new colors aesthetically fit. Moreover defining
the colors is time-consuming. To reconstruct an image with
many segments, only the automatic matching of the colors
of the front and back side can be used, of course.
An enhancement of our approach could be achieved by
an automatic matching of user-defined colors to the colors
of the front side. For example, a matching of these colors
could be done by analyzing the colors found on the back
side. A better matching of the colors of the front and back
side could enhance this approach and reduce the errors. For
instance, not only the number of shared pixels but also the
size of the clusters could be considered. An enhancement of
the registration as well as a better automatic segmentation
could also improve the result. With a fuzzy clustering as described in [CSUN05], the number of undesired colors could
probably be reduced.
Our second approach, if using nonautomatic segmentation
and automated clustering leads to satisfactory results. If the
colors of the back side are used for the reconstruction, the
necessary time and user effort are acceptable, and a first impression of the original appearance can well be gained.

5.3. Comparison of the two approaches
To compare the two methods with each other, we applied
them to the part ”widow” of the Trajan tapestry and judged
the results visually. In practically all aspects the color clustering approach proves to be better than the back side approach. In most cases the result of the reconstruction looks
better and contains less faults. The color clustering approach
c The Eurographics Association 2005.


is more flexible and thus a result close to the assumed original appearance can be achieved more easily. Also an adaption to other applications, like the digital restoration of paintings, is only possible with this approach, because of the dependency of the back side approach on the existence of a
back side. Only with respect to automation the two methods
prove equal. Figure 9 shows the result of a reconstruction
using the back side approach. The disturbing features on the
back side were marked by hand and filled by inpainting. Figure 10 shows the corresponding result using the color clustering approach. The segments were created by user interaction and the clustering was done in RGB color space using a
quantization value of 25 and automatic color matching.

6. Final remarks
Fortunately, quite a few Burgundian tapestries have survived
until today. There are many good reasons to provide digital
photographs of high quality of them, an important one is that
such digital copies make experimentation possible - without damaging the precious originals. The present paper deals
with such an experimentation by proposing two methods for
reconstructing the original appearance of such tapestries.
The back side approach is more straightforward but normally less useful than the color clustering approach. Burgundian tapestries make image processing difficult: the normally
very complex scene, the irregular structure of the woven fabric, as well as the patches and hanging loose threads on the
back side are the reasons that user interaction cannot be completely avoided. Our methods cannot be expected to be perfect, but they are more than a first step, and experimenting
with them offers much valuable insight.
It is rather well-known how colors were manufactured
in the Middle Ages [HdG04, Sch02]. Using this knowledge
could enhance our two methods. Discussing further the results with professional restorers could give ideas for improvements. A completely different approach would consist
in building 3D models of Burgundian tapestries, as it has
already been done successfully with other kinds of textiles
[AMT03, AMTF03]. Adapting our methods to other kinds
of tapestries, e.g. modern carpets, seems promising too.

150

S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

7. Acknowledgement
The authors would like to thank Peter Jezler and Karen
Christie of the Bern Historical Museum for their interest and
support.

[AMTF03] A DABALA N., M AGNENAT-T HALMANN N.,
F EI G.: Visualization of woven cloth. In Eurographics
Symposium on Rendering (2003), Christensen P., CohenOr D., (Eds.), pp. 178–185.
[BSCB00] B ERTALMIO M., S APIRO G., C ASELLES V.,
BALLESTER C.: Image inpainting. In SIGGRAPH ’00:
Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques (2000), ACM
Press/Addison-Wesley Publishing Co., pp. 417–424.
[CSUN05] C HANG Y., S AITO S., U CHIKAWA K.,
NAKAJIMA M.: Example-based color stylization of images. ACM Trans. Appl. Percept. 2, 3 (2005), 322–345.
[GRS96] G RÖLLER E., R AU R. T., S TRASSER W.: Modeling textiles as three dimensional textures. In Proceedings of the eurographics workshop on Rendering
techniques ’96 (London, UK, 1996), Springer-Verlag,
pp. 205–ff.
[HdG04] H OFENK DE G RAAF J. H.: The Colourful Past.
No. ISBN 3-905014-25-4. Abegg Stiftung, 2004.
[Jez01] J EZLER P.: Burgunder Tapisserien in neuem Licht,
2001. http://www.bhm.ch/auxx/medien/311001\_3\
_d.pdf.

Figure 9: A reconstruction using the back side approach

[Kok98] KOKARAM A.: Motion Picture Restoration.
No. ISBN 3-540-76040-7. Springer Verlag, 1998.
Image Processing in Java.
[Lyo99] LYON D. A.:
No. ISBN 0-13-974577-7. Prentice Hall PTR, 1999.
[Pra04] P RADHAN N.: Digital Image Restoration Techhttp://www.ces.
niques and Automation, 2004.
clemson.edu/~stb/ece847/projects/proj06.pdf.
[RAGS01] R EINHARD E., A SHIKHMIN M., G OOCH B.,
S HIRLEY P.: Color transfer between images. IEEE Comput. Graph. Appl. 21, 5 (2001), 34–41.
[RBSS01] R APP -B URI A., S TUCKY-S CHÜRER M.: Burgundische Tapisserien. No. ISBN 3-7774-9260-4. Hirmer
Verlag GMBH, München, 2001.
[Sch02] S CHWEPPE H.: Handbuch der Naturfarbstoffe:
Vorkommen, Verwendung, Nachweis. No. ISBN 3-60965130-X. ecomed Verlagsgesellschaft, 1902.
[SKPB00] S OBOTTKA K., K RONENBERG H., P ERROUD
T., B UNKE H.: Text extraction from colored book and
journal covers. IJDAR: International Journal on Document Analysis and Recognition 2 (2000), 163–176.

Figure 10: A reconstruction using the color clustering approach

References

[XCL∗ 01] X U Y.-Q., C HEN Y., L IN S., Z HONG H.,
W U E., G UO B., S HUM H.-Y.: Photorealistic rendering of knitwear using the lumislice. In SIGGRAPH
’01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques (New York, NY,
USA, 2001), ACM Press, pp. 391–398.

[AMT03] A DABALA N., M AGNENAT-T HALMANN N.: A
procedural thread texture model. Journal of Graphics
Tools 8(3) (2003), 33–40.
c The Eurographics Association 2005.


Cover Image Credits
front cover: (from left to right)
Mark Mudge, Jean-Pierre Voutaz, Carla Schroer, and Marlin Lum: "Reflection Transformation
Imaging and Virtual Representations of Coins from the Hospice of the Grand St. Bernard",
pp. 29–39
G. H. Bendels, R. Schnabel, and R. Klein: "Detail-Preserving Surface Inpainting", pp. 41–48
Veronica Sundstedt, Diego Gutierrez, Fermin Gomez, and Alan Chalmers: "Participating Media
for High-Fidelity Cultural Heritage", pp. 83–90

Author Index
Abad, M. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
Akama, Ryo . . . . . . . . . . . . . . . . . . . . . 59, 157
Alzua-Sorzabal, A. . . . . . . . . . . . . . . . . . . . 121
Arretxea, L. . . . . . . . . . . . . . . . . . . . . . . . . . 121
Ballan, L. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Bendels, Gerhard H. . . . . . . 13, 41, 155, 156
Berndt, René . . . . . . . . . . . . . . . . . . . . . 49, 155
Bieri, Hanspeter . . . . . . . . . . . . . . . . . 143, 160
Boubekeur, Tamy . . . . . . . . . . . . . . . . . 75, 158
Brusco, N. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Buhalis, Dimitrios . . . . . . . . . . . . . . . . . . . . 129
Carpendale, S. . . . . . . . . . . . . . . . . . . . . . . . . 91
Chalmers, Alan . . . . . . . . . . . . . . 83, 107, 160
Cortelazzo, G. M. . . . . . . . . . . . . . . . . . . . . . 21
Dähne, P. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Debattista, K. . . . . . . . . . . . . . . . . . . . . . . . . 107
Duguet, Florent . . . . . . . . . . . . . . . . . . . 75, 158
Erçil, A. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Fellner, Dieter W. . . . . . . . . . . . . . . . . . 49, 155
Gerth, Björn . . . . . . . . . . . . . . . . . . . . . . 49, 155
Glanzman, W. D. . . . . . . . . . . . . . . . . . . . . . . 91
Gomez, Fermin . . . . . . . . . . . . . . . . . . . 83, 160
Gutierrez, Diego . . . . . . . . . . . . . . . . . . 83, 160
Havemann, Sven . . . . . . . . . . . . . . . . . . 49, 155
Heras Ciechomski, Pablo de . . . . . . . 91, 159
Jiang, Xiaoyi . . . . . . . . . . . . . . . . . . . . 143, 160

Klein, Reinhard . . . . . . . . . . . 13, 41, 155, 156
Linaza, M. T. . . . . . . . . . . . . . . . . . . . . . . . . 121
Lum, Marlin . . . . . . . . . . . . . . . . . . . . . . . . . . 29
Maïm, Jonathan . . . . . . . . . . . . . . . . . . 91, 159
Malerczyk, C. . . . . . . . . . . . . . . . . . . . . . . . 113
Maupu, Damien . . . . . . . . . . . . . . . . . . 91, 159
Mudge, Mark . . . . . . . . . . . . . . . . . . . . . . . . . 29
Müller, Gero . . . . . . . . . . . . . . . . . . . . . 13, 155
Owen, Ruth . . . . . . . . . . . . . . . . . . . . . . . . . 129
Plemenos, Dimitri . . . . . . . . . . . . . . . . . . . . . 67
Pletinckx, Daniël . . . . . . . . . . . . . . . . . . . . . 129
Sagiroglu, M. S. . . . . . . . . . . . . . . . . . . . . . 137
Schär, Sonja . . . . . . . . . . . . . . . . . . . . 143, 160
Schertenleib, Sébastien . . . . . . . . . . . . 91, 159
Schlick, Christophe . . . . . . . . . . . . . . . 75, 158
Schnabel, R. . . . . . . . . . . . . . . . . . . . . . 41, 156
Schnaider, M. . . . . . . . . . . . . . . . . . . . . . . . . 113
Schroer, Carla . . . . . . . . . . . . . . . . . . . . . . . . . 29
Sokolov, Dmitry . . . . . . . . . . . . . . . . . . . . . . 67
Sundstedt, Veronica . . . . . . . . . . . . . . . 83, 160
Susperregui, A. . . . . . . . . . . . . . . . . . . . . . . 121
Tanaka, Hiromi T. . . . . . . . . . . . . . . . . 59, 157
Thalmann, Daniel . . . . . . . . . . . . . . . . . 91, 159
Voutaz, Jean-Pierre . . . . . . . . . . . . . . . . . . . . 29
Xu, Weiwei . . . . . . . . . . . . . . . . . . . . . . 59, 157
Zuk, T. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91

Color Plates

G. Müller et al. / Rapid Synchronous Acquisition of Geometry and Appearance of Cultural Heritage Artefacts

155

Figure 2: The Acquisition Pipeline: The multi-camera array records 151 images per camera per light direction, resulting in
22801 images. Here, only the first ten cameras (from left to right) and the first ten light directions (from top to bottom) are
shown. From the full set of these images, the BTF is constructed, while only a subset (the diagonal in this matrix notation) is
used for the geometry reconstruction.

Figure 9: Raytraced renderings of a captured and reconstructed echinite under novel lighting and viewing conditions. The
left and middle image are rendered with a small area light source and demonstrate the fine geometric details captured in the
BTF. The right image shows a relighting of the echinite with a complex image-based lighting environment captured in front of
the Pädagogische Fakultät in Bonn. The ground floor in the right image is covered with a synthetic leather BTF courtesy of
DaimlerChrysler AG.
Gerth, Berndt, Havemann, Fellner / 3D Modeling for Non-Expert Users

Figure 1: Creation of houses and palisades using variants of the polygon editor. A new house is created from a list of available
house types (1a). It can be moved (1b) and rotated (1c) as a whole prior to editing the outline polygon (1d). Row 2: Adaptation
of the ground polygon by alternating insertion and movement actions. A palisade is created very much like an arkade (3b-d).
c The Eurographics Association 2005.


156

G.H. Bendels & R. Schnabel & R. Klein / Detail-Preserving Surface Inpainting

Figure 11: Max Planck model with missing data. The hole is inpainted by copying appropriate fragments of other parts of the
object to the hole region.

Figure 12: Hierarchical reconstruction of the Stanford Bunny. First, a point set hierarchy of the defective bunny is constructed.
Starting with h = 3, each level is filled per-se, where in level h, the level h + 1 serves as Guidance Surface.

c The Eurographics Association 2005.


157

Xu et al / 3D Face Modeling from Ancient Kabuki Drawings

Figure 13: Mapping make-up patterns in Peking opera to the reconstructed face model

The way to get Kumadori
from the face of player

The ancient Kumadori

Figure 14: An ancient Kumadori

Define corresponding feature points

Project to a
cylindrical plane

Mesh

Texture Mapping

Mapping Result

Kumadori

Figure 15: Mapping the ancient Kumadori

c The Eurographics Association 2005.


158

T. Boubekeur, F. Duguet and C. Schlick / Rapid Visualization of Large Point-Based Surfaces

Figure 11: Real-time visualization of the St Matthew model (186 810 938 points). From left to right: the sub-sampled model,
the coarse mesh locally generated in the leaves of the stripping tree (in green), the coarse mesh with the high resolution normal
mapping with one white light source and 3 colored light sources.

c The Eurographics Association 2005.


T. Zuk & S. Carpendale & W. D. Glanzman / Visualizing Temporal Uncertainty in 3D Virtual Reconstructions

159

Figure 4: Juxtaposition of theoretical reconstructions and
survey data.

Figure 6: Uncertainty cues. From top to bottom: no cues,
rising/sinking cue, wireframe, and transparency.

Figure 5: Simulated archaeological reconstruction.

c The Eurographics Association 2005.


160

V. Sundstedt, D. Gutierrez, F. Gomez, A. Chalmers / Participating Media for High-Fidelity Cultural Heritage

Figure 9: (top left) High-fidelity reconstruction and sun simulation of the ancient Egyptian temple of Kalabsha rendered in
Radiance, (top right) photograph of volumetric light in an Egyptian temple, (bottom) visual comparison of two renderings of
one interior window of the Kalabsha temple rendered in Lucifer: (left) without participating media, (right) with participating
media.
S. Schär & H. Bieri & X. Jiang / Digital Restoration of Medieval Tapestries

Figure 11: The part ”widow” of the Trajan tapestry: front side (a), back side (b), reconstruction using the back side approach
(c), reconstruction using the color clustering approach

Figure 12: Disturbing features on the back side of the tapestry
c The Eurographics Association 2005.


View publication stats

